{
  "lastUpdated": "2025-09-23T17:49:51.601Z",
  "stats": {
    "totalQuickstarts": 17,
    "totalImages": 49,
    "successfulImageDownloads": 49,
    "failedImageDownloads": 0,
    "generatedImages": 3
  },
  "kickstarts": [
    {
      "id": "spending-transaction-monitor",
      "title": "Spending Transaction Monitor",
      "description": "No description available",
      "readmePreview": "<!-- omit from toc -->\n# Spending Transaction Monitor\n\nAlerting for credit card transactions with rule-based and future natural-language rules.\n\nFor c...",
      "readmeContent": "<!-- omit from toc -->\n# Spending Transaction Monitor\n\nAlerting for credit card transactions with rule-based and future natural-language rules.\n\nFor contribution guidelines and repo conventions, see [CONTRIBUTING.md](CONTRIBUTING.md).\n<!-- omit from toc -->\n## Table of contents\n\n- [Overview](#overview)\n- [How it works](#how-it-works)\n- [Getting started](#getting-started)\n- [üê≥ Container Deployment (Recommended)](#-container-deployment-recommended)\n  - [üöÄ Quick Start with Podman Compose](#-quick-start-with-podman-compose)\n- [üß™ Testing Alert Rules](#-testing-alert-rules)\n  - [**Interactive Alert Rule Testing**](#interactive-alert-rule-testing)\n  - [**Example Workflow**](#example-workflow)\n  - [**What the Test Does**](#what-the-test-does)\n  - [‚òÅÔ∏è OpenShift Deployment](#Ô∏è-openshift-deployment)\n- [üîß Local Development Mode](#-local-development-mode)\n  - [üîß Development Mode (Authentication Bypass)](#-development-mode-authentication-bypass)\n- [üîß Environment Variables Configuration](#-environment-variables-configuration)\n  - [Required Environment Variables](#required-environment-variables)\n    - [Database Configuration](#database-configuration)\n    - [SMTP Configuration (for email alerts)](#smtp-configuration-for-email-alerts)\n    - [API Configuration](#api-configuration)\n    - [LLM Configuration](#llm-configuration)\n  - [üê≥ Local Container Deployment (podman-compose.yml)](#-local-container-deployment-podman-composeyml)\n  - [‚òÅÔ∏è OpenShift Deployment (values.yaml)](#Ô∏è-openshift-deployment-valuesyaml)\n  - [üîß Local Development Environment Variables](#-local-development-environment-variables)\n- [Components](#components)\n- [Standards](#standards)\n- [Releases](#releases)\n- [Structure](#structure)\n\n## Overview\n\n- Monorepo managed with Turborepo\n- UI: React + Vite\n- API: FastAPI (async SQLAlchemy)\n- DB: PostgreSQL with SQLAlchemy models and Alembic migrations\n\nPackages\n- `packages/ui`: web app and Storybook\n- `packages/api`: API service and routes\n- `packages/db`: models, engine, Alembic, seed/verify scripts\n- `packages/ingestion-service`: transaction ingestion service with Kafka integration\n- `packages/evaluation`: rule evaluation (scaffold)\n- `packages/alerts`: alert delivery (scaffold)\n- `packages/configs/*`: shared ESLint/Prettier configs\n\n## How it works\n\n- Users create alert rules (amount, merchant, category, timeframe, location; notification methods: email/SMS/push/webhook).\n- Incoming transactions are stored and evaluated against active rules.\n- Triggered rules produce alert notifications which are delivered via configured channels.\n\n```mermaid\ngraph TD\n\n  %% UI\n  subgraph UI[\"UI (packages/ui)\"]\n    U[\"User\"] --> WUI[\"Web UI\"]\n  end\n\n  %% API\n  subgraph API[\"API (packages/api)\"]\n    API_APP[\"FastAPI App\"]\n    IN[\"Transaction API\"]\n  end\n\n  %% Evaluation\n  subgraph EVAL[\"Evaluation (packages/evaluation)\"]\n    EV[\"Rule Evaluation Service\"]\n  end\n\n  %% Alerts\n  subgraph ALERTS[\"Alerts (packages/alerts)\"]\n    AL[\"Alerts Service\"]\n  end\n\n  %% DB\n  subgraph DB[\"DB (packages/db) - PostgreSQL\"]\n    USERS[\"users\"]\n    CARDS[\"credit_cards\"]\n    AR[\"alert_rules\"]\n    TRX[\"transactions\"]\n    AN[\"alert_notifications\"]\n  end\n\n  %% Delivery\n  subgraph DELIV[\"Delivery Channels\"]\n    EM[\"Email\"]\n    SM[\"SMS\"]\n    PS[\"Push\"]\n    WH[\"Webhook\"]\n  end\n\n  %% External Source\n  subgraph EXT[\"External\"]\n    TS[\"Transaction Source\"]\n  end\n\n  %% Rule authoring\n  WUI -->|Create/Update Rule| API_APP\n  API_APP -->|Persist| AR\n\n  %% Transaction ingestion\n  TS --> IN\n  IN --> API_APP\n  API_APP -->|Store| TRX\n\n  %% Evaluation path\n  API_APP -->|Evaluate| EV\n  EV -->|Read| AR\n  EV -->|Read| TRX\n  EV -->|Create| AN\n  EV -->|Dispatch| AL\n\n  %% Alerts delivery\n  AL -->|Update| AN\n  AL --> EM\n  AL --> SM\n  AL --> PS\n  AL --> WH\n```\n\n## Getting started\n\nPrerequisites: Node 18+, pnpm 9+, Python 3.11+, uv, Podman (preferred) or Docker\n\nInstall\n```bash\npnpm setup\n```\n\n## üê≥ Container Deployment (Recommended)\n\n### üöÄ Quick Start with Podman Compose\n\n**Start with pre-built images:**\n```bash\nmake run-local\n```\n\n**Build and run from source:**\n```bash\nmake build-run-local\n```\n\n\n**Container URLs:**\n- Frontend: http://localhost:3000\n- API: http://localhost:3000/api/* (proxied)\n- API Docs: http://localhost:8000/docs\n- SMTP Web UI: http://localhost:3002\n- Database: localhost:5432\n\n**Container Management:**\n```bash\nmake run-local      # Start with registry images\nmake build-local    # Build images from source\nmake build-run-local # Build and start\nmake stop-local     # Stop all services\nmake logs-local     # View service logs\nmake reset-local    # Reset with fresh data\n```\n\n## üß™ Testing Alert Rules\n\nAfter starting the application with `make run-local`, you can test alert rules interactively:\n\n### **Interactive Alert Rule Testing**\n\n**List available sample alert rules:**\n\n```bash\nmake list-alert-samples\n```\n\nShows all available test scenarios with their descriptions, such as:\n\n- \"Alert when spending more than $500 in one transaction\"\n- \"Alert me if my dining expense exceeds the average of the last 30 days by more than 40%\"\n- \"Alert me if a transaction happens outside my home state\"\n\n**Interactive testing menu:**\n\n```bash\nmake test-alert-rules\n```\n\nThis command provides:\n\n- üìã **Clean menu** showing only alert rule descriptions (no technical filenames)\n- üìä **Data preview** with realistic transaction data adjusted to current time\n- üîç **User context** showing the test user profile and transaction history\n- ‚úÖ **Confirmation prompt** before running the actual test\n\n### **Example Workflow**\n\n1. **Start the application:**\n\n   ```bash\n   make run-local\n   ```\n\n2. **Browse available test scenarios:**\n\n   ```bash\n   make list-alert-samples\n   ```\n\n3. **Run interactive testing:**\n\n   ```bash\n   make test-alert-rules\n   ```\n\n   - Select an alert rule by number (1-16)\n   - Review the data preview showing exactly what will be tested\n   - Confirm to proceed with the test\n   - Watch the complete validation and creation process\n\n### **What the Test Does**\n\nThe test process:\n\n1. **Seeds database** with realistic user and transaction data\n2. **Validates the alert rule** using the NLP validation API\n3. **Creates the alert rule** if validation passes\n4. **Shows step-by-step results** including SQL queries and processing steps\n\n**Note:** Make sure the API server is running (`make run-local`) before testing alert rules.\n\n### ‚òÅÔ∏è OpenShift Deployment\n\n**Quick Deploy:**\n```bash\nmake full-deploy\n```\n\n**Step-by-step:**\n```bash\n# Login and setup\nmake login\nmake create-project\n\n# Build and push images\nmake build-all\nmake push-all\n\n# Deploy\nmake deploy\n```\n\n**OpenShift Management:**\n```bash\nmake deploy           # Deploy to OpenShift\nmake deploy-dev       # Deploy in development mode\nmake undeploy         # Remove deployment\nmake status           # Check deployment status\nmake logs-api         # View API logs\nmake logs-ui          # View UI logs\n```\n\n**Port Forwarding (for development):**\n```bash\nmake port-forward-api  # Forward API to localhost:8000\nmake port-forward-ui   # Forward UI to localhost:8080\nmake port-forward-db   # Forward DB to localhost:5432\n```\n\n## üîß Local Development Mode\n\n**üöÄ Start Development Mode** (starts DB, API, UI with auth bypassed)\n```bash\npnpm dev\n```\n\nThis command:\n- Starts PostgreSQL database\n- Starts FastAPI backend on port 8000\n- Starts React UI on port 5173\n- **Automatically enables development mode auth bypass** üîì\n\nThe UI will show a yellow banner: \"üîì Development Mode - Auth Bypassed\" when running.\n\n**Backend-only development** (API + Database with test data)\n```bash\npnpm dev:backend     # Complete backend setup + start\npnpm backend:setup   # Setup only (DB + migrations + seed)\npnpm backend:start   # Start API server only (port 8002)\npnpm backend:stop    # Stop database\n```\n\nCommon tasks\n\n```bash\npnpm build\npnpm test\npnpm lint\npnpm format\npnpm db:revision\npnpm db:verify\nmake test-alert-rules    # Interactive alert rule testing  \nmake list-alert-samples  # List available test scenarios\n```\n\n**Local Dev URLs:**\n- Web UI: http://localhost:5173 (shows dev mode banner)\n- API (full stack): http://localhost:8000 (auth bypass enabled)\n- API (backend-only): http://localhost:8002\n- API Docs: http://localhost:8000/docs\n- Component Storybook: http://localhost:6006\n\n### üîß Development Mode (Authentication Bypass)\n\n**Quick Start for New Developers:**\n```bash\ngit clone <repo>\npnpm setup\npnpm dev          # üîì Auth automatically bypassed in development\n```\nVisit http://localhost:5173 - you'll see a yellow banner and can use the app immediately as \"John Doe\" without any authentication setup.\n\n**How It Works:**\n- **Purpose**: Skip OAuth2/OIDC setup for faster development\n- **Auto-enabled**: When `VITE_ENVIRONMENT=development` (default)\n- **Visual indicator**: üîì Yellow banner: \"Development Mode - Auth Bypassed\"\n- **Mock user**: Automatically signed in as \"John Doe\" with admin roles\n- **Security**: Only works in development builds, never in production\n\n**Environment Controls:**\n```bash\n# Default: Auth bypass enabled (recommended for development)\npnpm dev\n\n# To test real Keycloak authentication in development:\nVITE_BYPASS_AUTH=false pnpm dev\n\n# Production: Auth always required (automatic)\nVITE_ENVIRONMENT=production\n```\n\n**Troubleshooting:**\n- **No dev banner?** Check console for \"Development auth provider initialized\"  \n- **Customize mock user:** Edit `packages/ui/src/constants/auth.ts`\n- **Test real auth:** See [`docs/auth/INTEGRATION.md`](docs/auth/INTEGRATION.md) for Keycloak setup\n\n## üîß Environment Variables Configuration\n\n### Required Environment Variables\n\nThe application requires these environment variables for proper operation:\n\n#### Database Configuration\n```bash\nDATABASE_URL=postgresql+asyncpg://user:password@host:port/database\n```\n\n#### SMTP Configuration (for email alerts)\n```bash\nSMTP_HOST=smtp.example.com\nSMTP_PORT=2525\nSMTP_USERNAME=\nSMTP_PASSWORD=\nSMTP_FROM_EMAIL=alerts@spending-monitor.com\nSMTP_USE_TLS=false\nSMTP_USE_SSL=false\n```\n\n#### API Configuration\n```bash\nENVIRONMENT=development|production\nBYPASS_AUTH=true|false\nAPI_PORT=8000\nCORS_ALLOWED_ORIGINS=http://localhost:3000,http://localhost:8080\nALLOWED_ORIGINS=http://localhost:3000,http://localhost:8080\n```\n\n#### LLM Configuration\n```bash\nLLM_PROVIDER=XXXXX\nBASE_URL=XXXXX\nAPI_KEY=sk-your-openai-api-key-here\nMODEL=XXXXX\n```\n\n### üê≥ Local Container Deployment (podman-compose.yml)\n\nEnvironment variables are configured in `podman-compose.yml` under the `api` service:\n\n```yaml\napi:\n  environment:\n    - DATABASE_URL=postgresql+asyncpg://user:password@postgres:5432/spending-monitor\n    - SMTP_HOST=smtp4dev\n    - SMTP_PORT=25\n    - SMTP_USERNAME=\n    - SMTP_PASSWORD=\n    - SMTP_FROM_EMAIL=spending-monitor@localhost\n    - SMTP_USE_TLS=false\n    - SMTP_USE_SSL=false\n    - ENVIRONMENT=development\n    - BYPASS_AUTH=true\n    - API_PORT=8000\n    - CORS_ALLOWED_ORIGINS=http://localhost:3000,http://localhost:8080\n    - ALLOWED_ORIGINS=http://localhost:3000,http://localhost:8080\n    - LLM_PROVIDER=XXXXX\n    - BASE_URL=XXXXX\n    - API_KEY=add-your-openai-api-key-here\n    - MODEL=XXXXX\n```\n\n**To customize for your environment:**\n1. Edit `podman-compose.yml`\n2. Update the environment variables under the `api` service\n3. Restart: `make stop-local && make run-local`\n\n### ‚òÅÔ∏è OpenShift Deployment (values.yaml)\n\nFor OpenShift deployment, configure environment variables in `deploy/helm/spending-monitor/values.yaml`:\n\n```yaml\napi:\n  env:\n    DATABASE_URL: \"postgresql+asyncpg://user:password@spending-monitor-db:5432/spending-monitor\"\n    SMTP_HOST: \"smtp.yourdomain.com\"\n    SMTP_PORT: \"587\"\n    SMTP_USERNAME: \"your-smtp-username\"\n    SMTP_PASSWORD: \"your-smtp-password\"\n    SMTP_FROM_EMAIL: \"alerts@yourdomain.com\"\n    SMTP_USE_TLS: \"true\"\n    SMTP_USE_SSL: \"false\"\n    ENVIRONMENT: \"production\"\n    BYPASS_AUTH: \"false\"\n    API_PORT: \"8000\"\n    CORS_ALLOWED_ORIGINS: \"https://your-ui-route.openshift.com\"\n    ALLOWED_ORIGINS: \"https://your-ui-route.openshift.com\"\n    LLM_PROVIDER: XXXXX\n    BASE_URL: XXXXX\n    API_KEY: \"add-your-openai-api-key-here\"\n    MODEL: XXXXX\n```\n\n**To customize for OpenShift:**\n1. Edit `deploy/helm/spending-monitor/values.yaml`\n2. Update the `api.env` section with your values\n3. Deploy: `make deploy`\n\n**Using OpenShift Secrets (Recommended for production):**\n```yaml\napi:\n  envFrom:\n    - secretRef:\n        name: spending-monitor-secrets\n  env:\n    ENVIRONMENT: \"production\"\n    BYPASS_AUTH: \"false\"\n```\n\n### üîß Local Development Environment Variables\n\nFor local development without containers, set these in your shell or `.env` file:\n\n```bash\n# Development settings\nexport ENVIRONMENT=development\nexport BYPASS_AUTH=true\nexport API_PORT=8000\n\n# Database (when using local PostgreSQL)\nexport DATABASE_URL=\"postgresql+asyncpg://user:password@localhost:5432/spending-monitor\"\n\n# SMTP (using local SMTP server or external service)\nexport SMTP_HOST=localhost\nexport SMTP_PORT=1025\nexport SMTP_FROM_EMAIL=dev@localhost\nexport SMTP_USE_TLS=false\nexport SMTP_USE_SSL=false\n\n# LLM Configuration\nexport LLM_PROVIDER=XXXXX\nexport BASE_URL=XXXXX\nexport API_KEY=add-your-openai-api-key-here\nexport MODEL=XXXXX\n```\n\nManual DB control (optional)\n```bash\npnpm db:start    # podman compose (fallback to docker compose)\npnpm db:upgrade\npnpm db:seed\npnpm db:stop\n```\n\nPython virtual environments\n```bash\n# Each Python package uses uv-managed venvs under the package directory\npnpm --filter @spending-monitor/api install:deps\npnpm --filter @spending-monitor/db install:deps\n```\n\n## Components\n\n- API (`packages/api`): health, users, transactions; async DB session; foundation for rule evaluation and NLP integration\n- DB (`packages/db`): SQLAlchemy models, Alembic migrations, seed/verify; local Postgres via Podman/Docker\n- UI (`packages/ui`): React app and components in Storybook\n\n## Standards\n\n- Conventional Commits; commitlint enforces messages\n- Branch names must match: `feat/*`, `fix/*`, `chore/*`, `docs/*`, `refactor/*`, `test/*`, `ci/*`, `build/*`, `perf/*`\n- Hooks\n  - pre-commit: UI Prettier/ESLint; API Ruff format/check on staged files\n  - pre-push: format:check, lint, test; commitlint on commit range; branch name check\n\n## Releases\n\nAutomated with semantic-release on CI, using commit messages to drive versioning and changelogs. Configuration in `.releaserc`.\n\n## Structure\n\n```\nspending-transaction-monitor/\n‚îú‚îÄ‚îÄ packages/\n‚îÇ   ‚îú‚îÄ‚îÄ api/\n‚îÇ   ‚îú‚îÄ‚îÄ db/\n‚îÇ   ‚îú‚îÄ‚îÄ ui/\n‚îÇ   ‚îú‚îÄ‚îÄ ingestion-service/\n‚îÇ   ‚îî‚îÄ‚îÄ configs/\n‚îú‚îÄ‚îÄ docs/\n‚îú‚îÄ‚îÄ turbo.json\n‚îú‚îÄ‚îÄ pnpm-workspace.yaml\n‚îî‚îÄ‚îÄ package.json\n```\n",
      "readmeFilename": "README.md",
      "images": [],
      "githubLink": "https://github.com/rh-ai-quickstart/spending-transaction-monitor",
      "categories": [
        "Python"
      ],
      "stars": 0,
      "lastUpdated": "9/23/2025",
      "generatedImage": "images/quickstarts/spending-transaction-monitor.svg",
      "imageGenerated": true,
      "imageType": "svg"
    },
    {
      "id": "ai-architecture-charts",
      "title": "Ai Architecture Charts",
      "description": "This repository contains all the helm charts to deploy LLM service, Llama Stack server, configuring pipeline server, minio, pgvector",
      "readmePreview": "# AI Architecture Charts\n\nA comprehensive collection of Helm charts for deploying end-to-end AI/ML infrastructure on OpenShift, featuring LlamaStack o...",
      "readmeContent": "# AI Architecture Charts\n\nA comprehensive collection of Helm charts for deploying end-to-end AI/ML infrastructure on OpenShift, featuring LlamaStack orchestration, model serving, vector databases, and supporting services.\n\n## Overview\n\nThis repository provides production-ready Helm charts for building AI applications with components that work seamlessly together. The architecture supports various AI use cases including RAG (Retrieval-Augmented Generation), conversational AI, document processing, and AI agent workflows.\n\n## Architecture Components\n\n### üß† Core AI Services\n\n#### [LlamaStack](./llama-stack/README.md)\nComprehensive AI orchestration platform that provides a unified API for multiple model providers, safety shields, and AI agent capabilities. Supports local models (via LLM Service), remote vLLM endpoints, and VertexAI integration.\n\n**Key Features:**\n- Multi-provider model support (local, remote, VertexAI)\n- Safety shields with Llama Guard and other safety models\n- AI agent capabilities with persistent memory\n- Automatic model discovery and URL generation\n\n#### [LLM Service](./llm-service/README.md)\nHigh-performance model serving infrastructure using vLLM runtime with OpenShift AI/KServe integration. Supports GPU and CPU deployment modes with any models compatible with vLLM.\n\n**Key Features:**\n- vLLM-based model serving with OpenAI-compatible API\n- Support for any vLLM-compatible models and sizes\n- GPU/CPU deployment flexibility\n- Tool calling and function execution support\n\n### üìä Data & Storage Services\n\n#### [PGVector](./pgvector/README.md)\nPostgreSQL with pgvector extension providing high-performance vector database capabilities for storing and querying embeddings in AI/ML applications.\n\n**Key Features:**\n- Vector similarity search (cosine, L2, inner product)\n- Multiple index types (IVFFlat, HNSW)\n- Support for various embedding dimensions\n- ACID compliance with PostgreSQL reliability\n\n#### [MinIO](./minio/README.md)\nS3-compatible object storage server for documents, models, and data in AI/ML pipelines. Provides scalable storage with web console management.\n\n**Key Features:**\n- S3-compatible API\n- Web-based management console\n- Bucket policies and lifecycle management\n- Sample file upload functionality\n\n#### [Oracle 23ai](./oracle23ai/README.md)\nOracle Database Free 23ai with AI Vector features, providing enterprise-grade database capabilities with built-in vector operations for AI applications.\n\n**Key Features:**\n- Native vector operations and similarity search\n- JSON duality and graph analytics\n- Enterprise database reliability\n- AI-optimized storage and indexing\n\n**TPC-DS Data Population Job:**\nThe Oracle 23ai chart includes an automated TPC-DS (Transaction Processing Performance Council Decision Support) data population job that creates comprehensive test datasets for AI/ML applications. This Kubernetes Job replaces manual database setup scripts with a cloud-native approach:\n\n- **Purpose**: Automatically populates the Oracle database with standardized TPC-DS benchmark data (25 tables with synthetic retail/e-commerce data)\n- **Scale Factor**: Configurable data volume (default generates ~1GB of test data)\n- **Schema Management**: Creates both SYSTEM and Sales schemas with proper data distribution\n- **Security**: Applies read-only restrictions to the Sales user for safe AI MCP server integration\n- **Automation**: Eliminates manual database setup, ensuring consistent test data across deployments\n- **Integration**: Provides realistic datasets for RAG applications, vector search testing, and AI agent development\n\nThe job runs automatically when `tpcds.enabled=true` and handles the complete lifecycle from database readiness verification to data loading and security configuration.\n\n### üîß Pipeline & Processing Services\n\n#### [Ingestion Pipeline](./ingestion-pipeline/README.md)\nComprehensive data ingestion pipeline that processes documents from various sources (S3, GitHub, URLs) and stores vector embeddings for semantic search and RAG applications.\n\n**Key Features:**\n- Multi-source data ingestion (S3/MinIO, GitHub, URLs)\n- Document chunking and embedding generation\n- REST API for pipeline management\n- Integration with vector databases\n\n#### [Configure Pipeline](./configure-pipeline/README.md)\nJupyter notebook environment for RAG configuration and pipeline setup. Provides interactive tools for configuring and testing AI pipelines.\n\n**Key Features:**\n- Pre-configured Jupyter environment\n- RAG pipeline configuration tools\n- MinIO integration for data access\n- Template and configuration management\n\n### üîå Integration & Tools\n\n#### [MCP Servers](./mcp-servers/README.md)\nModel Context Protocol servers that provide external tools and capabilities to AI models, enabling AI agents to interact with external systems and APIs.\n\n**Key Features:**\n- Weather information services\n- Server-Sent Events (SSE) endpoints\n- Custom tool development framework\n- Integration with LlamaStack agents\n\n#### [Oracle SQLcl MCP](./oracle-sqlcl/helm/README.md)\nMCP server that exposes Oracle SQLcl capabilities to AI agents via Toolhive, enabling database tooling and interactions from LlamaStack and compatible clients.\n\n**Key Features:**\n- Execute SQL/PLSQL against Oracle databases via Model Context Protocol\n- Integrates with Toolhive Operator (CRDs and operator managed as chart dependencies)\n- Orale connection managed via Kubernetes secrets and configurable service\n\n## Quick Start\n\n### Prerequisites\n\n- OpenShift cluster\n- Helm 3.x\n- Sufficient storage and compute resources\n- Access to container registries\n\n### Basic Deployment\n\nDeploy a complete AI stack with vector storage and model serving:\n\n```bash\n# 1. Deploy vector database\nhelm install pgvector ./pgvector/helm\n\n# 2. Deploy object storage\nhelm install minio ./minio/helm\n\n# 3. Deploy model serving\nhelm install llm-service ./llm-service/helm \\\n  --set models.llama-3-2-3b-instruct.enabled=true\n\n# 4. Deploy LlamaStack orchestration\nhelm install llama-stack ./llama-stack/helm \\\n  --set models.llama-3-2-3b-instruct.enabled=true\n\n# 5. Deploy ingestion pipeline\nhelm install ingestion-pipeline ./ingestion-pipeline/helm\n```\n\n### RAG Application Setup\n\nFor document processing and retrieval-augmented generation:\n\n```bash\n# Deploy storage and database\nhelm install minio ./minio/helm \\\n  --set sampleFileUpload.enabled=true\nhelm install pgvector ./pgvector/helm\n\n# Configure pipeline\nhelm install configure-pipeline ./configure-pipeline/helm\n\n# Deploy processing pipeline\nhelm install ingestion-pipeline ./ingestion-pipeline/helm \\\n  --set defaultPipeline.enabled=true \\\n  --set defaultPipeline.source=S3\n\n# Deploy model serving and orchestration\nhelm install llm-service ./llm-service/helm \\\n  --set models.llama-3-2-3b-instruct.enabled=true\nhelm install llama-stack ./llama-stack/helm \\\n  --set models.llama-3-2-3b-instruct.enabled=true\n```\n\n## Integration Patterns\n\n### LlamaStack + LLM Service\nLlamaStack provides orchestration while LLM Service handles model inference:\n- LLM Service deploys models as InferenceServices\n- LlamaStack automatically discovers and configures model endpoints\n- Unified API access through LlamaStack\n\n### Vector Storage Integration\nBoth PGVector and Oracle 23ai can serve as vector databases:\n- PGVector: Open-source PostgreSQL with pgvector extension\n- Oracle 23ai: Enterprise database with native AI vector features\n- Choose based on performance, compliance, and feature requirements\n\n### Multi-Source Data Ingestion\nIngestion Pipeline supports various data sources:\n- **S3/MinIO**: Object storage for documents and files\n- **GitHub**: Repository documentation and code\n- **URLs**: Direct document links and web content\n\n## Component Dependencies\n\n```mermaid\ngraph TB\n    LS[LlamaStack] --> LLM[LLM Service]\n    LS --> PG[PGVector]\n    LS --> MCP[MCP Servers]\n    \n    IP[Ingestion Pipeline] --> MINIO[MinIO]\n    IP --> PG\n    IP --> LS\n    \n    CP[Configure Pipeline] --> MINIO\n    CP --> IP\n    \n    LLM --> GPU[GPU Nodes]\n    PG --> STORAGE[Persistent Storage]\n    MINIO --> STORAGE\n```\n\n## Security Considerations\n\n- **Secrets Management**: All components use Kubernetes secrets for credentials\n- **Network Policies**: Implement network policies to restrict inter-component communication\n- **RBAC**: Configure role-based access control for service accounts\n- **TLS**: Enable TLS for external access and sensitive communications\n- **Safety Shields**: Use Llama Guard or other safety models for content moderation\n\n## Monitoring and Observability\n\n- **Prometheus Integration**: Many components support Prometheus metrics\n- **OpenTelemetry**: LlamaStack supports distributed tracing\n- **Logging**: All components provide structured logging\n- **Health Checks**: Kubernetes-native health and readiness probes\n\n## Development and Customization\n\nEach component is designed to be:\n- **Configurable**: Extensive values.yaml configuration options\n- **Extensible**: Support for custom models, tools, and integrations\n- **Scalable**: Horizontal and vertical scaling capabilities\n- **Production-ready**: Comprehensive monitoring and operational features\n\n### Container Image Building\n\nThe repository includes automated container image building through GitHub workflows:\n\n- **Supported components**: ingestion-pipeline and mcp-servers support building custom container images\n- **Automated publishing**: Images are built and pushed to Quay.io with chart versioning\n- **Custom development**: Add new MCP servers or modify existing components with automatic CI/CD\n- **Workflow integration**: Components with source code are automatically built when chart versions are updated\n\nTo add a new buildable component, place your source code and Containerfile in the appropriate component directory and configure the GitHub workflow matrix in `.github/workflows/publish-helm-charts.yaml`.\n\n### Helm Repository and Versioning\n\nThis project maintains a Helm repository at `https://rh-ai-quickstart.github.io/ai-architecture-charts` with full version history:\n\n- **Version tracking**: All chart versions are preserved and available for download\n- **Automated publishing**: GitHub workflow automatically packages and publishes charts to the repository\n- **Backward compatibility**: Previous versions remain accessible for rollbacks and compatibility\n- **Index management**: Helm repository index is automatically maintained with each release\n\n## Support and Documentation\n\n- **Component READMEs**: Detailed documentation for each Helm chart\n- **Configuration Examples**: Real-world configuration patterns\n- **Troubleshooting Guides**: Common issues and solutions\n- **Integration Examples**: How components work together\n\n## Using Charts as Dependencies\n\nThese charts can be used standalone or as dependencies in larger AI applications. Each chart is designed to work independently or as part of a composed solution.\n\n### Standalone Deployment\n\nEach chart can be deployed individually:\n\n```bash\n# Deploy individual components\nhelm install pgvector ./pgvector/helm\nhelm install minio ./minio/helm\nhelm install llm-service ./llm-service/helm\n```\n\n### Chart Dependencies\n\nReference these charts as dependencies in your own Chart.yaml without requiring a Helm repository:\n\n```yaml\n# Chart.yaml for your AI application\napiVersion: v2\nname: my-ai-application\nversion: 1.0.0\n\ndependencies:\n  - name: pgvector\n    version: \"0.1.0\"\n    repository: \"file://../ai-architecture-charts/pgvector/helm\"\n  \n  - name: minio\n    version: \"0.1.0\"\n    repository: \"file://../ai-architecture-charts/minio/helm\"\n  \n  - name: llm-service\n    version: \"0.1.0\"\n    repository: \"file://../ai-architecture-charts/llm-service/helm\"\n  \n  - name: llama-stack\n    version: \"0.2.18\"\n    repository: \"file://../ai-architecture-charts/llama-stack/helm\"\n  \n  - name: ingestion-pipeline\n    version: \"0.2.18\"\n    repository: \"file://../ai-architecture-charts/ingestion-pipeline/helm\"\n```\n\n### Configuring Subcharts\n\nConfigure the dependent charts in your values.yaml:\n\n```yaml\n# values.yaml for your AI application\npgvector:\n  secret:\n    dbname: \"my_ai_app_vectors\"\n  extraDatabases:\n    - name: agent_memory\n      vectordb: true\n\nminio:\n  secret:\n    user: \"ai_app_user\"\n    password: \"secure_password\"\n  sampleFileUpload:\n    enabled: true\n    bucket: \"ai-documents\"\n\nllm-service:\n  models:\n    llama-3-2-3b-instruct:\n      enabled: true\n    llama-guard-3-8b:\n      enabled: true\n\nllama-stack:\n  models:\n    llama-3-2-3b-instruct:\n      enabled: true\n    llama-guard-3-8b:\n      enabled: true\n      registerShield: true\n\ningestion-pipeline:\n  defaultPipeline:\n    enabled: true\n    source: S3\n    S3:\n      bucket_name: ai-documents\n      endpoint_url: http://minio:9000\n```\n\n### Shared Configuration with Global Values\n\nUse global values to configure multiple charts simultaneously, reducing duplication:\n\n```yaml\n# values.yaml for your AI application\nglobal:\n  models:\n    llama-3-2-3b-instruct:\n      enabled: true\n    llama-guard-3-8b:\n      enabled: true\n      registerShield: true\n  \n  mcp-servers:\n    mcp-weather:\n      deploy: true\n\n# MCP Servers Configuration Example\n# To enable and configure MCP servers, add the following to your values.yaml:\nmcp-servers:\n  toolhive:\n    crds:\n      enabled: false  # Set to false if CRDs already exist\n    operator:\n      enabled: false  # Set to false if operator already installed\n\n  mcp-servers:\n    mcp-weather:\n      mcpserver:\n        enabled: true\n        env:\n          TAVILY_API_KEY: \"\"  # Add your API key if needed\n        permissionProfile:\n          name: network\n          type: builtin\n\n    oracle-sqlcl:\n      mcpserver:\n        enabled: true\n        env:\n          ORACLE_USER: \"sales\"  # Sales schema user created by Oracle DB chart\n          ORACLE_PASSWORD: null  # Sourced from secret\n          ORACLE_CONNECTION_STRING: null  # Sourced from secret\n          ORACLE_CONN_NAME: \"oracle_connection\"\n        envSecrets:\n          ORACLE_PASSWORD:\n            name: oracle23ai\n            key: password\n          ORACLE_CONNECTION_STRING:\n            name: oracle23ai\n            key: jdbc-uri\n        permissionProfile:\n          name: network\n          type: builtin\n\n# Individual chart configurations\npgvector:\n  secret:\n    dbname: \"my_ai_app_vectors\"\n\nminio:\n  sampleFileUpload:\n    enabled: true\n    bucket: \"ai-documents\"\n\n# Global models will be merged with local configurations\n# Both llm-service and llama-stack will use the global.models settings\n```\n\n### Deployment Workflow\n\n```bash\n# 1. Update dependencies\nhelm dependency update\n\n# 2. Deploy complete AI stack\nhelm install my-ai-app . \\\n  --namespace my-ai-app \\\n  --create-namespace\n\n# 3. Verify deployment\nhelm list -n my-ai-app\n```\n\nThis approach allows you to compose comprehensive AI applications by combining these foundational charts with your own application-specific components while minimizing configuration duplication.\n\n## Contributing\n\nWhen contributing to this repository:\n1. Follow OpenShift best practices\n2. Update component READMEs for any changes\n3. Test integrations between components\n4. Ensure security and operational standards\n\n",
      "readmeFilename": "README.md",
      "images": [],
      "githubLink": "https://github.com/rh-ai-quickstart/ai-architecture-charts",
      "categories": [
        "Python"
      ],
      "stars": 7,
      "lastUpdated": "9/22/2025",
      "generatedImage": "images/quickstarts/ai-architecture-charts.svg",
      "imageGenerated": true,
      "imageType": "svg"
    },
    {
      "id": "RAG",
      "title": "RAG",
      "description": "No description available",
      "readmePreview": "# RAG Reference Architecture using LLaMA Stack, OpenShift AI, and PGVector\n\n## Description\n\nRetrieval-Augmented Generation (RAG) enhances Large Langua...",
      "readmeContent": "# RAG Reference Architecture using LLaMA Stack, OpenShift AI, and PGVector\n\n## Description\n\nRetrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by retrieving relevant external knowledge to improve accuracy, reduce hallucinations, and support domain-specific conversations. This architecture uses:\n\n- **OpenShift AI** for orchestration\n- **LLaMA Stack** for standardizing the core building blocks and simplifying AI application development\n- **PGVector** for semantic search\n- **Kubeflow Pipelines** for data ingestion\n- **Streamlit UI** for a user-friendly chatbot interface\n\n\n---\n\n## Architecture Diagram\n\n![RAG System Architecture](images/RAG_rag-architecture.png)\n\n*The architecture illustrates both the ingestion pipeline for document processing and the RAG pipeline for query handling. For more details click [here](docs/rag-reference-architecture.md).*\n\n---\n\n## Features\n\n- Multi-Modal Data Ingestion for ingesting unstructured data\n- Preprocessing pipelines for cleaning, chunking, and embedding generation using language models\n- Vector Store Integration to store dense embeddings\n- Integrates with LLMs to generate responses based on retrieved documents\n- Streamlit based web application\n- Runs on OpenShift AI for container orchestration and GPU acceleration\n- Llama Stack to standardize the core building blocks and simplify AI application development\n- Safety Guardrail to block harmful request / response\n- Integration with MCP servers\n\n---\n\n## Ingestion Use Cases\n\n### 1. BYOD (Bring Your Own Document)\n\nEnd users can upload files through a UI and receive contextual answers based on uploaded content.\n\n### 2. Pre-Ingestion\n\nEnterprise documents are pre-processed and ingested into the system for later querying via OpenShift AI/Kubeflow Pipelines.\n\n---\n\n## Key Components\n\n| Layer            | Component                      | Description |\n|------------------|--------------------------------|-------------|\n| **UI Layer**     | Streamlit / React              | Chat-based user interaction |\n| **Retrieval**    | Retriever                      | Vector search |\n| **Embedding**    | `all-MiniLM-L6-v2`             | Converts text to vectors |\n| **Vector DB**    | PostgreSQL + PGVector          | Stores embeddings |\n| **LLM**          | `Llama-3.2-3B-Instruct`        | Generates responses |\n| **Ingestor**     |  Kubeflow Pipeline             | Embeds documents and stores vectors |\n| **Storage**      |  S3 Bucket                     | Document source |\n\n---\n\n## Scalability & Performance\n\n- KServe for auto-scaling the model and embedding pods\n- GPU-based inference optimized using node selectors\n- Horizontal scaling of ingestion and retrieval components\n\n---\n\nThe quickstart supports two modes of deployments\n\n- Local\n- Openshift\n\n## OpenShift Installation\n\n### Minimum Requirements\n\n- OpenShift Cluster 4.16+ with OpenShift AI\n- OpenShift Client CLI - [oc](https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html/cli_tools/openshift-cli-oc#installing-openshift-cli)\n- Helm CLI - helm\n- [huggingface-cli](https://huggingface.co/docs/huggingface_hub/guides/cli) (optional)\n- 1 GPU/HPU with 24GB of VRAM for the LLM, refer to the chart below\n- 1 GPU/HPU with 24GB of VRAM for the safety/shield model (optional)\n- [Hugging Face Token](https://huggingface.co/settings/tokens)\n- Access to [Meta Llama](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/) model.\n- Access to [Meta Llama Guard](https://huggingface.co/meta-llama/Llama-Guard-3-8B/) model.\n- Some of the example scripts use `jq` a JSON parsing utility which you can acquire via `brew install jq`\n\n### Supported Models\n\n| Function    | Model Name                             | Hardware    | AWS\n|-------------|----------------------------------------|-------------|-------------\n| Embedding   | `all-MiniLM-L6-v2`                     | CPU/GPU/HPU |\n| Generation  | `meta-llama/Llama-3.2-3B-Instruct`     | L4/HPU      | g6.2xlarge\n| Generation  | `meta-llama/Llama-3.1-8B-Instruct`     | L4/HPU      | g6.2xlarge\n| Generation  | `meta-llama/Meta-Llama-3-70B-Instruct` | A100 x2/HPU | p4d.24xlarge\n| Safety      | `meta-llama/Llama-Guard-3-8B`          | L4/HPU      | g6.2xlarge\n\nNote: the 70B model is NOT required for initial testing of this example.  The safety/shield model `Llama-Guard-3-8B` is also optional. \n\n---\n\n#### Installation steps\n\n1. Clone the repo so you have a working copy\n\n```bash\ngit clone https://github.com/rh-ai-quickstart/RAG\n```\n\n2. Login to your OpenShift Cluster\n\n```bash\noc login --server=\"<cluster-api-endpoint>\" --token=\"sha256~XYZ\"\n```\n\n3. **Hardware Configuration**: Determine what hardware acceleration is available in your cluster and configure accordingly.\n\n   **For NVIDIA GPU nodes**: If GPU nodes are tainted, find the taint key. In the example below the key for the taint is `nvidia.com/gpu`\n\n   ```bash\n   oc get nodes -l nvidia.com/gpu.present=true -o yaml | grep -A 3 taint \n   ```\n   \n   **For Intel Gaudi HPU nodes**: If HPU nodes are tainted, find the taint key. The taint key is typically `habana.ai/gaudi`\n\n   ```bash\n   oc get nodes -l habana.ai/gaudi.present=true -o yaml | grep -A 3 taint \n   ```\n   \n   The output of either command may be something like below:\n   ```\n   taints:\n     - effect: NoSchedule\n       key: nvidia.com/gpu  # or habana.ai/gaudi for HPU\n       value: \"true\"\n   ```\n\n   You can work with your OpenShift cluster admin team to determine what labels and taints identify GPU-enabled or HPU-enabled worker nodes. It is also possible that all your worker nodes have accelerators therefore have no distinguishing taint.\n\n4. Navigate to Helm deploy directory\n\n```bash\ncd deploy/helm\n```\n\n5. List available models\n\n```bash\nmake list-models\n```\n\nThe above command will list the models to use in the next command\n\n```bash\n(Output)\nmodel: llama-3-1-8b-instruct (meta-llama/Llama-3.1-8B-Instruct)\nmodel: llama-3-2-1b-instruct (meta-llama/Llama-3.2-1B-Instruct)\nmodel: llama-3-2-1b-instruct-quantized (RedHatAI/Llama-3.2-1B-Instruct-quantized.w8a8)\nmodel: llama-3-2-3b-instruct (meta-llama/Llama-3.2-3B-Instruct)\nmodel: llama-3-3-70b-instruct (meta-llama/Llama-3.3-70B-Instruct)\nmodel: llama-guard-3-1b (meta-llama/Llama-Guard-3-1B)\nmodel: llama-guard-3-8b (meta-llama/Llama-Guard-3-8B)\n```\n\nThe \"guard\" models can be used to test shields for profanity, hate speech, violence, etc.\n\n6. Install via make\n\nUse the taint key from above as the `LLM_TOLERATION` and `SAFETY_TOLERATION`\n\nThe namespace will be auto-created\n\n**GPU Deployment Examples (Default):**\n\nTo install only the RAG example, no shields, use the following command:\n\n```bash\nmake install NAMESPACE=llama-stack-rag LLM=llama-3-2-3b-instruct LLM_TOLERATION=\"nvidia.com/gpu\"\n```\n\nTo install both the RAG example as well as the guard model to allow for shields, use the following command:\n\n```bash\nmake install NAMESPACE=llama-stack-rag LLM=llama-3-2-3b-instruct LLM_TOLERATION=\"nvidia.com/gpu\" SAFETY=llama-guard-3-8b SAFETY_TOLERATION=\"nvidia.com/gpu\"\n```\n\n*Note: `DEVICE=gpu` is the default and can be omitted.*\n\n**Intel Gaudi HPU Deployment Examples:**\n\nTo install only the RAG example on Intel Gaudi HPU nodes:\n\n```bash\nmake install NAMESPACE=llama-stack-rag LLM=llama-3-2-3b-instruct LLM_TOLERATION=\"habana.ai/gaudi\" DEVICE=hpu\n```\n\nTo install both the RAG example and guard model on Intel Gaudi HPU nodes:\n\n```bash\nmake install NAMESPACE=llama-stack-rag LLM=llama-3-2-3b-instruct LLM_TOLERATION=\"habana.ai/gaudi\" SAFETY=llama-guard-3-8b SAFETY_TOLERATION=\"habana.ai/gaudi\" DEVICE=hpu\n```\n\n**CPU Deployment Example:**\n\nTo install on CPU nodes only:\n\n```bash\nmake install NAMESPACE=llama-stack-rag LLM=llama-3-2-3b-instruct DEVICE=cpu\n```\n\n**Simplified Commands (No Tolerations Needed):**\n\nIf you have no tainted nodes (all worker nodes have accelerators), you can use simplified commands:\n\n```bash\n# GPU deployment (default - DEVICE=gpu can be omitted)\nmake install NAMESPACE=llama-stack-rag LLM=llama-3-2-3b-instruct SAFETY=llama-guard-3-8b\n\n# HPU deployment  \nmake install NAMESPACE=llama-stack-rag LLM=llama-3-2-3b-instruct SAFETY=llama-guard-3-8b DEVICE=hpu\n\n# CPU deployment\nmake install NAMESPACE=llama-stack-rag LLM=llama-3-2-3b-instruct SAFETY=llama-guard-3-8b DEVICE=cpu\n```\n\nWhen prompted, enter your **[Hugging Face Token]((https://huggingface.co/settings/tokens))**.\n\nNote: This process may take 10 to 30 minutes depending on the number and size of models to be downloaded. \n\n7. Watch/Monitor\n\n```bash\noc get pods -n llama-stack-rag\n```\n\n```\n(Output)\nNAME                                                               READY   STATUS      RESTARTS   AGE\ndemo-rag-vector-db-v1-0-8mkf9                                      0/1     Completed   0          10m\nds-pipeline-dspa-7788689675-9489m                                  2/2     Running     0          10m\nds-pipeline-metadata-envoy-dspa-948676f89-8knw8                    2/2     Running     0          10m\nds-pipeline-metadata-grpc-dspa-7b4bf6c977-cb72m                    1/1     Running     0          10m\nds-pipeline-persistenceagent-dspa-ff9bdfc76-ngddb                  1/1     Running     0          10m\nds-pipeline-scheduledworkflow-dspa-7b64d87fd8-58d87                1/1     Running     0          10m\nds-pipeline-workflow-controller-dspa-5799548b68-bxpdp              1/1     Running     0          10m\nfetch-and-store-pipeline-tmxwj-system-container-driver-287597120   0/2     Completed   0          3m43s\nfetch-and-store-pipeline-tmxwj-system-container-driver-922184592   0/2     Completed   0          2m54s\nfetch-and-store-pipeline-tmxwj-system-container-impl-3210250134    0/2     Completed   0          4m33s\nfetch-and-store-pipeline-tmxwj-system-container-impl-3248801382    0/2     Completed   0          3m32s\nfetch-and-store-pipeline-tmxwj-system-dag-driver-3443954210        0/2     Completed   0          4m6s\nllama-3-2-3b-instruct-predictor-00001-deployment-6bbf96f8674677    3/3     Running     0          10m\nllamastack-6d5c5b999b-5lffb                                        1/1     Running     0          11m\nmariadb-dspa-74744d65bd-fdxjd                                      1/1     Running     0          10m\nminio-0                                                            1/1     Running     0          10m\nminio-dspa-7bb47d68b4-nvw7t                                        1/1     Running     0          10m\npgvector-0                                                         1/1     Running     0          10m\nrag-7fd7b47844-nlfvr                                               1/1     Running     0          11m\nrag-mcp-weather-9cc97d574-nf5q8                                    1/1     Running     0          11m\nrag-pipeline-notebook-0                                            2/2     Running     0          10m\nupload-sample-docs-job-f5k5w                                       0/1     Completed   0          10m\n```\n\n8. Verify:\n\n```bash\noc get pods -n llama-stack-rag\noc get svc -n llama-stack-rag\noc get routes -n llama-stack-rag\n```\n\nNote: The key pods to watch include **predictor** in their name, those are the kserve model servers running vLLM\n\n```bash\noc get pods -l component=predictor\n```\n\nLook for **3/3** under the Ready column\n\nThe **inferenceservice** CR describes the limits, requests, model name, serving-runtime, chat-template, etc. \n\n```bash\noc get inferenceservice llama-3-2-3b-instruct \\\n  -n llama-stack-rag-1 \\\n  -o jsonpath='{.spec.predictor.model}' | jq\n```\n\nWatch the **llamastack** pod as that one becomes available after all the model servers are up.\n\n```bash\n oc get pods -l app.kubernetes.io/name=llamastack\n```\n\n### Using the RAG UI\n\n1. Get the route url for the application and open in your browser\n\n```bash\nURL=http://$(oc get routes -l app.kubernetes.io/name=rag -o jsonpath=\"{range .items[*]}{.status.ingress[0].host}{end}\")\necho $URL\nopen $URL\n```\n\n![RAG UI Main](images/RAG_rag-ui-1.png)\n\n2. Click on **Upload Documents**\n\n3. Upload your PDF document\n\n4. Name and Create a Vector Database\n\n![RAG UI Main 2](images/RAG_rag-ui-2.png)\n\n5. Once you've recieved **Vector database created successfully!**, navigate back to **Chat** and select the newly created vector db.\n\n![RAG UI Main 3](images/RAG_rag-ui-3.png)\n\n6. Ask a question pertaining to your document!\n\n![RAG UI Main 4](images/RAG_rag-ui-4.png)\n\nRefer to the [post installation](docs/post_installation.md) document for batch document ingestion.\n\n## Uninstalling the RAG application\n\n```bash\nmake uninstall NAMESPACE=llama-stack-rag\n```\nor\n\n```bash\noc delete project llama-stack-rag\n```\n\n## Adding a new model\nTo add another model follow these steps:\n\n1. Edit `deploy\\helm\\rag\\values.yaml` \n\n    Update the **global\\models** section\n    ```yaml\n    global:\n      models:\n        granite-vision-3-2-2b:\n          id: ibm-granite/granite-vision-3.2-2b\n          enabled: true      \n          resources:\n            limits:\n              nvidia.com/gpu: \"1\"\n          tolerations:\n          - key: \"nvidia.com/gpu\"\n            operator: Exists\n            effect: NoSchedule\n          args:\n          - --tensor-parallel-size\n          - \"1\"\n          - --max-model-len\n          - \"6144\"\n          - --enable-auto-tool-choice\n          - --tool-call-parser\n          - granite\n        llama-guard-3-8b:\n          id: meta-llama/Llama-Guard-3-8B\n          enabled: true\n          registerShield: true\n          tolerations:\n          - key: \"nvidia.com/gpu\"\n            operator: Exists\n            effect: NoSchedule\n          args:\n          - --max-model-len\n          - \"14336\"\n    ```\n\n  Note: Make sure you have permission to download the models from Huggingface and enough GPUs to support all the models you have requested.  Also **max-model-len** uses additional VRAM therefore you have to scale that parameter to fit your hardware. \n\n2. Run the **make** command again to update the project\n\n    ```bash\n    make install NAMESPACE=llama-stack-rag LLM=llama-3-2-3b-instruct LLM_TOLERATION=\"nvidia.com/gpu\"\n    ```\n\n    ```bash\n    (Output)\n    NAME                                                                READY   STATUS                   RESTARTS      AGE\n    demo-rag-vector-db-v1-0-vz5mf                                       0/1     Completed                0             35m\n    ds-pipeline-dspa-6dcf8c7b8f-vkhw8                                   2/2     Running                  1 (34m ago)   34m\n    ds-pipeline-metadata-envoy-dspa-7659ddc8d9-qvtct                    2/2     Running                  0             34m\n    ds-pipeline-metadata-grpc-dspa-8665cd5c6c-mfrj7                     1/1     Running                  0             34m\n    ds-pipeline-persistenceagent-dspa-56f888bc78-lzq9s                  1/1     Running                  0             34m\n    ds-pipeline-scheduledworkflow-dspa-c94d5c95d-rr8td                  1/1     Running                  0             34m\n    ds-pipeline-workflow-controller-dspa-5799548b68-z2lcl               1/1     Running                  0             34m\n    fetch-and-store-pipeline-w7gxh-system-container-driver-1552269565   0/2     Completed                0             30m\n    fetch-and-store-pipeline-w7gxh-system-container-driver-2057025395   0/2     Completed                0             30m\n    fetch-and-store-pipeline-w7gxh-system-container-impl-1487941461     0/2     Completed                0             30m\n    fetch-and-store-pipeline-w7gxh-system-container-impl-883889707      0/2     Completed                0             29m\n    fetch-and-store-pipeline-w7gxh-system-dag-driver-190510417          0/2     Completed                0             30m\n    granite-vision-3-2-2b-predictor-00001-deployment-5dbcf6f454mrd6     3/3     Running                  0             10m\n    granite-vision-3-2-2b-predictor-00001-deployment-5dbcf6f45xxk5x     0/3     ContainerStatusUnknown   3             13m\n    llama-3-2-3b-instruct-predictor-00001-deployment-6f845f65674ncq     3/3     Running                  0             35m\n    llama-guard-3-8b-predictor-00001-deployment-6cbff4965c-gzx5v        3/3     Running                  0             13m\n    llamastack-7989d974fc-w24fn                                         1/1     Running                  0             13m\n    mariadb-dspa-74744d65bd-kb2dh                                       1/1     Running                  0             35m\n    minio-0                                                             1/1     Running                  0             35m\n    minio-dspa-7bb47d68b4-kb722                                         1/1     Running                  0             35m\n    pgvector-0                                                          1/1     Running                  0             35m\n    rag-7fd7b47844-jkqtf                                                1/1     Running                  0             35m\n    rag-mcp-weather-9cc97d574-s8vpt                                     1/1     Running                  0             35m\n    rag-pipeline-notebook-0                                             2/2     Running                  0             35m\n    upload-sample-docs-job-952gj                                        0/1     Completed                0             35m\n    ```\n\nReturn to the RAG UI and look into the **Inspect** tab to see the additional models and shields. \n\n![RAG UI Main 5](images/RAG_rag-ui-5.png)\n\nThe newly added shield can be tested via the UI by selecting **Agent-based** and Chat\n\n![RAG UI Main 6](images/RAG_rag-ui-6.png)\n\n\n## Local Development Setup\n\nRefer to the [local setup guide](docs/local_setup_guide.md) document for configuring your workstation for code changes and local testing.\n\n\n\n\n",
      "readmeFilename": "README.md",
      "images": [
        {
          "original": "docs/img/rag-architecture.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/RAG/main/docs/img/rag-architecture.png",
          "local": "images/RAG_rag-architecture.png",
          "alt": "RAG System Architecture"
        },
        {
          "original": "./docs/img/rag-ui-1.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/RAG/main/./docs/img/rag-ui-1.png",
          "local": "images/RAG_rag-ui-1.png",
          "alt": "RAG UI Main"
        },
        {
          "original": "./docs/img/rag-ui-2.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/RAG/main/./docs/img/rag-ui-2.png",
          "local": "images/RAG_rag-ui-2.png",
          "alt": "RAG UI Main 2"
        },
        {
          "original": "./docs/img/rag-ui-3.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/RAG/main/./docs/img/rag-ui-3.png",
          "local": "images/RAG_rag-ui-3.png",
          "alt": "RAG UI Main 3"
        },
        {
          "original": "./docs/img/rag-ui-4.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/RAG/main/./docs/img/rag-ui-4.png",
          "local": "images/RAG_rag-ui-4.png",
          "alt": "RAG UI Main 4"
        },
        {
          "original": "./docs/img/rag-ui-5.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/RAG/main/./docs/img/rag-ui-5.png",
          "local": "images/RAG_rag-ui-5.png",
          "alt": "RAG UI Main 5"
        },
        {
          "original": "./docs/img/rag-ui-6.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/RAG/main/./docs/img/rag-ui-6.png",
          "local": "images/RAG_rag-ui-6.png",
          "alt": "RAG UI Main 6"
        }
      ],
      "githubLink": "https://github.com/rh-ai-quickstart/RAG",
      "categories": [
        "Python"
      ],
      "stars": 7,
      "lastUpdated": "9/22/2025",
      "generatedImage": "images/quickstarts/RAG.svg",
      "imageGenerated": true,
      "imageType": "svg"
    },
    {
      "id": "product-recommender-system",
      "title": "Product Recommender System",
      "description": "Red Hat AI quickstart Recommendation System algorithm, for  interactions between users and products in an online store.",
      "readmePreview": "# AI Quickstart - Product Recommender System\n\nWelcome to the Product Recommender System Quickstart!\nUse this to quickly get a recommendation engine wi...",
      "readmeContent": "# AI Quickstart - Product Recommender System\n\nWelcome to the Product Recommender System Quickstart!\nUse this to quickly get a recommendation engine with user-item relationships up and running in your environment.\n\nTo see how it's done, jump straight to [installation](#install).\n\n## üéØ Description\nThe Product Recommender System Quickstart enables the rapid establishment of a scalable and personalized product recommendation service.\n\nThe system recommends items to users based on their previous interactions with products and the behavior of similar users.\n\nIt supports recommendations for both existing and new users. New users are prompted to select their preferences to personalize their experience.\n\nUsers can interact with the user interface to view items, add items to their cart, make purchases, or submit reviews.\n\n### Main Features\nTo find products in the application you can do a:\n* Scroll recommended items.\n* Search items by text (semantic search).\n* Search items by Image (find similar items in the store).\n\n## üìÅ Project Structure\n\nThis repository contains multiple components that work together to create a complete recommendation system:\n\n### üèóÔ∏è Core Components\n\n#### [recommendation-core/](recommendation-core/README.md)\n**ML Library & Models** - The foundational Python library for building recommendation systems\n- **Multi-modal neural networks**: Two-tower architecture for users and items with support for text, image, and numerical features\n- **Feature processing**: Comprehensive data preprocessing for text, image, numerical, and categorical data handling\n- **CLIP integration**: Advanced text and image feature extraction using OpenAI's CLIP model for semantic understanding\n- **Data generation**: Synthetic datasets creation and AI-powered product image generation using Stable Diffusion\n- **Search services**: High-performance text and image-based similarity search with vector embeddings\n- **Model serving**: Real-time inference capabilities for recommendation generation\n- **Data utilities**: Tools for data validation, transformation, and feature engineering\n\n#### [recommendation-training/](recommendation-training/README.md)\n**Training Pipeline** - Kubeflow-based ML training workflows for production-ready model training\n- **Data loading**: Feast feature store integration for consistent feature serving across training and inference\n- **Model training**: Two-tower neural network training with configurable architectures and hyperparameters\n- **Model registration**: Model Registry integration for versioning, tracking, and deployment management\n- **Candidate generation**: Embedding generation and online serving for real-time recommendations\n- **Batch scoring**: Automated recommendation generation and storage for offline serving\n- **Pipeline orchestration**: Kubeflow Pipelines for reproducible ML workflows\n- **Cluster operations**: OpenShift integration for credential management and service discovery\n- **Monitoring**: Training metrics, model performance tracking, and pipeline observability\n\n\n\n### üöÄ Deployment Components\n\n#### [backend/](backend/)\n**FastAPI Backend** - High-performance REST API and business logic layer\n- **User authentication**: JWT-based authentication system with secure token management\n- **Product management**: Complete CRUD operations for products, users, and interactions\n- **Recommendation serving**: Real-time recommendation API with caching and optimization\n- **Feast integration**: Feature store for online serving with real-time feature updates\n- **Static file serving**: Frontend asset delivery and CDN-like functionality\n- **Database integration**: PostgreSQL with pgvector for vector similarity search\n- **API documentation**: Auto-generated OpenAPI/Swagger documentation\n- **Error handling**: Comprehensive error management and logging\n- **Rate limiting**: API rate limiting and request throttling\n- **Health checks**: System health monitoring and status endpoints\n\n#### [frontend/](frontend/)\n**React Frontend** - Modern, responsive user interface and interactions\n- **Product browsing**: Interactive item catalog with filtering and sorting capabilities\n- **User preferences**: Intelligent preference selection for new users with guided onboarding\n- **Shopping cart**: Full e-commerce cart functionality with add/remove items\n- **Recommendation display**: Personalized item recommendations with real-time updates\n- **Image search**: Visual similarity search capabilities with drag-and-drop interface\n- **Text search**: Semantic search with autocomplete and search suggestions\n- **User dashboard**: Personal user dashboard with purchase history and preferences\n- **Responsive design**: Mobile-first design with cross-device compatibility\n- **Performance optimization**: Lazy loading, caching, and optimized bundle delivery\n- **Accessibility**: WCAG compliant interface with screen reader support\n\n#### [helm/](helm/)\n**Kubernetes Deployment** - Complete system orchestration and infrastructure management\n- **Helm charts**: All components packaged for easy deployment with configurable values\n- **Service mesh**: Istio-based traffic management with advanced routing and security\n- **Database setup**: PostgreSQL and pgvector configuration with automated initialization\n- **Monitoring**: OpenShift built-in monitoring stack (Prometheus/Thanos + Grafana) for comprehensive observability\n- **Security**: RBAC, network policies, and security context constraints\n- **Auto-scaling**: Horizontal Pod Autoscaler (HPA) for dynamic resource management\n- **Ingress configuration**: Load balancing and SSL termination setup\n- **Resource management**: CPU and memory limits with resource quotas\n- **Backup and recovery**: Automated backup strategies for data persistence\n- **Multi-environment support**: Development, staging, and production configurations\n\n### üìä Data & Storage\n\n#### [figures/](figures/)\n**Architecture Diagrams** - Comprehensive system design documentation\n- **Data processing pipeline**: End-to-end data flow from ingestion to serving\n- **Training workflow**: Detailed ML model training process and pipeline stages\n- **Inference architecture**: Real-time recommendation serving and caching strategies\n- **Search capabilities**: Text and image search flows with vector similarity\n- **System integration**: Component interaction and data flow visualization\n- **Deployment topology**: Kubernetes resource relationships and networking\n\n### üîß Infrastructure Components\n\n#### [tests/](tests/)\n**Testing Framework** - Comprehensive testing suite for all system components\n- **Unit tests**: Component-level testing for individual functions and classes\n- **Integration tests**: End-to-end testing for component interactions\n- **Performance tests**: Load testing and performance benchmarking\n- **Security tests**: Vulnerability scanning and security validation\n- **API tests**: REST API testing with automated test suites\n- **UI tests**: Frontend testing with automated browser testing\n\n\n\n## üèóÔ∏è Architecture Overview\n\n### System Components\n\nThe recommendation system consists of several interconnected components:\n\n1. **Data Generation** (`recommendation-core/generation/`)\n   - Synthetic user, item, and interaction data with realistic patterns\n   - Product image generation using Stable Diffusion for diverse catalog\n   - Realistic e-commerce dataset creation with configurable parameters\n   - Data validation and quality assurance processes\n\n2. **Feature Store** (`recommendation-core/feature_repo/`)\n   - Feast-based feature management with online/offline serving\n   - Real-time feature updates and versioning\n   - Feature transformation and preprocessing pipelines\n   - Feature monitoring and drift detection\n\n3. **Model Training** (`recommendation-training/`)\n   - Two-tower neural network training with configurable architectures\n   - Embedding generation for users and items with similarity optimization\n   - Model versioning and registration with metadata tracking\n   - Automated hyperparameter tuning and model selection\n\n4. **Inference Serving** (`backend/`)\n   - Real-time recommendation API with sub-second response times\n   - Similarity search capabilities with vector database integration\n   - User preference management and personalization\n   - Caching strategies for improved performance\n\n5. **User Interface** (`frontend/`)\n   - Product browsing and search with advanced filtering\n   - User preference selection with intelligent recommendations\n   - Shopping cart functionality with persistent state\n   - Responsive design with mobile optimization\n\n### Data Flow\n\n```\nData Generation ‚Üí Feature Store ‚Üí Model Training ‚Üí Model Registry ‚Üí Inference Serving ‚Üí User Interface\n```\n\n### Component Interactions\n\n1. **Data Pipeline**: Raw data flows through the feature store for consistent feature serving\n2. **Training Pipeline**: Models are trained using Kubeflow Pipelines with automated workflows\n3. **Model Registry**: Trained models are versioned and stored for deployment\n4. **Inference Engine**: Real-time recommendations are served through the backend API\n5. **User Interface**: Frontend provides interactive access to recommendations and search\n\n## üîÑ Complete System Workflow\n\n### Overview\nThe Product Recommender System follows a comprehensive workflow from data generation to user interaction. This section walks through the entire process, from creating synthetic datasets to users adding items to their cart.\n\n### Phase 1: Data Generation & Preparation\n\n#### 1.1 Synthetic Dataset Creation\n```bash\n# Generate Amazon-like e-commerce data\npython -m recommendation_core.generation.dataset_gen_amazon \\\n    --n_users 1000 \\\n    --n_items 5000 \\\n    --n_interactions 20000\n```\n\n**What happens:**\n- **User profiles** are generated with realistic demographics and preferences\n- **Product catalog** is created with categories, prices, and descriptions\n- **User-item interactions** are simulated with realistic patterns\n- **Parquet files** are saved to `src/recommendation_core/feature_repo/data/`\n\n**When dataset_gen_amazon.py is Run:**\n\n**Manual Execution by Developers** (Primary Method)\nDevelopers run it manually during development and setup:\n\n**When this happens:**\n- **Initial setup** - When setting up the project for the first time\n- **Development** - When developers need fresh test data\n- **Testing** - When testing the recommendation system with different dataset sizes\n- **Demo preparation** - When preparing demos with specific data volumes\n\n**No Automated Generation**\nThere's **no automated process** that runs `dataset_gen_amazon.py`:\n- ‚ùå No CI/CD pipeline runs it\n- ‚ùå No container startup script runs it\n- ‚ùå No training pipeline runs it\n- ‚úÖ Only manual execution by developers\n\n**Training Pipeline Uses Pre-Generated Data**\nThe training pipeline **doesn't run** `dataset_gen_amazon.py` directly. Instead, it uses **pre-generated parquet files**:\n\n```python\n# From train-workflow.py - load_data_from_feast()\nif dataset_url is not None and dataset_url != \"\":\n    logger.info(\"using custom remote dataset\")\n    dataset_provider = RemoteDatasetProvider(dataset_url, force_load=True)\nelse:\n    logger.info(\"using pre generated dataset\")  # ‚Üê Uses existing parquet files\n    dataset_provider = LocalDatasetProvider(store)\n```\n\n**What this means:**\n- The training pipeline expects the parquet files to **already exist**\n- It loads data from `src/recommendation_core/feature_repo/data/`\n- The files are created **before** the training pipeline runs\n\n**Why This Design:**\n- **Reproducible datasets** - Same data across all environments\n- **Fast training** - No need to generate data during training\n- **Version control** - Dataset files can be committed to git\n- **Consistent testing** - Same test data for all tests\n\n**Parquet File Creation Process:**\n\n**Location:** All parquet files are written to:\n```\nsrc/recommendation_core/feature_repo/data/\n```\n\n**Files Generated:**\nThe `dataset_gen_amazon.py` script creates **9 parquet files** (binary columnar format) with the following structure:\n\n**Main Dataset Files:**\n1. `recommendation_users.parquet` - User profiles with demographics and preferences\n2. `recommendation_items.parquet` - Product catalog with features and pricing\n3. `recommendation_interactions.parquet` - User-item interaction history\n\n**Dummy/Feature Files:**\n4. `dummy_item_embed.parquet` - Placeholder item embeddings for feature store\n5. `dummy_user_embed.parquet` - Placeholder user embeddings for feature store\n6. `user_items.parquet` - User-item relationship mappings\n7. `item_textual_features_embed.parquet` - Text feature embeddings\n8. `item_clip_features_embed.parquet` - CLIP image feature embeddings\n\n**Code Implementation:**\n```python\n# From dataset_gen_amazon.py - Lines 265-315\ndata_path = pathlib.Path(\"src/recommendation_core/feature_repo/data\")\ndata_path.mkdir(parents=True, exist_ok=True)\n\n# Save main datasets\nusers.to_parquet(data_path / \"recommendation_users.parquet\", index=False)\nitems.to_parquet(data_path / \"recommendation_items.parquet\", index=False)\ninteractions.to_parquet(data_path / \"recommendation_interactions.parquet\", index=False)\n\n# Save dummy/feature files\ndummy_item_embed_df.to_parquet(data_path / \"dummy_item_embed.parquet\", index=False)\ndummy_user_embed_df.to_parquet(data_path / \"dummy_user_embed.parquet\", index=False)\ndummy_user_items_df.to_parquet(data_path / \"user_items.parquet\", index=False)\ndummy_textual_feature_df.to_parquet(data_path / \"item_textual_features_embed.parquet\", index=False)\ndummy_clip_feature_df.to_parquet(data_path / \"item_clip_features_embed.parquet\", index=False)\n```\n\n**Data Generation Details:**\n\n**User Data (`recommendation_users.parquet`):**\n- **User IDs**: 26-character alphanumeric strings\n- **User Names**: \"Customer\" prefix with unique identifiers\n- **Signup Dates**: Random dates within 2023\n- **Preferences**: 1-5 categories per user (e.g., \"Electronics|Computers&Accessories\")\n\n**Item Data (`recommendation_items.parquet`):**\n- **Item IDs**: Amazon-style B0XXXXXXXX format\n- **Product Names**: Realistic product names (e.g., \"WiFi Ultra AC1200\")\n- **Categories**: Hierarchical categories (e.g., \"Computers&Accessories|NetworkingDevices|Routers\")\n- **Pricing**: Actual price, discounted price, and discount percentage\n- **Ratings**: Random ratings (1-5 stars) with rating counts\n- **Descriptions**: Detailed product descriptions for image generation\n- **Image Links**: References to generated product images\n\n**Interaction Data (`recommendation_interactions.parquet`):**\n- **Interaction Types**: view, cart, purchase, rate\n- **Timestamps**: Realistic interaction timing\n- **User-Item Matching**: Biased selection based on user preferences\n- **Reviews**: Generated review titles and content for rated items\n- **Quantities**: Purchase quantities for buy interactions\n\n**Integration with Image Generation:**\nThe dataset generation script **references generated images**:\n```python\n# From dataset_gen_amazon.py\nimg_link = f\"/images/item_{safe_name}.png\"\n```\n\n**Directory Structure Created:**\n```\nrecommendation-core/\n‚îî‚îÄ‚îÄ src/\n    ‚îî‚îÄ‚îÄ recommendation_core/\n        ‚îî‚îÄ‚îÄ feature_repo/\n            ‚îî‚îÄ‚îÄ data/                    # ‚Üê All parquet files written here\n                ‚îú‚îÄ‚îÄ recommendation_users.parquet\n                ‚îú‚îÄ‚îÄ recommendation_items.parquet\n                ‚îú‚îÄ‚îÄ recommendation_interactions.parquet\n                ‚îú‚îÄ‚îÄ dummy_item_embed.parquet\n                ‚îú‚îÄ‚îÄ dummy_user_embed.parquet\n                ‚îú‚îÄ‚îÄ user_items.parquet\n                ‚îú‚îÄ‚îÄ item_textual_features_embed.parquet\n                ‚îî‚îÄ‚îÄ item_clip_features_embed.parquet\n```\n\n**Key Features:**\n- **Auto-creation**: Directory is created automatically if it doesn't exist\n- **Relative paths**: Uses relative paths from execution location\n- **Feast integration**: Location aligns with Feast feature store structure\n- **Training pipeline**: Files are expected by the training workflow\n- **Version control**: Files can be committed to git for reproducibility\n\n**Why These Parquet Files Are Created:**\n\n**1. Synthetic Data for Development & Testing**\n- **No real data dependency**: Eliminates need for actual e-commerce data\n- **Controlled environment**: Predictable data patterns for testing\n- **Privacy compliance**: No PII or sensitive business data\n- **Reproducible results**: Same data across all environments\n\n**2. Machine Learning Model Training**\n- **Feature engineering**: Structured data for neural network training\n- **User-item interactions**: Training data for collaborative filtering\n- **Multi-modal features**: Text, image, and numerical data for advanced models\n- **Realistic patterns**: Simulated user behavior for model validation\n\n**3. Feast Feature Store Integration**\n- **Online serving**: Real-time feature access for inference\n- **Offline training**: Batch feature serving for model training\n- **Feature versioning**: Consistent feature definitions across environments\n- **Scalable architecture**: Supports both online and offline serving\n\n**4. Demo & Presentation Purposes**\n- **Complete e-commerce experience**: Full product catalog with images\n- **Realistic user interactions**: Simulated shopping behavior\n- **Visual content**: Generated product images for UI\n- **Scalable demonstrations**: Configurable dataset sizes\n\n**Where Parquet Files Are Used:**\n\n**1. Training Pipeline (`recommendation-training/`)**\n```python\n# From train-workflow.py - load_data_from_feast()\ndataset_provider = LocalDatasetProvider(store)  # Loads from parquet files\nitem_df = dataset_provider.item_df()           # recommendation_items.parquet\nuser_df = dataset_provider.user_df()           # recommendation_users.parquet\ninteraction_df = dataset_provider.interaction_df()  # recommendation_interactions.parquet\n```\n\n**2. Feast Feature Store (`recommendation-core/feature_repo/`)**\n```yaml\n# feature_store.yaml references parquet files\ndata_source:\n  path: data/recommendation_items.parquet  # Item features\n  path: data/recommendation_users.parquet  # User features\n  path: data/recommendation_interactions.parquet  # Interaction history\n```\n\n**3. Backend API (`backend/`)**\n```python\n# From feast_service.py - Real-time inference\nstore = FeatureStore(repo_path=\"src/recommendation_core/feature_repo/\")\ndataset_provider = LocalDatasetProvider(store, data_dir=\"/app/backend/src/services/feast/data\")\n```\n\n**4. Frontend Display (`frontend/`)**\n- **Product catalog**: Items from `recommendation_items.parquet`\n- **User preferences**: Data from `recommendation_users.parquet`\n- **Recommendation serving**: Results from trained models using parquet data\n\n**5. Model Registry & Deployment**\n- **Model training**: Uses parquet files as training data\n- **Embedding generation**: Creates user/item embeddings from parquet data\n- **Model serving**: Serves recommendations based on parquet-based features\n\n**Data Flow Through the System:**\n\n```\nParquet Files Created ‚Üí Feast Feature Store ‚Üí Training Pipeline ‚Üí\nModel Training ‚Üí Embedding Generation ‚Üí Model Registry ‚Üí\nBackend API ‚Üí Frontend Display ‚Üí User Interactions\n```\n\n**Benefits of This Approach:**\n\n**1. Development Efficiency**\n- **Quick setup**: No need to source real e-commerce data\n- **Consistent environment**: Same data across development, testing, production\n- **Rapid iteration**: Easy to regenerate datasets with different parameters\n\n**2. Production Readiness**\n- **Scalable architecture**: Can handle real data with same structure\n- **Feature store integration**: Ready for online/offline serving\n- **Model versioning**: Reproducible training with versioned datasets\n\n**3. Demo & Sales**\n- **Complete experience**: Full e-commerce functionality\n- **Visual appeal**: Generated product images\n- **Realistic interactions**: Simulated user behavior\n- **Configurable scale**: Adjustable dataset sizes for different demos\n\n**4. Research & Experimentation**\n- **Controlled variables**: Predictable data for A/B testing\n- **Feature experimentation**: Easy to modify data structure\n- **Model comparison**: Consistent datasets for model evaluation\n\n#### 1.2 AI-Powered Image Generation\n```bash\n# Generate product images using Stable Diffusion\npython -m recommendation_core.generation.generate_images\n```\n\n**What happens:**\n- **Product descriptions** are used as prompts for Stable Diffusion\n- **Synthetic product images** are generated for each item\n- **Images are saved** to `src/recommendation_core/generation/data/generated_images/`\n- **Dataset references** these images in the `img_link` field\n\n**What generate_images.py Does:**\n- **Reads product data** from `src/recommendation_core/feature_repo/data/item_df_output.parquet`\n- **Uses product descriptions** as prompts for Stable Diffusion\n- **Generates synthetic product images** using AI\n- **Saves images** to `src/recommendation_core/generation/data/generated_images/`\n- **Creates realistic product visuals** for the recommendation system\n\n**Features:**\n- **Stable Diffusion v1.5**: High-quality image generation\n- **Text-to-image**: Product descriptions to images\n- **GPU acceleration**: CUDA support for faster generation\n- **Batch processing**: Efficient multi-image generation\n\n**Integration with Dataset Generation:**\nThe generated images are **referenced by dataset_gen_amazon.py**:\n\n```python\n# From dataset_gen_amazon.py\nimg_link = f\"/images/item_{safe_name}.png\"\n```\n\n**Current Status:**\n- ‚úÖ **99 images generated** and stored in repository\n- ‚úÖ **Images actively used** by the recommendation system\n- ‚úÖ **Manual execution** - Run when new products are added\n- ‚úÖ **One-time setup** - Images are committed to git for reuse\n\n**Data Flow:**\n```\n1. dataset_gen_amazon.py creates product data with descriptions\n         ‚Üì\n2. generate_images.py reads product descriptions as prompts\n         ‚Üì\n3. Stable Diffusion generates images from text descriptions\n         ‚Üì\n4. Images saved to generated_images/ directory\n         ‚Üì\n5. dataset_gen_amazon.py references these images in img_link\n         ‚Üì\n6. Recommendation system displays generated product images\n```\n\n**Image Storage Location:**\n```\nrecommendation-core/\n‚îî‚îÄ‚îÄ src/\n    ‚îî‚îÄ‚îÄ recommendation_core/\n        ‚îî‚îÄ‚îÄ generation/\n            ‚îî‚îÄ‚îÄ data/\n                ‚îî‚îÄ‚îÄ generated_images/           # ‚Üê Images stored here\n                    ‚îú‚îÄ‚îÄ item_AirFlow Maestro.png\n                    ‚îú‚îÄ‚îÄ item_BassBoost Pro.png\n                    ‚îú‚îÄ‚îÄ item_CableMax 8K.png\n                    ‚îú‚îÄ‚îÄ item_DigitalCanvas Pro.png\n                    ‚îú‚îÄ‚îÄ item_Galaxy X23 Ultra.png\n                    ‚îî‚îÄ‚îÄ ... (99 total images)\n```\n\n**File Naming Convention:**\n- **Format**: `item_{ProductName}.png`\n- **Example**: `item_WiFi Ultra AC1200.png`\n- **URL Encoding**: Spaces replaced with `%20` in dataset references\n- **Size**: ~200-600KB per image (high-quality PNG format)\n\n**Why This Design:**\n- **Synthetic data** - No need for real product images\n- **Consistent style** - All images generated with same AI model\n- **Scalable** - Can generate unlimited product images\n- **Demo-friendly** - Provides visual content for demonstrations\n\n#### 1.3 Feature Store Setup\n```bash\n# Apply Feast feature store configuration\nfeast apply\n```\n\n**What happens:**\n- **Feature views** are created for users, items, and interactions\n- **Online/offline serving** is configured for real-time feature access\n- **Feature transformations** are applied for consistent data processing\n\n**Feast's Architecture & Multiple Roles:**\n\n**Data Flow:**\n```\nStatic Parquet Files ‚Üí Feast Feature Store (PostgreSQL with PGVector extension) ‚Üí Model Training\n```\n\n**Feast with PostgreSQL + PGVector as Data Store:**\n- **Feature Schema Management**: Knows about your features and their schemas\n- **Data Versioning & Lineage**: Handles data versioning and lineage tracking\n- **ML-Optimized APIs**: Provides machine learning optimized APIs\n- **Batch & Real-time Serving**: Manages both batch and real-time serving\n- **Vector Operations**: Uses PGVector extension for similarity search\n- **Scalable Storage**: PostgreSQL handles large datasets efficiently\n- **Concurrent Access**: Supports high-throughput operations\n\n**Feast's Multiple Roles in the System:**\n\n**1. Initial Setup (Data Ingestion)**\n- **Creates Feature Store** using Parquet files with PostgreSQL backend\n- **Sets up schema** and feature definitions\n- **Configures feature views** for users, items, and interactions\n- **Establishes data lineage** and versioning\n- **Feast Apply Job (One-Time Setup)**\n  ```yaml\n  apiVersion: batch/v1\n  kind: Job  # ‚Üê Regular Job, NOT CronJob\n  metadata:\n    name: feast-apply-job\n  ```\n  - **One-time execution**: Runs only once during initial deployment\n  - **Infrastructure setup**: Configures feature store schema and definitions\n  - **No recurring schedule**: Not scheduled to run repeatedly\n  - **Persistent configuration**: Once set up, feature store remains stable\n  - **Prerequisite for training**: Training pipeline waits for this job to complete\n\n**2. Real-time Serving (Production Use)**\nFeast is actively used for real-time serving in production:\n\n**3. Real-time Product Recommendations**\n```python\n# From feast_service.py - Real-time recommendation serving\nstore = FeatureStore(repo_path=\"src/recommendation_core/feature_repo/\")\nuser_features = store.get_online_features(features=user_feature_view, entity_rows=[{\"user_id\": user_id}])\n```\n\n**What happens in real-time:**\n- **User requests recommendations** ‚Üí Feast serves pre-computed top-k items\n- **Sub-second response times** for instant recommendations\n- **Personalized results** based on user preferences and history\n- **Scalable serving** for multiple concurrent users\n\n**4. New User Recommendations (Real-time)**\n```python\n# Embedding-based recommendations for new users\nuser_embed = user_encoder(**data_preproccess(user_as_df))[0]\ntop_k = store.retrieve_online_documents(query=user_embed.tolist(), top_k=k)\n```\n\n**What happens in real-time:**\n- **New user signs up** ‚Üí User embedding generated on-the-fly\n- **Vector similarity search** ‚Üí PGVector finds similar items instantly\n- **Real-time results** ‚Üí Recommendations served immediately\n\n**5. Real-time Semantic Search**\n```python\n# Text and image-based similarity search\nsearch_service = SearchService(store)\nresults_df = search_service.search_by_text(text, k)\n```\n\n**What happens in real-time:**\n- **User enters search query** ‚Üí Text embedding generated instantly\n- **Vector similarity search** ‚Üí PGVector finds similar products\n- **Real-time results** ‚Üí Search results served immediately\n\n**6. Training Data Serving (Batch)**\n```python\n# From train-workflow.py - Batch training data\ndataset_provider = LocalDatasetProvider(store)\nitem_df = dataset_provider.item_df()\nuser_df = dataset_provider.user_df()\ninteraction_df = dataset_provider.interaction_df()\n```\n\n**7. Real-time Embedding Storage and Retrieval**\n```python\n# Vector similarity search with PGVector\nsimilar_items = store.retrieve_online_documents(\n    query=user_embedding,\n    top_k=10,\n    features=[\"item_embedding:item_id\"]\n)\n```\n\n**8. User Management & Profile Data**\n```python\n# Get all existing users from feature store\ndef get_all_existing_users(self) -> List[dict]:\n    user_df = self.dataset_provider.user_df()\n    return user_df\n```\n\n**9. Product Catalog & Item Details**\n```python\n# Fetch full product details from feature store\nsuggested_item = self.store.get_online_features(\n    features=self.store.get_feature_service(\"item_service\"),\n    entity_rows=[{\"item_id\": item_id} for item_id in top_item_ids],\n).to_df()\n```\n\n**10. Image-Based Product Search**\n```python\n# Search products by uploaded image\ndef search_item_by_image_file(self, image: PILImage.Image, k=5):\n    results_df = self.search_by_image_service.search_by_image(image, k)\n    top_item_ids = results_df[\"item_id\"].tolist()\n    return self._item_ids_to_product_list(top_item_ids)\n```\n\n**11. Individual Product Lookup**\n```python\n# Get specific product by ID\ndef get_item_by_id(self, item_id: int) -> Product:\n    product_list = self._item_ids_to_product_list([item_id])\n    return product_list[0]\n```\n\n**Real-time Recommendation Flow:**\n```\nUser Request ‚Üí Feast API ‚Üí PostgreSQL+PGVector ‚Üí Vector Similarity Search ‚Üí\nReal-time Results ‚Üí User Interface\n```\n\n**Complete Feast Usage Summary:**\n- ‚úÖ **Product Recommendations** - Personalized item suggestions\n- ‚úÖ **User Management** - User profiles and preferences\n- ‚úÖ **Product Catalog** - Item details and metadata\n- ‚úÖ **Semantic Search** - Text and image-based search\n- ‚úÖ **Training Data** - Batch data for model training\n- ‚úÖ **Vector Operations** - Embedding storage and retrieval\n- ‚úÖ **Real-time Serving** - Sub-second response times\n- ‚úÖ **Feature Versioning** - Consistent feature definitions\n\n**Key Benefits of This Architecture:**\n- **Unified Data Access**: Same features for training and inference\n- **Real-time Performance**: Sub-second response times for recommendations\n- **Scalable Storage**: PostgreSQL handles large datasets efficiently\n- **Vector Operations**: PGVector enables fast similarity search\n- **Data Consistency**: Versioned features ensure reproducibility\n\n### Phase 2: Model Training & Registration\n\n#### 2.1 Training Pipeline Execution\n\n**Why Models Are Trained Periodically:**\n\nModels are trained **daily at midnight UTC** to address the dynamic nature of e-commerce environments and ensure optimal recommendation performance. **Data drift and concept drift** occur as user preferences evolve over time, new products are added to catalogs, seasonal patterns emerge, and market dynamics shift. Without periodic retraining, models would lose accuracy as they become outdated and fail to capture current user behaviors and product trends. **Continuous learning** is essential because daily user interactions‚Äîincluding clicks, purchases, ratings, and browsing patterns‚Äîprovide valuable fresh training data that improves model accuracy and enables better cold-start recommendations for new users and items. From a **business perspective**, periodic training directly impacts revenue optimization by generating more relevant recommendations that increase conversion rates, enhances user engagement through improved personalization, provides competitive advantage by staying current with market trends, and boosts customer satisfaction through more accurate suggestions. **Technical benefits** include maintaining model freshness to reflect current data patterns, enabling regular performance monitoring against new data, supporting A/B testing to compare model versions, and providing rollback capabilities for safety. The **data pipeline integration** supports this through streaming data collection of new interactions, daily feature store updates with fresh features, periodic embedding refreshes for users and items, and model registry version control for lifecycle management.\n```bash\n# Run Kubeflow training pipeline\npython train-workflow.py\n```\n\n**What happens:**\n- **Data loading** from Feast feature store\n- **Two-tower model training** (UserTower + ItemTower)\n- **Embedding generation** for all users and items\n- **Model versioning** and registration in Model Registry\n\n#### 2.2 Batch Scoring & Candidate Generation\n```python\n# Generate top-k recommendations for all users\nfor user in users:\n    user_embedding = user_encoder(user_features)\n    similar_items = similarity_search(user_embedding, top_k=10)\n    store_recommendations(user_id, similar_items)\n```\n\n**What happens:**\n- **User embeddings** are generated using trained UserTower\n- **Item embeddings** are generated using trained ItemTower\n- **Similarity search** finds top-k items for each user\n- **Recommendations** are stored in online feature store\n\n### Phase 3: System Deployment\n\n#### 3.1 Infrastructure Setup\n```bash\n# Deploy using Helm charts\nhelm install product-recommender ./helm\n```\n\n**What happens:**\n- **PostgreSQL + pgvector** database is deployed\n- **Backend API** (FastAPI) is deployed with Feast integration\n- **Frontend** (React) is deployed with static file serving\n- **Monitoring** (OpenShift built-in Prometheus/Thanos + Grafana) is configured\n\n#### 3.2 Model Deployment\n```bash\n# Deploy trained models to serving infrastructure\nkubectl apply -f model-deployment.yaml\n```\n\n**What happens:**\n- **Model artifacts** are loaded from Model Registry\n- **Inference endpoints** are created for real-time serving\n- **Health checks** are configured for model availability\n- **Scaling policies** are applied for load management\n\n#### 3.3 Monitoring Configuration\n**OpenShift Built-in Monitoring Stack:**\n\nThe product-recommender-system leverages **OpenShift's built-in monitoring infrastructure**:\n\n**Prometheus/Thanos Configuration:**\n- **OpenShift Monitoring**: Uses OpenShift's built-in Prometheus/Thanos stack\n- **Metrics Collection**: Automatically collects metrics from all deployed components\n- **Service Discovery**: OpenShift automatically discovers and scrapes metrics endpoints\n- **Long-term Storage**: Thanos provides long-term metrics retention\n\n**Grafana Configuration:**\n- **OpenShift Grafana**: Uses OpenShift's built-in Grafana instance\n- **Dashboard Access**: Available through OpenShift Console ‚Üí Monitoring ‚Üí Dashboards\n- **Pre-built Dashboards**: OpenShift provides default dashboards for Kubernetes metrics\n- **Custom Dashboards**: Can be created for application-specific metrics\n\n**Metrics Endpoints:**\n```yaml\n# Backend metrics endpoint (automatically discovered by OpenShift)\nannotations:\n  prometheus.io/scrape: \"true\"\n  prometheus.io/path: \"/metrics\"\n  prometheus.io/port: \"8000\"\n```\n\n**Monitoring Access:**\n- **OpenShift Console**: Navigate to Monitoring ‚Üí Dashboards\n- **Grafana URL**: Available through OpenShift Console\n- **Prometheus URL**: Available through OpenShift Console\n- **Metrics API**: Direct access to Prometheus/Thanos API\n\n**What's Monitored:**\n- **Application metrics**: Request rates, response times, error rates\n- **Infrastructure metrics**: CPU, memory, disk usage\n- **Database metrics**: PostgreSQL performance and connections\n- **Model metrics**: Training times, inference latency, model accuracy\n- **Feast metrics**: Feature store performance and serving latency\n\n### Phase 4: User Interaction Flow\n\n#### 4.1 New User Onboarding\n```\nUser visits application ‚Üí Selects preferences ‚Üí Gets personalized recommendations\n```\n\n**What happens:**\n- **User registration** with basic profile information\n- **Preference selection** from available categories\n- **User embedding** is generated from preferences\n- **Initial recommendations** are served based on preferences\n\n#### 4.2 Existing User Experience\n```\nUser logs in ‚Üí System loads preferences ‚Üí Serves personalized recommendations\n```\n\n**What happens:**\n- **User authentication** with JWT tokens\n- **Preference retrieval** from user profile\n- **Recommendation serving** from cached results\n- **Real-time updates** based on recent interactions\n\n#### 4.3 Product Discovery & Search\n\n**Text-Based Search:**\n```\nUser enters search query ‚Üí Semantic embedding ‚Üí Vector similarity search ‚Üí Results\n```\n\n**Image-Based Search:**\n```\nUser uploads image ‚Üí CLIP encoding ‚Üí Vector similarity search ‚Üí Similar products\n```\n\n**What happens:**\n- **Query embedding** using the same model as training\n- **Vector similarity search** in pgvector database\n- **Result ranking** by similarity score\n- **Product display** with images and details\n\n#### 4.4 Shopping Cart Integration\n\n**Adding Items to Cart:**\n```\nUser clicks \"Add to Cart\" ‚Üí Backend validates ‚Üí Cart updated ‚Üí UI refreshed\n```\n\n**What happens:**\n- **Item validation** (availability, price, etc.)\n- **Cart state management** in backend database\n- **Real-time updates** to frontend\n- **Persistent storage** across sessions\n\n**Cart Management:**\n```\nUser views cart ‚Üí Modifies quantities ‚Üí Proceeds to checkout ‚Üí Purchase completed\n```\n\n**What happens:**\n- **Cart retrieval** from user session\n- **Quantity updates** with validation\n- **Checkout process** with payment integration\n- **Purchase recording** in interaction history\n\n### Phase 5: Continuous Learning & Updates\n\n#### 5.1 Interaction Tracking\n```python\n# Record user interactions for model updates\ndef record_interaction(user_id, item_id, interaction_type):\n    interaction = {\n        \"user_id\": user_id,\n        \"item_id\": item_id,\n        \"timestamp\": datetime.now(),\n        \"interaction_type\": interaction_type  # view, cart, purchase, rate\n    }\n    store_interaction(interaction)\n```\n\n**What happens:**\n- **User actions** are tracked (views, cart additions, purchases)\n- **Interaction data** is stored in feature store\n- **Real-time updates** to user preferences\n- **Model retraining** triggers based on new data\n\n#### 5.2 Model Retraining\n```bash\n# Automated retraining pipeline\nkubectl create job --from=cronjob/model-retraining\n```\n\n**What happens:**\n- **New interaction data** is loaded from feature store\n- **Model retraining** with updated datasets\n- **Performance evaluation** against previous models\n- **Model deployment** if performance improves\n\n**Retraining Schedule:**\n```yaml\n# From run-pipeline-job.yaml\nschedule: \"0 0 * * *\"  # Run daily at midnight\nconcurrencyPolicy: Forbid\n```\n\n**Schedule Details:**\n- **Frequency**: **Daily at midnight (00:00 UTC)**\n- **Cron Expression**: `\"0 0 * * *\"` (minute hour day month weekday)\n- **Concurrency Policy**: `Forbid` (prevents overlapping runs)\n- **Timezone**: UTC\n\n**What this means:**\n- **Daily retraining** - Models are retrained every day at midnight\n- **No overlapping runs** - Only one training job runs at a time (prevents multiple concurrent training jobs)\n- **Automatic updates** - New interaction data is incorporated daily\n- **Performance monitoring** - Models are evaluated against previous versions\n\n**What Happens During Midnight Training:**\n\n**Phase 1: Data Loading & Preparation**\n```python\n# Load data from Feast feature store\nload_data_task = load_data_from_feast()\nitem_df = pd.read_parquet(item_df_input.path)\nuser_df = pd.read_parquet(user_df_input.path)\ninteraction_df = pd.read_parquet(interaction_df_input.path)\n```\n- **Load user data** - User profiles and preferences from Feast\n- **Load item data** - Product catalog and features from Feast\n- **Load interaction data** - User-item interactions from Feast\n- **Data validation** - Ensure data quality and completeness\n\n**Phase 2: Model Training**\n```python\n# Train two-tower neural network\nitem_encoder, user_encoder, models_definition = create_and_train_two_tower(\n    item_df, user_df, interaction_df, return_model_definition=True\n)\n```\n- **UserTower training** - Neural network for user embeddings\n- **ItemTower training** - Neural network for item embeddings\n- **Two-tower architecture** - Collaborative filtering with deep learning\n- **Model optimization** - Loss minimization and gradient descent\n\n**Training Data Sources:**\n\n**1. Static Dataset (Primary)**\n```python\n# Load from Feast feature store\ndataset_provider = LocalDatasetProvider(store)\nitem_df = dataset_provider.item_df()           # recommendation_items.parquet\nuser_df = dataset_provider.user_df()           # recommendation_users.parquet\ninteraction_df = dataset_provider.interaction_df()  # recommendation_interactions.parquet\n```\n- **User data**: User profiles, preferences, demographics\n- **Item data**: Product catalog, features, categories, pricing\n- **Interaction data**: User-item interactions (views, cart, purchase, ratings)\n\n**2. Streaming Data (Real-time)**\n```python\n# Additional data from PostgreSQL database\nif table_exists(engine, \"new_users\"):\n    stream_users_df = pd.read_sql(\"SELECT * FROM new_users\", engine)\n    user_df = pd.concat([user_df, stream_users_df], axis=0)\n\nif table_exists(engine, \"stream_interaction\"):\n    stream_interaction_df = pd.read_sql(\"SELECT * FROM stream_interaction\", engine)\n    interaction_df = pd.concat([interaction_df, stream_interaction_df], axis=0)\n```\n- **New users**: Real-time user registrations and preferences\n- **Stream interactions**: Live user interactions (views, purchases, ratings)\n- **Data fusion**: Combines static and streaming data for training\n\n**3. Data Processing Pipeline**\n```python\n# Data preprocessing and feature engineering\ndataset = preproccess_pipeline(item_df, user_df, interaction_df)\n```\n- **Feature extraction**: Numerical, categorical, and text features\n- **Text embedding**: BGE model for product descriptions\n- **Interaction weighting**: Calculates interaction strength and magnitude\n- **Data alignment**: Matches users, items, and interactions\n\n**Training Data Structure:**\n- **User features**: Demographics, preferences, signup date\n- **Item features**: Categories, pricing, ratings, product descriptions\n- **Interaction features**: Interaction type, timestamp, rating, quantity\n- **Magnitude calculation**: Interaction strength based on type and frequency\n\n**Phase 3: Embedding Generation**\n```python\n# Generate embeddings for all users and items\nitem_embed_df[\"embedding\"] = item_encoder(**proccessed_items).detach().numpy().tolist()\nuser_embed_df[\"embedding\"] = user_encoder(**proccessed_users).detach().numpy().tolist()\n```\n- **User embeddings** - Generate vector representations for all users\n- **Item embeddings** - Generate vector representations for all items\n- **Batch processing** - Efficient embedding generation for large datasets\n- **Vector storage** - Store embeddings in PostgreSQL with PGVector\n\n**Phase 4: Model Registration & Deployment**\n```python\n# Save trained models\ntorch.save(item_encoder.state_dict(), item_output_model.path)\ntorch.save(user_encoder.state_dict(), user_output_model.path)\n\n# Register in Model Registry\ncreate_model_registry_task = registry_model_to_model_registry(\n    author=fetch_api_credentials_task.outputs[\"author\"],\n    user_token=fetch_api_credentials_task.outputs[\"user_token\"],\n    host=fetch_api_credentials_task.outputs[\"host\"],\n    bucket_name=train_model_task.outputs[\"bucket_name\"],\n    new_version=train_model_task.outputs[\"new_version\"],\n    object_name=train_model_task.outputs[\"object_name\"],\n    torch_version=train_model_task.outputs[\"torch_version\"],\n)\n```\n- **Model serialization** - Save trained models to disk\n- **Version management** - Increment model version numbers\n- **Model registry** - Register new models in central registry\n- **Deployment preparation** - Prepare models for production serving\n\n**Where Training Knowledge is Stored:**\n\n**1. Model Artifacts (MinIO Object Storage)**\n```python\n# Store trained models in MinIO\nminio_client.fput_object(\n    bucket_name=\"user-encoder\",\n    object_name=f\"user-encoder-{new_version}.pth\",\n    file_path=user_output_model.path,\n)\n```\n- **UserTower model**: `user-encoder-{version}.pth` in MinIO bucket\n- **ItemTower model**: `item-encoder-{version}.pth` in MinIO bucket\n- **Model configurations**: JSON files with model architecture details\n- **Version tracking**: Incremental versioning (1.0.0, 1.0.1, etc.)\n\n**2. Model Registry (Centralized Management)**\n```python\n# Register models in Model Registry\nregistry_model_to_model_registry(\n    bucket_name=bucket_name,\n    new_version=new_version,\n    object_name=object_name,\n    torch_version=torch_version,\n)\n```\n- **Model metadata**: Version, architecture, performance metrics\n- **Model lineage**: Training history and data sources\n- **Deployment tracking**: Which models are in production\n- **A/B testing**: Model comparison and rollback capabilities\n\n**3. Feature Store (PostgreSQL + PGVector)**\n```python\n# Push embeddings to online feature store\nstore.push(\n    \"item_embed_push_source\",\n    item_embed_df,\n    to=PushMode.ONLINE,\n    allow_registry_cache=False,\n)\n```\n- **User embeddings**: Vector representations of all users\n- **Item embeddings**: Vector representations of all items\n- **Real-time serving**: Embeddings available for instant recommendations\n- **Vector database**: PostgreSQL with PGVector extension\n\n**4. Database Version Tracking (PostgreSQL)**\n```python\n# Track model versions in database\nCREATE TABLE model_version (\n    id SERIAL PRIMARY KEY,\n    version VARCHAR(50) NOT NULL,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n```\n- **Version history**: Complete model version timeline\n- **Deployment tracking**: Which version is currently active\n- **Rollback capability**: Easy reversion to previous models\n- **Audit trail**: Full training and deployment history\n\n**5. Model Definition Files (JSON)**\n```python\n# Save model architecture definitions\nmodels_definition = {\n    \"items_num_numerical\": dataset.items_num_numerical,\n    \"items_num_categorical\": dataset.items_num_categorical,\n    \"users_num_numerical\": dataset.users_num_numerical,\n    \"users_num_categorical\": dataset.users_num_categorical,\n}\n```\n- **Architecture specs**: Model structure and feature dimensions\n- **Feature mappings**: How input features map to model layers\n- **Reproducibility**: Exact model configuration for rebuilding\n- **Deployment config**: Model loading and serving parameters\n\n**Phase 5: Feature Store Updates**\n```python\n# Push embeddings to online feature store\nstore.push(\n    \"item_embed_push_source\",\n    item_embed_df,\n    to=PushMode.ONLINE,\n    allow_registry_cache=False,\n)\n```\n- **Embedding deployment** - Push new embeddings to online store\n- **Real-time serving** - Update feature store for immediate use\n- **Vector database** - Update PostgreSQL+PGVector with new embeddings\n- **Cache invalidation** - Clear old embeddings and load new ones\n\n**Training Job Details:**\n- **Single comprehensive job** - One training pipeline that handles all model components\n- **Complete retraining** - UserTower, ItemTower, and embeddings are all retrained together\n- **Sequential processing** - Data loading ‚Üí Model training ‚Üí Embedding generation ‚Üí Model registration\n- **Single deployment** - All updated models are deployed as a complete set\n\n**Why \"Only one training job runs at a time\":**\n- **Resource efficiency** - Prevents multiple GPU-intensive training jobs from competing\n- **Data consistency** - Ensures all models are trained on the same dataset snapshot\n- **Deployment safety** - Prevents conflicts when registering new model versions\n- **Monitoring clarity** - Single training run is easier to track and debug\n\n### Data Flow Summary\n\n```\nData Generation ‚Üí Feature Store ‚Üí Model Training ‚Üí Model Registry ‚Üí\nDeployment ‚Üí User Interaction ‚Üí Interaction Tracking ‚Üí Model Retraining\n```\n\n### Key Integration Points\n\n1. **Feast Feature Store**: Central data hub for all features\n2. **Model Registry**: Version control for ML models\n3. **pgvector Database**: Vector similarity search\n4. **FastAPI Backend**: Real-time recommendation serving\n5. **React Frontend**: User interface and interactions\n6. **Kubeflow Pipelines**: Automated training workflows\n\nThis workflow ensures a complete, production-ready recommendation system that continuously learns and improves based on user interactions.\n\n## üìã Architecture Diagrams\n\n### Data Processing Pipeline\n<img src=\"figures/data_processing_pipeline.drawio.png\" alt=\"Data Processing Pipeline\" width=\"80%\">\n\n### Training & Batch Scoring\n\n#### Recommendation Algorithm Stages:\n\n1. **Filtering**\n   Removes invalid candidates based on user demographics (e.g., age, item availability in the region) and previously viewed items.\n\n2. **Ranking**\n   Identifies the most relevant top-k items based on previous interactions between users and items (trained with two-tower algorithm).\n\n3. **Business Ordering**\n   Reorders candidates according to business logic and priorities.\n\n#### Training Process\n* Feast takes the Raw data (item table, user table, interaction table) and stores the items, users, and interactions as Feature Views.\n* Using the Two-Tower architecture technique, we train the item and user encoders based on the existing user-item interactions.\n\n<img src=\"figures/training_and_batch_scoring.drawio.png\" alt=\"Training & Batch scoring\" width=\"80%\">\n\n#### Batch Scoring\n* After completing the training of the Encoders, embed all items and users, then push them in the PGVector database as embedding.\n* Because we use batch scoring, we calculate for each user the top k recommended items using the item embeddings\n* Pushes this top k items for each user to the online store Feature Store.\n\n### Inference\n\n#### Existing User Case:\n* Sending a get request from the EDB vectorDB to get the embedding of the existing user.\n* Perform a similarity search on the item vectorDB to get the top k similar items.\n\n#### New User Case:\n* The new users will be embedded into a vector representation.\n* The user vector will do a similarity search from the EDB PGVector to get the top k suggested items\n\n<img src=\"figures/Inference.drawio.png\" alt=\"Inference\" width=\"80%\">\n\n### Search by Text & Search by Image\n1. Embed the user query into embeddings.\n2. Search the top-k closest items that were generated with the same model at batch inference time.\n3. Return to user the recommended items\n\n<img src=\"figures/search_by.drawio.png\" alt=\"Search by Text/Image\" width=\"80%\">\n\n## üîç How to Verify System Health\n\n### System Health Verification\n\nThe product-recommendation-system provides multiple ways to verify it's working properly:\n\n#### 1. **Health Check Endpoints**\n\n**Backend Health Checks:**\n```bash\n# Liveness check - basic system availability\ncurl http://your-backend-url/health/live\n# Expected: {\"status\": \"alive\"}\n\n# Readiness check - full system readiness including database\ncurl http://your-backend-url/health/ready\n# Expected: {\"status\": \"ready\"}\n```\n\n**Frontend Health Check:**\n```bash\n# Frontend availability\ncurl http://your-frontend-url/\n# Expected: 200 OK with application content\n```\n\n**Feast Health Check:**\n```bash\n# Feature store health\ncurl http://your-feast-url/health\n# Expected: 200 OK\n```\n\n#### 2. **Integration Tests**\n\n**Run Automated Tests:**\n```bash\n# Run comprehensive integration tests\n./run_integration_tests.sh tests/integration/test_endpoints.tavern.yaml\n```\n\n**Test Coverage:**\n- ‚úÖ **Frontend Health**: Verifies UI is accessible\n- ‚úÖ **Backend Health**: Checks API availability and database connectivity\n- ‚úÖ **Feast Health**: Validates feature store functionality\n- ‚úÖ **Response Validation**: Ensures correct JSON responses\n\n#### 3. **Monitoring & Metrics**\n\n**OpenShift Monitoring Dashboard:**\n```bash\n# Access monitoring through OpenShift Console\n# Navigate to: Monitoring ‚Üí Dashboards\n```\n\n**Key Metrics to Monitor:**\n- ‚úÖ **Application Metrics**: Request rates, response times, error rates\n- ‚úÖ **Infrastructure Metrics**: CPU, memory, disk usage\n- ‚úÖ **Database Metrics**: PostgreSQL performance and connections\n- ‚úÖ **Model Metrics**: Training times, inference latency, model accuracy\n- ‚úÖ **Feast Metrics**: Feature store performance and serving latency\n\n**Metrics Endpoints:**\n```yaml\n# Backend metrics endpoint (automatically discovered by OpenShift)\nannotations:\n  prometheus.io/scrape: \"true\"\n  prometheus.io/path: \"/metrics\"\n  prometheus.io/port: \"8000\"\n```\n\n#### 4. **Functional Verification**\n\n**Recommendation API Test:**\n```bash\n# Test recommendation generation\ncurl -X POST http://your-backend-url/recommendations \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"user_id\": \"test_user\"}'\n# Expected: List of recommended products\n```\n\n**Search Functionality Test:**\n```bash\n# Test text search\ncurl -X POST http://your-backend-url/search/text \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"smartphone\"}'\n# Expected: Relevant product search results\n```\n\n**User Management Test:**\n```bash\n# Test user creation and preferences\ncurl -X POST http://your-backend-url/users \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"test_user\", \"preferences\": [\"electronics\"]}'\n# Expected: User created successfully\n```\n\n#### 5. **Training Pipeline Verification**\n\n**Check Training Job Status:**\n```bash\n# Verify daily training pipeline is running\noc get cronjob kfp-run-job -n your-namespace\n# Expected: Active cronjob with last successful run\n```\n\n**Model Registry Check:**\n```bash\n# Verify models are being registered\noc get pods -l app=model-registry -n your-namespace\n# Expected: Model registry pods running\n```\n\n**Feature Store Verification:**\n```bash\n# Check feature store data\noc get pods -l app=feast -n your-namespace\n# Expected: Feast pods running with data\n```\n\n#### 6. **Performance Benchmarks**\n\n**Response Time Verification:**\n- ‚úÖ **API Response**: < 500ms for recommendations\n- ‚úÖ **Search Response**: < 1s for text/image search\n- ‚úÖ **Database Queries**: < 100ms for feature retrieval\n- ‚úÖ **Model Inference**: < 200ms for embedding generation\n\n**Throughput Verification:**\n- ‚úÖ **Concurrent Users**: System handles multiple simultaneous requests\n- ‚úÖ **Recommendation Quality**: Relevant and diverse product suggestions\n- ‚úÖ **Search Accuracy**: High precision and recall for search queries\n\n#### 7. **Error Monitoring**\n\n**Check for Errors:**\n```bash\n# View application logs\noc logs -f deployment/backend -n your-namespace\n\n# Check for error patterns\noc logs deployment/backend -n your-namespace | grep ERROR\n```\n\n**Common Issues to Monitor:**\n- ‚ùå **Database Connection Failures**: Check PostgreSQL connectivity\n- ‚ùå **Model Loading Errors**: Verify model files in MinIO\n- ‚ùå **Feature Store Errors**: Check Feast configuration\n- ‚ùå **Training Pipeline Failures**: Monitor daily training jobs\n\n#### 8. **End-to-End Testing**\n\n**Complete User Journey:**\n1. ‚úÖ **User Registration**: Create new user account\n2. ‚úÖ **Preference Selection**: Set user preferences\n3. ‚úÖ **Recommendation Generation**: Get personalized recommendations\n4. ‚úÖ **Product Search**: Search by text or image\n5. ‚úÖ **Cart Management**: Add items to cart\n6. ‚úÖ **Checkout Process**: Complete purchase flow\n\n### Success Criteria\n\n**System is working properly when:**\n- ‚úÖ All health endpoints return 200 OK\n- ‚úÖ Integration tests pass (100% success rate)\n- ‚úÖ Response times meet performance benchmarks\n- ‚úÖ Training pipeline runs successfully daily\n- ‚úÖ Models are being updated and registered\n- ‚úÖ Feature store contains current data\n- ‚úÖ User interactions generate relevant recommendations\n- ‚úÖ Search functionality returns accurate results\n- ‚úÖ No critical errors in application logs\n- ‚úÖ Monitoring dashboards show healthy metrics\n\n## üîß Requirements\n\nDepend on the scale and speed required, for small amount of users have minimum of:\n* No GPU required; for larger scale and faster performance, use GPUs.\n* 4 CPU cores.\n* 16 Gi of RAM.\n* Storage: 8 Gi (depend on the input dataset).\n\n### Required Software\n\n* `oc` command installed\n* `helm` command installed\n* Red Hat OpenShift.\n* Red Hat OpenShift AI version 2.2 and above.\n* Red Hat Authorino Operator (stable update channel, version 1.2.1 or later)\n* Red Hat OpenShift Serverless Operator\n* Red Hat OpenShift Service Mesh Operator\n\n#### Make sure you have configured\nUnder openshiftAI DataScienceCluster CR change modelregistry, and feastoperator to `Managed` state which by default are on `Removed`:\n```yaml\napiVersion: datasciencecluster.opendatahub.io/v1\nkind: DataScienceCluster\nmetadata:\n  name: default-dsc\n...\nspec:\n  components:\n    codeflare:\n      managementState: Managed\n    kserve:\n      managementState: Managed\n      nim:\n        managementState: Managed\n      rawDeploymentServiceConfig: Headless\n      serving:\n        ingressGateway:\n          certificate:\n            secretName: rhoai-letscrypt-cert\n            type: Provided\n        managementState: Managed\n        name: knative-serving\n    modelregistry:\n      managementState: Managed\n      registriesNamespace: rhoai-model-registries\n    feastoperator:\n      managementState: Managed\n    trustyai:\n      managementState: Managed\n    kueue:\n      managementState: Managed\n    workbenches:\n      managementState: Managed\n      workbenchNamespace: rhods-notebooks\n    dashboard:\n      managementState: Managed\n    modelmeshserving:\n      managementState: Managed\n    datasciencepipelines:\n      managementState: Managed\n```\n\n### Required Permissions\n\n* Standard user. No elevated cluster permissions required\n\n## üöÄ Installation\n\n### Quick Start\n\n1. Fork and clone the repository:\n   ```bash\n   # Fork via GitHub UI, then:\n   git clone https://github.com/<your-username>/product-recommender-system.git\n   cd product-recommender-system\n   ```\n\n2. Navigate to the helm directory:\n   ```bash\n   cd helm/\n   ```\n\n3. Set the namespace environment variable to define on which namespace the quickstart will be installed:\n   ```bash\n   # Replace <namespace> with your desired namespace\n   export NAMESPACE=<namespace>\n   ```\n\n4. Install using make (this should take 8~ minutes with the default data, and with custom data maybe less or more):\n   ```bash\n   # This will create the namespace and deploy all components\n   make install\n   ```\n\n* Or installing and defining a namespace together:\n   ```bash\n   # Replace <namespace> with your desired namespace and install in one command\n   make install NAMESPACE=<namespace>\n   ```\n\n### Custom Dataset Configuration\n\nBy default, a dataset is automatically generated when the application is installed on the cluster.\n\nTo use a custom dataset instead, provide a URL by setting the `DATASET_URL` property during installation:\n\n```bash\n# Replace <custom_dataset_url> with the desired dataset URL\nmake install DATASET_URL=<custom_dataset_url>\n```\n\n### Advanced Configuration\n\nFor detailed configuration options, see the [helm/README.md](helm/) for deployment options and [backend/README.md](backend/) for API configuration.\n\n## üßπ Uninstallation\n\nTo uninstall the recommender system and clean up resources:\n\n1. Navigate to the helm directory:\n   ```bash\n   cd helm/\n   ```\n\n2. Uninstalling with namespace specified:\n   ```bash\n   # Replace <namespace> with your namespace\n   make uninstall NAMESPACE=<namespace>\n   ```\n\n## üìö Documentation\n\n### Component-Specific Documentation\n\n- **[recommendation-core/README.md](recommendation-core/README.md)** - ML library documentation, model architectures, and data generation\n- **[recommendation-training/README.md](recommendation-training/README.md)** - Training pipeline documentation, Kubeflow workflows, and model registration\n- **[recommendation-model-registry/README.md](recommendation-model-registry/README.md)** - Infrastructure tools documentation and cluster credential management\n- **[backend/README.md](backend/)** - API documentation, authentication, and service configuration\n- **[frontend/README.md](frontend/)** - User interface documentation and component architecture\n- **[helm/README.md](helm/)** - Deployment documentation, Helm charts, and Kubernetes configuration\n\n### Development Guides\n\n- **Model Development**: See [recommendation-core/README.md](recommendation-core/README.md) for adding new model architectures\n- **Training Pipeline**: See [recommendation-training/README.md](recommendation-training/README.md) for modifying training workflows\n- **API Development**: See [backend/README.md](backend/) for extending the REST API\n- **UI Development**: See [frontend/README.md](frontend/) for frontend component development\n\n### Troubleshooting\n\n- **Deployment Issues**: Check [helm/README.md](helm/) for common deployment problems\n- **Training Problems**: See [recommendation-training/README.md](recommendation-training/README.md) for pipeline troubleshooting\n- **Model Issues**: Refer to [recommendation-core/README.md](recommendation-core/README.md) for model debugging\n- **Infrastructure**: Check [recommendation-model-registry/README.md](recommendation-model-registry/README.md) for cluster access issues\n\n## ü§ù Contributing\n\n### Development Workflow\n\n1. **Fork the repository**\n2. **Create a feature branch** for your changes\n3. **Make changes** in the appropriate component directory\n4. **Update documentation** in the relevant README files\n5. **Test your changes** using the component-specific testing procedures\n6. **Submit a pull request** with detailed description\n\n### Component-Specific Guidelines\n\n- **recommendation-core**: Follow the development guidelines in [recommendation-core/README.md](recommendation-core/README.md)\n- **recommendation-training**: See contribution guidelines in [recommendation-training/README.md](recommendation-training/README.md)\n- **backend**: Follow API development guidelines in [backend/README.md](backend/)\n- **frontend**: See UI development guidelines in [frontend/README.md](frontend/)\n\n## üìÑ License\n\nThis project is licensed under the same terms as the Red Hat AI Quickstart program.\n\n## üîó References\n\n- [Red Hat OpenShift AI Documentation](https://access.redhat.com/documentation/en-us/red_hat_openshift_ai)\n- [Kubeflow Pipelines Documentation](https://www.kubeflow.org/docs/components/pipelines/)\n- [Feast Documentation](https://docs.feast.dev/)\n- [PyTorch Documentation](https://pytorch.org/docs/)\n- [React Documentation](https://react.dev/)\n- [FastAPI Documentation](https://fastapi.tiangolo.com/)\n\n---\n\n**Note**: This is a comprehensive recommendation system designed for production use. Always test thoroughly in development environments before deploying to production. For component-specific questions, refer to the individual README files in each subdirectory.\n",
      "readmeFilename": "README.md",
      "images": [],
      "githubLink": "https://github.com/rh-ai-quickstart/product-recommender-system",
      "categories": [
        "Python"
      ],
      "stars": 1,
      "lastUpdated": "9/22/2025"
    },
    {
      "id": "basic-speech-to-text-with-whisper",
      "title": "Basic Speech To Text With Whisper",
      "description": "Quickly set up Whisper on Red Hat OpenShift AI to enable seamless speech-to-text transcription.",
      "readmePreview": "# Whisper Quickstart on OpenShift AI\n\nWelcome to the whisper quickstart on OpenShift AI! This project helps you quickly set up the required components...",
      "readmeContent": "# Whisper Quickstart on OpenShift AI\n\nWelcome to the whisper quickstart on OpenShift AI! This project helps you quickly set up the required components on Red Hat OpenShift AI and begin transcribing audio with minimal configuration.\n\nTo see how it's done, jump straight to [installation](#install). \n\n\n# Whisper Quickstart on OpenShift AI\n\nThis repository provides a Helm chart and supporting resources for a quick start with **speech-to-text transcription using Whisper** on **Red Hat OpenShift AI**.  \nThe deployment provisions all required components, including:\n\n- A **Workbench** in OpenShift AI with preloaded example notebooks.  \n- A deployed **Whisper-large-v3 model** served through a GPU-enabled runtime (`vLLM 0.9.2.2`).  \n- Pre-cloned quickstart repository with data samples and example workflows.  \n\nWith this setup, users can start experimenting with **automatic audio transcription** using either:  \n1. **Direct HTTP requests** to the deployed model.  \n2. **The OpenAI Python client** interface.  \n\n---\n\n## Architecture Overview\n\nThe following diagram illustrates the components deployed by this Helm chart:\n<p align=\"center\">\n  <img src=\"./assets/images/architecture-img.png\" alt=\"Architecture\" width=\"380\"/>\n</p>\n\n- **Workbench**: Hosts Jupyter notebooks with example workflows and preloaded data samples.  \n- **Whisper-large-v3 model**: Deployed through a GPU ServingRuntime (vLLM 0.9.2.2).  \n- **Two alternatives to trigger transcription request**:  \n  - **Option 1**: Send audio files via HTTP request.  \n  - **Option 2**: Use the OpenAI Python client.  \n\n---\n\n## Requirements \n\n### Recommended hardware requirements \n\n- GPU required : +24GiB vRAM\n- CPU cores: 16 cores   \n- Memory: 64Gi+ RAM \n- Storage: 15Gi\n\n### Minimum hardware requirements \n\n- GPU required : 1 x NVIDIA GPU with 24GiB vRAM\n- CPU cores: 8+ cores \n- Memory: 32Gi+ RAM \n- Storage: 10Gi \n\n### Required software  \n\n- Red Hat OpenShift Container Platform\n- Red Hat OpenShift AI\n    - KServe needs to be enabled\n- Red Hat OpenShift Serverless\n- Red Hat OpenShift Service Mesh 2\n- Red Hat - Authorino Operator \n- Nvidia GPU Operator \n- Node Feature Discovery Operator \n\n### Required permissions\n\n- Cluster admin permissions are required\n---\n\n## Install\n\n**Please note before you start**\n\nThis example was tested on Red Hat OpenShift 4.19.9 & Red Hat OpenShift AI 2.22.1.  \n\n### Clone the repository\n\n```\ngit clone https://github.com/rh-ai-quickstart/basic-speech-to-text-with-whisper.git && cd basic-speech-to-text-with-whisper/\n```\n\n### Install with Helm\nDeploy the Whisper Quickstart resources using Helm:\n\n```bash\nhelm install whisper-quickstart whisper-quickstart-1.0.0.tgz \\\n  --namespace whisper-quickstart --create-namespace \\\n  --set namespaces=\"whisper-quickstart\" \\\n  --set model.name=\"whisper-large-v3\" \\\n  --set model.storageUri=\"oci://quay.io/redhat-ai-services/modelcar-catalog:whisper-large-v3\"\n```\n\nThis command will:  \n- Create a namespace `whisper-quickstart`.  \n- Deploy the **Workbench** and preconfigured **Whisper-large-v3 model**.  \n- Clone the quickstart repository into the Workbench environment.  \n\n---\n\n## Verifying the Deployment\n\nAfter installation, verify that the resources are running:\n\n```bash\noc get all -n whisper-quickstart\n```\n\nEnsure that the following are present and ready:  \n- **Deployment**: `whisper-large-v3-predictor`  \n- **StatefulSet**: `whisper-workbench`  \n- **Services**: including the predictor and Workbench endpoints  \n\n![oc-get-all](images/basic-speech-to-text-with-whisper_oc-get-all.png)\n\n---\n\n## Accessing the Workbench\n\n1. Log in to the **OpenShift AI dashboard**.  \n2. Locate the **Workbench** named `whisper-workbench` under the `whisper-quickstart` data science project.  \n3. Open the Workbench ‚Äî the quickstart repository will already be cloned.  \n4. Inside, you will find two example notebooks to start transcribing audio.  \n\n![workbench](images/basic-speech-to-text-with-whisper_workbench.png)\n![notebook-page](images/basic-speech-to-text-with-whisper_notebook-page.png)\n\n---\n\n## Finding the Model Endpoint\n\nThe deployed Whisper model can be accessed within the cluster.  \nThere are two ways to find the service endpoint:\n\n### Option 1: Using the OpenShift Web Console\n- Navigate to **Networking ‚Üí Services ‚Üí whisper-large-v3-predictor**.  \n- Under **Service routing**, copy the **Hostname** (e.g., `whisper-large-v3-predictor.whisper-quickstart.svc.cluster.local`).  \n\n![Service-Routing](images/basic-speech-to-text-with-whisper_serving-route.png)\n\n### Option 2: Using the `oc` CLI\n```bash\noc get svc whisper-large-v3-predictor -n whisper-quickstart \\\n  -o jsonpath='\"http://{.metadata.name}.{.metadata.namespace}.svc.cluster.local:{.spec.ports[?(@.name==\"http\")].targetPort}{\"\\n\"}\"'\n```\n\nThis will return a URL similar to:\n\n```\nhttp://whisper-large-v3-predictor.whisper-quickstart.svc.cluster.local:8080\n```\n![oc-get-svc](images/basic-speech-to-text-with-whisper_oc-get-svc.png)\n---\n\n## Running Transcription Examples\n\nFrom the Workbench, open one of the provided notebooks:\n\n- **HTTP request**: Demonstrates sending local audio files to the deployed service endpoint.  \n- **OpenAI client**: Demonstrates using the OpenAI Python client library for transcription.  \n\nNote: Both notebooks are preconfigured with demo endpoints ‚Äî please update them before you run the cells.  \n\n![example-transcribe](images/basic-speech-to-text-with-whisper_example-transcribe.png)\n\n---\n\n## Cleanup\n\nTo uninstall all deployed resources:\n\n```bash\nhelm uninstall whisper-quickstart -n whisper-quickstart\noc delete namespace whisper-quickstart\n```\n\n---\n\n",
      "readmeFilename": "README.md",
      "images": [
        {
          "original": "./assets/images/oc-get-all.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/basic-speech-to-text-with-whisper/main/./assets/images/oc-get-all.png",
          "local": "images/basic-speech-to-text-with-whisper_oc-get-all.png",
          "alt": "oc-get-all"
        },
        {
          "original": "./assets/images/workbench.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/basic-speech-to-text-with-whisper/main/./assets/images/workbench.png",
          "local": "images/basic-speech-to-text-with-whisper_workbench.png",
          "alt": "workbench"
        },
        {
          "original": "./assets/images/notebook-page.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/basic-speech-to-text-with-whisper/main/./assets/images/notebook-page.png",
          "local": "images/basic-speech-to-text-with-whisper_notebook-page.png",
          "alt": "notebook-page"
        },
        {
          "original": "./assets/images/serving-route.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/basic-speech-to-text-with-whisper/main/./assets/images/serving-route.png",
          "local": "images/basic-speech-to-text-with-whisper_serving-route.png",
          "alt": "Service-Routing"
        },
        {
          "original": "./assets/images/oc-get-svc.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/basic-speech-to-text-with-whisper/main/./assets/images/oc-get-svc.png",
          "local": "images/basic-speech-to-text-with-whisper_oc-get-svc.png",
          "alt": "oc-get-svc"
        },
        {
          "original": "./assets/images/example-transcribe.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/basic-speech-to-text-with-whisper/main/./assets/images/example-transcribe.png",
          "local": "images/basic-speech-to-text-with-whisper_example-transcribe.png",
          "alt": "example-transcribe"
        }
      ],
      "githubLink": "https://github.com/rh-ai-quickstart/basic-speech-to-text-with-whisper",
      "categories": [
        "Jupyter Notebook"
      ],
      "stars": 0,
      "lastUpdated": "9/18/2025"
    },
    {
      "id": "lls-observability",
      "title": "Lls Observability",
      "description": "Llama Stack Observability quickstart",
      "readmePreview": "# Llama Stack Telemetry & Observability\n\nObservability & telemetry quickstart for both Llama-Stack and OpenShift AI.\n\nThis repository provides helm ch...",
      "readmeContent": "# Llama Stack Telemetry & Observability\n\nObservability & telemetry quickstart for both Llama-Stack and OpenShift AI.\n\nThis repository provides helm charts for deploying AI services with telemetry and observability on Llama-Stack, OpenShift and OpenShift AI.\n\nJump straight to [installation](#installation) to get started quickly.\n\n## Table of Contents\n\n- [Detailed description](#detailed-description)\n    - [Architecture](#architecture)\n    - [Components](#components)\n- [Requirements](#requirements)\n- [Installation](#installation)\n- [Advanced Usage](#advanced-usage)\n    - [MaaS Integration (Optional)](#maas-integration-optional)\n    - [Uninstall](#uninstall)\n- [References](#references)\n\n## Detailed description\n\nThis telemetry and observability quickstart addresses the critical needs for Large Language Model (LLM) infrastructure. As AI workloads become more complex, organizations need:\n\n- **AI observability** into model performance, resource utilization, and distributed tracing\n- **Standardized deployment patterns** for consistent, scalable AI service delivery\n- **Enterprise-grade monitoring** with OpenShift-native observability tools\n\nThis repository provides helm charts for both the monitoring infrastructure and AI service deployments needed to run Llama Stack reliably in production environments.\n\n## FSI Use Case: Financial AI Audit System\n\nThis quickstart includes a **Financial Services Industry (FSI) compliance demo** that demonstrates auditability and traceability for AI-powered financial applications.\n\nThe demo shows how distributed tracing captures every step of an AI-driven loan decision process:\n\n![FSI Tracing Flow](images/lls-observability_traces4.png)\n\nThis provides complete audit trails for regulatory compliance and risk management in financial AI systems.\n\n### Architecture\n\nThe proposed observability & telemetry architecture:\n\n![observability architecture diagram](images/lls-observability_architecture.png)\n\nThis architecture demonstrates the complete observability ecosystem, from AI workload telemetry collection through distributed tracing to comprehensive monitoring dashboards.\n\n### Components\n\nAll components are organized by dependency layers in the [`./helm/`](./helm/) directory:\n\n#### Phase 1: Operators (`./helm/01-operators/`)\n- **[`cluster-observability-operator`](./helm/01-operators/cluster-observability-operator/)** - PodMonitor/ServiceMonitor CRDs and UI plugins\n- **[`grafana-operator`](./helm/01-operators/grafana-operator/)** - Grafana operator for visualization and dashboard management\n- **[`otel-operator`](./helm/01-operators/otel-operator/)** - Red Hat Build of OpenTelemetry operator\n- **[`tempo-operator`](./helm/01-operators/tempo-operator/)** - Distributed tracing backend operator\n\n#### Phase 2: Observability Infrastructure (`./helm/02-observability/`)\n- **[`tempo`](./helm/02-observability/tempo/)** - Distributed tracing backend with S3-compatible storage\n- **[`otel-collector`](./helm/02-observability/otel-collector/)** - OpenTelemetry collector configurations for telemetry collection and processing\n- **[`grafana`](./helm/02-observability/grafana/)** - Visualization and dashboard management with pre-built dashboards\n- **[`uwm`](./helm/02-observability/uwm/)** - User Workload Monitoring with PodMonitors for VLLM and AI workloads\n- **[`distributed-tracing-ui-plugin`](./helm/02-observability/distributed-tracing-ui-plugin/)** - OpenShift console integration for tracing\n\n#### Phase 3: AI Services (`./helm/03-ai-services/`)\n- **[`llama-stack-instance`](./helm/03-ai-services/llama-stack-instance/)** - Complete Llama Stack Instance with configurable endpoints\n- **[`llama3.2-3b`](./helm/03-ai-services/llama3.2-3b/)** - Llama 3.2 3B model deployment on vLLM\n- **[`llama-stack-playground`](./helm/03-ai-services/llama-stack-playground/)** - Interactive Llama-Stack web interface for testing\n- **[`llama-guard`](./helm/03-ai-services/llama-guard/)** - Llama Guard 1B for providing Safety / Shield mechanisms \n\n#### Phase 4: MCP Servers (`./helm/04-mcp-servers/`)\n- **[`mcp-weather`](./helm/04-mcp-servers/mcp-weather/)** - MCP weather service\n- **[`hr-api`](./helm/04-mcp-servers/hr-api/)** - MCP HR API demonstration service\n- **[`openshift-mcp`](./helm/04-mcp-servers/openshift-mcp/)** - MCP OpenShift/Kubernetes operations service\n\n### Observability in Action\n\nThe telemetry and observability stack provides comprehensive visibility into AI workload performance and distributed system behavior.\n\n#### Distributed Tracing Examples\n\n![Llama Stack Request Tracing](images/lls-observability_traces1.png)\n\n**End-to-End Request Tracing**: Complete visibility into AI inference request flows through the Llama Stack infrastructure.\n\n![Detailed Service Interaction Tracing](images/lls-observability_traces2.png)\n\n**Create Agent from LlamaStack Tracing**: Detailed trace view showing complex interactions between different services in the AI stack.\n\nThese traces provide insights into:\n- Request latency and service dependencies\n- Error tracking and performance bottlenecks\n- Load distribution across model endpoints\n\n## Requirements\n\n### Minimum Hardware Requirements\n\n- **CPU**: 8+ cores recommended for full stack deployment\n- **Memory**: 16GB+ RAM for monitoring stack, additional memory based on AI workload requirements\n- **Storage**: 100GB+ for observability data retention\n- **GPU**: NVIDIA GPU required for AI model inference (varies by model size)\n\n### Required Software\n\n- **OpenShift 4.12+** or **Kubernetes 1.24+**\n- **OpenShift AI 2.19 onwards**\n- **Helm 3.8+** for chart deployment\n- **oc CLI** or **kubectl** for cluster management\n\n### Required Permissions\n\n- **Cluster Admin** - Required for operator installation and observability stack setup\n- **GPU Access** - Required for AI workload deployment\n\n## Installation\n\n### Quick Start - Automated Installation\n\n**Option 1: Complete Stack (Recommended)**\n```bash\n# Run the full installation script\n./scripts/install-full-stack.sh\n```\n\n**Option 2: Phase-by-Phase Installation**\n```bash\n# Phase 1: Install operators\n./scripts/install-operators.sh\n\n# Phase 2: Deploy observability infrastructure\n./scripts/deploy-observability.sh\n\n# Phase 3: Deploy AI workloads\n./scripts/deploy-ai-workloads.sh\n```\n\n**Option 3: Using Makefile (Optional)**\n```bash\n# Install everything in one command\nmake install-all\n\n# Or phase-by-phase\nmake install-operators      # Phase 1\nmake deploy-observability   # Phase 2  \nmake deploy-ai              # Phase 3\n```\n\n## Advanced Usage\n\n### Manual Step-by-Step Installation\n\nFor users who prefer to understand each step or need to customize the installation.\n\nSet these environment variables before running the installation commands:\n\n```bash\nexport OBSERVABILITY_NAMESPACE=\"observability-hub\"\nexport UWM_NAMESPACE=\"openshift-user-workload-monitoring\"\nexport AI_SERVICES_NAMESPACE=\"llama-serve\"\n```\n\nLaunch this instructions:\n\n```bash\n# 1. Create required namespaces\noc create namespace ${OBSERVABILITY_NAMESPACE}\noc create namespace ${UWM_NAMESPACE}\noc create namespace ${AI_SERVICES_NAMESPACE}\n\n# 2. Install required operators\nhelm install cluster-observability-operator ./helm/01-operators/cluster-observability-operator\nhelm install grafana-operator ./helm/01-operators/grafana-operator\nhelm install otel-operator ./helm/01-operators/otel-operator\nhelm install tempo-operator ./helm/01-operators/tempo-operator\n\n# 3. Wait for operators to be ready\noc wait --for=condition=Ready pod -l app.kubernetes.io/name=cluster-observability-operator -n openshift-cluster-observability-operator --timeout=300s\noc wait --for=condition=Ready pod -l app.kubernetes.io/name=observability-operator -n openshift-cluster-observability-operator --timeout=300s\n\n# 4. Deploy observability infrastructure\nhelm install tempo ./helm/02-observability/tempo -n ${OBSERVABILITY_NAMESPACE}\nhelm install otel-collector ./helm/02-observability/otel-collector -n ${OBSERVABILITY_NAMESPACE}\nhelm install grafana ./helm/02-observability/grafana -n ${OBSERVABILITY_NAMESPACE}\n\n# 5. Enable User Workload Monitoring for AI workloads\nhelm template uwm ./helm/02-observability/uwm -n ${OBSERVABILITY_NAMESPACE} | oc apply -f-\n\n# Verify UWM setup\noc get configmap user-workload-monitoring-config -n ${UWM_NAMESPACE}\noc get podmonitors -n ${OBSERVABILITY_NAMESPACE}\n\n# 6. Deploy AI workloads\n# Deploy MCP servers in AI services namespace\nhelm install mcp-weather ./helm/04-mcp-servers/mcp-weather -n ${AI_SERVICES_NAMESPACE}\nhelm install hr-api ./helm/04-mcp-servers/hr-api -n ${AI_SERVICES_NAMESPACE}\n\n# Milvus vector database is configured inline within llama-stack-instance\n# No external Milvus deployment needed\n\n# Deploy AI services in AI services namespace  \nhelm install llama3-2-3b ./helm/03-ai-services/llama3.2-3b -n ${AI_SERVICES_NAMESPACE} \\\n  --set model.name=\"meta-llama/Llama-3.2-3B-Instruct\" \\\n  --set resources.limits.\"nvidia\\.com/gpu\"=1\n\nhelm install llama-stack-instance ./helm/03-ai-services/llama-stack-instance -n ${AI_SERVICES_NAMESPACE} \\\n  --set 'mcpServers[0].name=weather' \\\n  --set 'mcpServers[0].uri=http://mcp-weather.${AI_SERVICES_NAMESPACE}.svc.cluster.local:80' \\\n  --set 'mcpServers[0].description=Weather MCP Server for real-time weather data'\n\nhelm install llama-stack-playground ./helm/03-ai-services/llama-stack-playground -n ${AI_SERVICES_NAMESPACE} \\\n  --set playground.llamaStackUrl=\"http://llama-stack-instance.${AI_SERVICES_NAMESPACE}.svc.cluster.local:80\"\n\nhelm install llama-guard ./helm/03-ai-services/llama-guard -n ${AI_SERVICES_NAMESPACE}\n\n# 7. Enable tracing UI\nhelm install distributed-tracing-ui-plugin ./helm/02-observability/distributed-tracing-ui-plugin\n```\n\n### MaaS Integration (Optional)\n\n**Model-as-a-Service (MaaS) Remote Models**\n\nThe Llama Stack can optionally integrate with external MaaS (Models as a Server) to access remote models without local GPU requirements.\n\n**Configuration Options:**\n```yaml\nmaas:\n  enabled: true                    # Enable MaaS provider\n  apiToken: \"your-api-token\"      # Authentication token\n  url: \"https://endpoint.com/v1\"  # MaaS endpoint URL\n  maxTokens: 200000               # Maximum token limit\n  tlsVerify: false                # TLS verification\n  modelId: \"model-name\"           # Specific MaaS model identifier\n```\n\n### Uninstall\n\n**Complete Stack Uninstall**\n```bash\n# Uninstall everything in reverse order\nmake clean\n```\n\n## References\n\n### Documentation\n- [OpenShift Distributed Tracing](https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/distributed_tracing/index)\n- [OpenShift Observability](https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html/monitoring)\n- [User Workload Monitoring](https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html/monitoring/enabling-monitoring-for-user-defined-projects)\n\n### Related Projects\n- [Llama Stack](https://github.com/meta-llama/llama-stack)\n- [vLLM](https://github.com/vllm-project/vllm)\n- [OpenTelemetry](https://opentelemetry.io/)\n- [Grafana](https://grafana.com/)\n- [Tempo](https://grafana.com/oss/tempo/)\n\n### Community\n- [Red Hat AI Quickstarts](https://github.com/rh-ai-quickstart)\n- [OpenShift AI Documentation](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_cloud_service)\n",
      "readmeFilename": "README.md",
      "images": [
        {
          "original": "assets/images/traces4.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/lls-observability/main/assets/images/traces4.png",
          "local": "images/lls-observability_traces4.png",
          "alt": "FSI Tracing Flow"
        },
        {
          "original": "./assets/images/architecture.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/lls-observability/main/./assets/images/architecture.png",
          "local": "images/lls-observability_architecture.png",
          "alt": "observability architecture diagram"
        },
        {
          "original": "assets/images/traces1.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/lls-observability/main/assets/images/traces1.png",
          "local": "images/lls-observability_traces1.png",
          "alt": "Llama Stack Request Tracing"
        },
        {
          "original": "assets/images/traces2.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/lls-observability/main/assets/images/traces2.png",
          "local": "images/lls-observability_traces2.png",
          "alt": "Detailed Service Interaction Tracing"
        }
      ],
      "githubLink": "https://github.com/rh-ai-quickstart/lls-observability",
      "categories": [
        "Jupyter Notebook"
      ],
      "stars": 2,
      "lastUpdated": "9/17/2025"
    },
    {
      "id": "ai-virtual-agent",
      "title": "Ai Virtual Agent",
      "description": "No description available",
      "readmePreview": "# AI Virtual Agent Quickstart\n\nA platform for creating and managing AI-powered virtual agents with knowledge base integration, built on top of LlamaSt...",
      "readmeContent": "# AI Virtual Agent Quickstart\n\nA platform for creating and managing AI-powered virtual agents with knowledge base integration, built on top of LlamaStack.\n\n## What is this?\n\nThis platform provides the tools to build and deploy conversational AI agents that can:\n\n- **Access knowledge bases** - Upload documents and create searchable knowledge bases for RAG (Retrieval-Augmented Generation)\n- **Use tools** - Integrate web search, databases, and custom tools through the Model Context Protocol (MCP)\n- **Apply guardrails** - Built-in safety measures and content filtering\n- **Scale in production** - Kubernetes-ready architecture\n\n### Key Features\n\nü§ñ **Agent Management** - Create and configure AI agents with different capabilities\nüìö **Knowledge Integration** - Document search and question answering via RAG\nüí¨ **Real-time Chat** - Streaming conversations with session history\nüîß **Tool Ecosystem** - Built-in tools plus extensible MCP server support\nüõ°Ô∏è **Safety Controls** - Configurable guardrails and content filtering\n\n## Quick Start\n\n### Local Development\n\nFor local containerized development (without cluster):\n\nüìñ **[‚Üí See Local Development Guide](DEVELOPMENT.md)**\n\n### Local Development\n\nFor local development setup:\n\n```bash\n# Navigate to local deployment directory\ncd deploy/local\n\n# Start all services with Docker Compose\nmake compose-up\n\n# Or start step-by-step:\n# 1. Start database (automatically initializes with permissions)\npodman compose up -d\n# or with Docker:\n# docker-compose up -d\n\n# 2. Start backend\ncd ../../backend && python -m venv venv && source venv/bin/activate\npip install -r requirements.txt && alembic upgrade head\nuvicorn main:app --reload &\n\n# 3. Start frontend\ncd ../frontend && npm install && npm run dev\n```\n\n> **Note**: The PostgreSQL database is automatically initialized with proper permissions. Works with both Docker and Podman. No manual database setup needed!\n\n**Access your app:**\n- Frontend: http://localhost:5173\n- API: http://localhost:8000\n- Docs: http://localhost:8000/docs\n\n### Cluster Deployment\n\nFor production installation on Kubernetes/OpenShift:\n\n```bash\n# Navigate to cluster deployment directory\ncd deploy/cluster\n\n# Install with interactive prompts for configuration\nmake install NAMESPACE=your-namespace\n\n# Or set environment variables and install\nexport NAMESPACE=ai-virtual-agent\nexport HF_TOKEN=your-huggingface-token\nmake install\n```\n\nüìñ **[Full Installation Guide ‚Üí](INSTALLING.md)**\n\n## Project Structure\n\n```\nai-virtual-agent/\n‚îú‚îÄ‚îÄ frontend/           # React UI with PatternFly components\n‚îú‚îÄ‚îÄ backend/            # FastAPI server with PostgreSQL\n‚îú‚îÄ‚îÄ mcpservers/         # Custom MCP tool servers\n‚îú‚îÄ‚îÄ docs/               # Architecture and API documentation\n‚îú‚îÄ‚îÄ deploy/\n‚îÇ   ‚îú‚îÄ‚îÄ cluster/        # Kubernetes/Helm cluster deployment\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ helm/       # Helm chart files\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scripts/    # Cluster deployment scripts\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Containerfile # Cluster container image\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Makefile    # Cluster deployment commands\n‚îÇ   ‚îî‚îÄ‚îÄ local/          # Local development deployment\n‚îÇ       ‚îú‚îÄ‚îÄ compose.dev.yaml # Docker Compose for local dev\n‚îÇ       ‚îú‚îÄ‚îÄ dev/        # Local development configs\n‚îÇ       ‚îî‚îÄ‚îÄ Makefile    # Local development commands\n‚îî‚îÄ‚îÄ tests/              # Integration test suite\n```\n\n## Architecture Overview\n\nThe platform integrates several components:\n\n- **React Frontend** - Web interface for agent and chat management\n- **FastAPI Backend** - API server handling business logic and data persistence\n- **LlamaStack** - AI platform managing models, agents, and inference\n- **PostgreSQL + pgvector** - Data storage with vector search capabilities\n- **Kubernetes Pipeline** - Document processing and knowledge base ingestion\n\n![Architecture](images/ai-virtual-agent_architecture.png)\n\nüìñ **[Detailed Architecture ‚Üí](docs/virtual-agents-architecture.md)**\n\n## Getting Started Guides\n\n### üë©‚Äçüíª **For Developers**\n- **[Local Development Guide](DEVELOPMENT.md)** - Containerized development environment (without cluster)\n- **[Contributing Guide](CONTRIBUTING.md)** - Development setup and workflow\n- **[Backend API Reference](docs/api-reference.md)** - Complete API documentation\n- **[Frontend Architecture](frontend/README.md)** - UI components and patterns\n\n### üöÄ **For Deployment**\n- **[Installation Guide](INSTALLING.md)** - Production deployment on Kubernetes\n- **[Agent Templates](docs/agent-templates-ingestion.md)** - Pre-built agent configurations\n- **[Knowledge Base Setup](docs/knowledge-base-architecture.md)** - Document processing pipeline\n\n### üîß **For Integration**\n- **[MCP Servers](mcpservers/README.md)** - Building custom tool integrations\n- **[Testing Guide](tests/README.md)** - Running integration tests\n- **[API Reference](docs/api-reference.md)** - Backend API endpoints\n\n## Example Use Cases\n\n**Customer Support Agent**\n```typescript\nconst agent = await createAgent({\n  name: \"Support Bot\",\n  model: \"llama3.1-8b-instruct\",\n  knowledge_bases: [\"support-docs\"],\n  tools: [\"builtin::rag\", \"builtin::web_search\"]\n});\n```\n\n**Domain Expert (Banking)**\n```typescript\nconst expert = await initializeAgentTemplate({\n  template: \"commercial_banker\",\n  knowledge_bases: [\"banking-regulations\"]\n});\n```\n\n## Development Commands\n\n**Local Development:**\n```bash\ncd deploy/local\n\n# Start everything locally with Docker Compose\nmake compose-up\n\n# Stop all services\nmake compose-down\n\n# View logs from development services\nmake compose-logs\n\n# Restart development services\nmake compose-restart\n\n# Show status of development services\nmake compose-status\n```\n\n**Cluster Deployment:**\n```bash\ncd deploy/cluster\n\n# Install on cluster\nmake install NAMESPACE=your-namespace\n\n# Uninstall from cluster\nmake uninstall NAMESPACE=your-namespace\n\n# Check status\nmake install-status NAMESPACE=your-namespace\n```\n\n> Note: All Makefile targets automatically load environment variables from a `.env` file in the repository root if it exists. No manual `export` is required for common workflows.\n\n### Environment setup (.env)\n\nCreate a `.env` file in the repository root to configure your local environment. All Makefile targets will dynamically load this file if present:\n\n```bash\ncp .env.example .env\n# then edit `.env` as needed\n```\n\nAt minimum, set:\n\n```bash\nDATABASE_URL=postgresql+asyncpg://admin:password@localhost:5432/ai_virtual_agent\n```\n\nOptional toggles:\n\n```bash\n# Skip attachments bucket initialization/access during local dev\nDISABLE_ATTACHMENTS=true\n\n# Provide admin bootstrap for Alembic seeding (optional)\n# ADMIN_USERNAME=admin\n# ADMIN_EMAIL=admin@change.me\n```\n\n### Attachments (optional dependency)\n\nIf you plan to use file attachments locally or see this error during tests:\n\n```\nImportError: failed to find libmagic. Check your installation\n```\n\ninstall the MIME detection dependency (libmagic) and Python binding:\n\n- macOS (Homebrew):\n  - `brew install file`\n  - `pip install python-magic`\n- Ubuntu/Debian:\n  - `sudo apt-get install libmagic1` (or `libmagic-dev`)\n  - `pip install python-magic`\n- Fedora/RHEL:\n  - `sudo dnf install file libmagic`\n  - `pip install python-magic`\n\nNotes:\n- Unit tests import parts of the attachments stack. Without libmagic installed you can still proceed by installing the packages above.\n- If you‚Äôre not using attachments in local dev, you can set `DISABLE_ATTACHMENTS=true` in `.env` to skip bucket-related startup paths.\n\n\n## Community & Support\n\n- **üêõ Issues** - [Report bugs and request features](https://github.com/rh-ai-quickstart/ai-virtual-agent/issues)\n- **üí¨ Discussions** - [Ask questions and share ideas](https://github.com/rh-ai-quickstart/ai-virtual-agent/discussions)\n- **ü§ù Contributing** - See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines\n- **üìö Documentation** - Browse `/docs` for detailed guides\n\n## License\n\n[MIT License](LICENSE) - Built with ‚ù§Ô∏è by the Red Hat Ecosystem App Engineering team\n",
      "readmeFilename": "README.md",
      "images": [
        {
          "original": "docs/images/architecture.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/ai-virtual-agent/main/docs/images/architecture.png",
          "local": "images/ai-virtual-agent_architecture.png",
          "alt": "Architecture"
        }
      ],
      "githubLink": "https://github.com/rh-ai-quickstart/ai-virtual-agent",
      "categories": [
        "Python"
      ],
      "stars": 5,
      "lastUpdated": "9/17/2025"
    },
    {
      "id": "openshift-ai-observability-summarizer",
      "title": "Openshift Ai Observability Summarizer",
      "description": "AI quickstart that provides interactive dashboard to analyze AI Model Performance as well as Openshift metrics collected from Prometheus",
      "readmePreview": "# OpenShift AI Observability Summarizer\n\n[![CNCF Compatible](https://img.shields.io/badge/CNCF%20Compatible-Yes-blue.svg)](https://www.cncf.io/)\n[![Li...",
      "readmeContent": "# OpenShift AI Observability Summarizer\n\n[![CNCF Compatible](images/openshift-ai-observability-summarizer_CNCF%20Compatible-Yes-blue.svg)](https://www.cncf.io/)\n[![License: MIT](images/openshift-ai-observability-summarizer_License-MIT-yellow.svg)](LICENSE)\n[![Build Status](images/openshift-ai-observability-summarizer_badge.svg)](https://github.com/rh-ai-quickstart/openshift-ai-observability-summarizer/actions)\n\n\n<img src=\"docs/img/logo.png\" alt=\"OpenShift AI Observability Summarizer\" width=\"200\"/>\n\n\n[Design Document](https://docs.google.com/document/d/1bXBCL4fbPlRqQxwhGX1p12CS_E6-9oOyFnYSpbQskyI/edit?usp=sharing)\n\n## Overview\n\nOpenShift AI Observability Summarizer is an **open source, CNCF-style project** for advanced monitoring and automated summarization of AI model and OpenShift cluster metrics. It provides an interactive dashboard for analyzing metrics collected from Prometheus and generating human-readable, AI-powered insights and reports.\n\n- **Monitors vLLM deployments, OpenShift fleet health, and GPU utilization**\n- **Generates actionable summaries using LLMs**\n- **Supports alerting, notifications, and exportable reports**\n\n---\n\n## Table of Contents\n\n- [Features](#features)\n- [GPU Monitoring](#gpu-monitoring)\n- [Architecture](#architecture)\n- [Getting Started](#getting-started)\n- [Usage](#usage)\n- [Build & Deploy](#build--deploy)\n- [Local Development](#local-development-via-port-forwarding)\n- [Running Tests with Pytest](#running-tests-with-pytest)\n- [GitHub Actions CI/CD](#github-actions-cicd)\n- [Semantic Versioning](#semantic-versioning)\n- [Helm Charts Documentation](#helm-charts-documentation)\n- [Contributing](#contributing)\n- [Community](#community)\n- [License](#license)\n\n---\n\n\n## Features\n\n### **1. vLLM Monitoring**\n- Visualize core vLLM metrics (GPU usage, latency, request volume, etc.)\n- Dynamic DCGM GPU metrics discovery (temperature, power, memory)\n- Real-time performance analysis and anomaly detection\n\n### **2. OpenShift Fleet Monitoring** \n- Cluster-wide and namespace-scoped metric analysis\n- GPU & Accelerators fleet monitoring with comprehensive DCGM metrics\n- Workloads, Storage, Networking, and Application Services monitoring\n- Enhanced unit formatting (¬∞C, Watts, GB, MB/s, etc.)\n\n### **3. AI-Powered Insights**\n- Generate summaries using fine-tuned Llama models\n- Chat with an MLOps assistant based on real metrics\n- Support for both internal and external LLM models\n\n### **4. Report Generation**\n- Export analysis as HTML, PDF, or Markdown reports\n- Time-series charts and metric visualizations\n- Automated metric calculations and trend analysis\n\n### **5. Alerting & Notifications**\n- Set up alerts for vLLM models and OpenShift metrics  \n- Slack notifications when alerts are triggered\n- Custom alert thresholds and conditions\n\n### **6. Distributed Tracing Integration**\n- Tracing support with OpenTelemetry and Tempo to monitor request flows across your AI services.\n\n### **6. AI Assistant Integration (MCP Server)**\n- **Model Context Protocol (MCP) server** for AI assistants (Claude Desktop, Cursor IDE)\n- **Natural language analysis** - ask questions like \"What is the GPU temperature?\"\n- **Real-time data access** - connects directly to Prometheus/Thanos\n- **AI-powered insights** - full LLM integration for intelligent metric analysis\n\nüìñ **Quick Setup**: \n```bash\ncd src/mcp_server && python setup_integration.py\n```\nSee [`src/mcp_server/README.md`](src/mcp_server/README.md) for complete documentation.\n\n---\n\n### GPU Monitoring\n\n### **DCGM Metrics Support**\nAutomatically discovers and monitors:\n- **Temperature**: GPU core and memory temperature (¬∞C)\n- **Power**: Real-time power consumption (Watts)  \n- **Memory**: GPU memory usage (GB) and utilization (%)\n- **Energy**: Total energy consumption (Joules)\n- **Performance**: GPU utilization, clock speeds (MHz)\n\n### **Fleet View**\nMonitor GPU health across your entire OpenShift cluster:\n- Cluster-wide GPU temperature averaging\n- Power consumption trends\n- Memory usage patterns\n- Vendor/model detection and inventory\n\n---\n\n## Architecture\n\n### Core Components\n\n### **Monitoring Stack**\n- **Prometheus**: Prometheus scrapes the /metrics endpoint offered by vLLM. It can store metrics itself in its own time-series database on a local disk which is\n                  highly optimized for fast queries on recent data. This is perfect for real-time monitoring and alerting but is not\n                  ideal for long term and multi-year storage. This is where Thanos Querier comes in.\n- **Thanos Querier**: Extends Prometheus by solving the problem of long-term retention. Thanos is capable of taking data blocks that Prometheus saves to\n                      its local disk and uploading them to inexpensive and durable object storage, like Amazon S3, Google Cloud Storage, or Azure Blob Storage.\n                      Querier gives you a cost-effective way of retaining years of metrics data available for historical analysis and trend reporting.\n                      Querier sidecars run alongside your Prometheus servers, providing access to real-time and recent metrics.                  \n- **DCGM**: GPU monitoring and telemetry\n- **Streamlit UI**: Multi-dashboard interface (vLLM, OpenShift, Chat)\n- **FastAPI Backend**: metrics-api for web UI and report generation\n- **MCP Server**: Model Context Protocol server for AI assistant integration\n- **Report Generator**: PDF/HTML/Markdown export capabilities\n- **llm-service:** LLM inference (Llama models)\n- **llama-stack:** Backend API\n- **vLLM:** Model serving, exports Prometheus /metrics\n- **Prometheus/Thanos:** Metrics scraping, long-term storage\n- **DCGM:** GPU monitoring\n- **Streamlit UI:** Multi-dashboard frontend\n- **MCP:** Metric Collection & Processing backend\n- **Report Generator:** PDF/HTML/Markdown export\n\n![Architecture](images/openshift-ai-observability-summarizer_arch-2.jpg)\n\n### **Key Components**\n1. **vLLM Dashboard**: Monitor model performance, GPU usage, latency\n2. **OpenShift Dashboard**: Fleet monitoring with cluster-wide and namespace views\n3. **Chat Interface**: Interactive Q&A with metrics-aware AI assistant\n4. **MCP Server**: AI assistant integration via Model Context Protocol\n5. **Report Generator**: Automated analysis reports in multiple formats\n\n---\n\n## Getting Started\n\n### Prerequisites\n\n- OpenShift cluster with GPU nodes (for DCGM metrics)\n- `oc` CLI with cluster-admin permissions\n- `helm` v3.x\n- `yq` (YAML processor)\n- Deployed Prometheus/Thanos\n- Operators for distributed tracing\n  - Red Hat Build of OpenTelemetry Operator\n  - Tempo Operator\n  - Cluster Observability Operator\n- (Optional) DCGM exporter for GPU monitoring\n- (Optional) Slack Webhook URL for alerting ([How to create a Webhook for your Slack Workspace](https://api.slack.com/messaging/webhooks))\n\n\n### Installing the OpenShift AI Observability Summarizer\n\nUse the included `Makefile` to install everything:\n```bash\nmake install NAMESPACE=your-namespace\n```\nThis will install the project with the default LLM deployment, `llama-3-2-3b-instruct`.\n\n### Using an Existing Model\n\nTo use an existing model instead of deploying a new one, specify `LLM_URL` as the model service URL:\n\n```bash\n# URL with port (no processing applied)\nmake install LLM_URL=http://llama-3-2-3b-instruct-predictor.dev.svc.cluster.local:8080/v1 NAMESPACE=your-namespace\n\n# URL without port (automatically adds :8080/v1)\nmake install LLM_URL=http://llama-3-2-3b-instruct-predictor.dev.svc.cluster.local NAMESPACE=your-namespace\n```\n\n**URL Processing**: If the `LLM_URL` doesn't contain a port (`:PORT` format), the system will automatically append `:8080/v1` to the URL. This simplifies configuration while maintaining flexibility for custom ports.\n\n**Token Management**: When `LLM_URL` is specified, the system will not prompt for a Hugging Face token since you're using an existing model that doesn't require new model deployment.\n\nThis is useful when:\n- You already have a model deployed in your cluster\n- You want to share a model across multiple namespaces\n- You prefer not to deploy redundant model instances\n- You want to avoid unnecessary token prompts for external models\n\n### Choosing different models\n\nTo see all available models:\n```bash\nmake list-models\n```\n```\n(Output)\nmodel: llama-3-1-8b-instruct (meta-llama/Llama-3.1-8B-Instruct)\nmodel: llama-3-2-1b-instruct (meta-llama/Llama-3.2-1B-Instruct)\nmodel: llama-3-2-1b-instruct-quantized (RedHatAI/Llama-3.2-1B-Instruct-quantized.w8a8)\nmodel: llama-3-2-3b-instruct (meta-llama/Llama-3.2-3B-Instruct)\nmodel: llama-3-3-70b-instruct (meta-llama/Llama-3.3-70B-Instruct)\nmodel: llama-guard-3-1b (meta-llama/Llama-Guard-3-1B)\nmodel: llama-guard-3-8b (meta-llama/Llama-Guard-3-8B)\n```\nYou can use the `LLM` flag during installation to set a model from this list for deployment:\n```\nmake install NAMESPACE=your-namespace LLM=llama-3-2-3b-instruct \n```\n\n### With GPU tolerations\n```bash\nmake install NAMESPACE=your-namespace LLM=llama-3-2-3b-instruct LLM_TOLERATION=\"nvidia.com/gpu\"\n```\n\n### With safety models\n```bash\nmake install NAMESPACE=your-namespace \\\n  LLM=llama-3-2-3b-instruct LLM_TOLERATION=\"nvidia.com/gpu\" \\\n  SAFETY=llama-guard-3-8b SAFETY_TOLERATION=\"nvidia.com/gpu\"\n```\n\n### With alerting\n```bash\nmake install NAMESPACE=your-namespace ALERTS=TRUE\n```\nEnabling alerting will deploy alert rules, a cron job to monitor vLLM metrics, and AI-powered Slack notifications.\n\n### Accessing the Application\n\nThe default configuration deploys:\n- **llm-service** - LLM inference \n- **llama-stack** - Backend API\n- **pgvector** - Vector database\n- **metric-mcp** - Metrics collection & processing API\n- **metric-ui** - Multi-dashboard Streamlit interface\n- **OpenTelemetry Collector** - Distributed tracing collection\n- **Tempo** - Trace storage and analysis\n- **MinIO** - Object storage for traces\n\nNavigate to your **Openshift Cluster --> Networking --> Route** and you should be able to see the route for your application. You can also navigate to **Observe > Traces** in the OpenShift console to view traces.\n\nOn terminal you can access the route with -\n\n```bash\noc get route\n\nNAME              HOST/PORT                                                               PATH   SERVICES        PORT   TERMINATION     WILDCARD\nmetric-ui-route   metric-ui-route-llama-1.apps.tsisodia-spark.2vn8.p1.openshiftapps.com          metric-ui-svc   8501   edge/Redirect   None\n```\n\n### Openshift Summarizer Dashboard \n![UI](images/openshift-ai-observability-summarizer_os.png)\n\n### vLLM SUmmarizer Dashboard \n![UI](images/openshift-ai-observability-summarizer_vllm.png)\n\n### Chat with Prometheus \n![UI](images/openshift-ai-observability-summarizer_chat.png)\n\n### Report Generated \n![UI](images/openshift-ai-observability-summarizer_report.png)\n\n\nTo uninstall:\n\n```bash\nmake uninstall NAMESPACE=metric-summarizer\n```\n\n---\n\n## Usage\n\n### **Multi-Dashboard Interface**\nAccess via the OpenShift route: `oc get route ui`\n\n#### **vLLM Metric Summarizer**\n1. Select your AI model and namespace\n2. Choose time range for analysis  \n3. Click **Analyze Metrics** for AI-powered insights\n4. Download reports in HTML/PDF/Markdown format\n\n#### **OpenShift Metrics Dashboard**\n1. Choose metric category (Fleet Overview, GPU & Accelerators, etc.)\n2. Select scope: Cluster-wide or Namespace-scoped\n3. Analyze performance with AI-generated summaries\n4. Monitor GPU temperature, power usage, and utilization across fleet\n\n#### **Chat with Prometheus**\n1. Ask natural language questions about your metrics\n2. Get specific PromQL queries and insights\n3. Interactive troubleshooting with metrics context\n\n#### **Key Monitoring Categories**\n- **Fleet Overview**: Pods, CPU, Memory, GPU temperature\n- **GPU & Accelerators**: Temperature, power, utilization, memory (GB)  \n- **Workloads & Pods**: Container metrics, restarts, failures\n- **Storage & Networking**: I/O rates, network throughput\n- **Application Services**: HTTP metrics, endpoints, errors\n\n#### Generate Reports\nYou can generate detailed metric reports in multiple formats directly from the dashboard:\n\n- **HTML Report**: Interactive and visually rich, suitable for sharing or archiving.\n- **PDF Report**: Print-ready, ideal for documentation or compliance needs.\n- **Markdown Report**: Lightweight, easy to edit or integrate into wikis and documentation.\n\nTo generate a report:\n1. Complete your analysis in either the vLLM or OpenShift dashboard.\n2. Click the **Download Report** button.\n3. Choose your preferred format (HTML, PDF, or Markdown).\n4. The report will be generated and downloaded automatically, containing charts, summaries, and key insights from your session.\n\n---\n\n## Build & Deploy\n\nThe project includes a comprehensive Makefile that simplifies building, pushing, and deploying the application components.\n\n### Building and Pushing Container Images\n\nThe application consists of multiple services that need to be built as container images for OpenShift deployment.\n\n#### **Build All Images**\n\n```bash\n# Build all container images (metrics-api, metric-ui, metric-alerting)\nmake build\n\n# Build with custom tag\nmake build TAG=v1.0.0\n\n# Build with custom registry\nmake build REGISTRY=your-registry.com/your-org\n```\n\n#### **Build Individual Components**\n\n```bash\n# Build FastAPI Backend (metrics-api)\nmake build-metrics-api\n\n# Build Streamlit UI (metric-ui)\nmake build-ui\n\n# Build Alerting Service (metric-alerting)\nmake build-alerting\n```\n\n#### **Push Images to Registry**\n\n```bash\n# Push all images to registry\nmake push\n\n# Push with custom tag\nmake push TAG=v1.0.0\n\n# Push individual components\nmake push-metrics-api\nmake push-ui\nmake push-alerting\n```\n\n#### **Complete Build and Push Workflow**\n\n```bash\n# Build and push all images in one command\nmake build-and-push\n\n# With custom configuration\nmake build-and-push TAG=v1.0.0 REGISTRY=your-registry.com/your-org\n```\n\n### Deploy to OpenShift\n\n#### **Basic Deployment**\n\n```bash\n# Deploy to OpenShift namespace\nmake deploy NAMESPACE=your-namespace\n\n# Deploy with alerting enabled\nmake deploy-with-alerts NAMESPACE=your-namespace\n```\n\n#### **Complete Build, Push, and Deploy Workflow**\n\n```bash\n# Complete workflow: build ‚Üí push ‚Üí deploy\nmake build-deploy NAMESPACE=your-namespace\n\n# Complete workflow with alerting\nmake build-deploy-alerts NAMESPACE=your-namespace\n```\n\n#### **Deployment Management**\n\n```bash\n# Check deployment status\nmake status NAMESPACE=your-namespace\n\n# Uninstall deployment\nmake uninstall NAMESPACE=your-namespace\n```\n\n### Configuration Options\n\nThe Makefile supports various configuration options via environment variables:\n\n```bash\n# Set custom registry\nexport REGISTRY=your-registry.com/your-org\n\n# Set custom tag\nexport TAG=v1.0.0\n\n# Set target platform\nexport PLATFORM=linux/amd64\n\n# Show current configuration\nmake config\n```\n\n### Local Development\n\nFor local development with port-forwarding:\n\n```bash\n# Set up local development environment\nmake install-local NAMESPACE=your-namespace\n\n# If model is in different namespace\nmake install-local NAMESPACE=default-ns MODEL_NAMESPACE=model-ns\n```\n\nThis will run the `./scripts/local-dev.sh` script to set up port-forwarding to Llamastack, llm-service, and Thanos.\n\n### Available Models\n\n```bash\n# List available models for deployment\nmake list-models\n```\n\n### Cleanup\n\n```bash\n# Clean up local images\nmake clean\n```\n\n\n## Local Development via Port-Forwarding\n\nFor local development of the metrics API/UI and MCP server, use the unified development environment script that handles port-forwarding to Llamastack, LLM service, and Thanos.\n\n**Pre-requisites**:\n1. You have a deployment on the cluster already.\n2. You are logged into the cluster and can execute `oc` commands against the cluster.\n\n### Quick Setup with Makefile\n\nThe easiest way to set up local development is using the Makefile:\n\n```bash\n# Set up local development environment\nmake install-local NAMESPACE=your-namespace\n```\n\nThis will run the `./scripts/local-dev.sh` script automatically.\n\n### Manual Setup\n\nIf you prefer to run the script manually, follow these steps:\n\n1. **Make sure you are logged into the cluster and can execute `oc` commands against the cluster.**\n2. Install `uv` by following instructions on the [uv website](https://github.com/astral-sh/uv)\n3. Sync up the environment and development dependencies using `uv` in the base directory:\n### Quick Start\n```bash\n# 1. Setup environment\nuv sync --group dev\n# Note: Virtual environment activation is handled automatically by the script\n\n# 2. Start development environment (includes all port forwarding)\n./scripts/local-dev.sh -n <DEFAULT_NAMESPACE>\n\n# 3. If model is in different namespace (optional)\n./scripts/local-dev.sh -n <DEFAULT_NAMESPACE> -m <MODEL_NAMESPACE>\n```\n\n### What the script does:\n- ‚úÖ **Activates Python virtual environment** (.venv)\n- ‚úÖ **Port forwards Prometheus/Thanos** (localhost:9090)\n- ‚úÖ **Port forwards LLM server** (localhost:8321) \n- ‚úÖ **Port forwards Model service** (localhost:8080)\n- ‚úÖ **Starts metrics API** (localhost:8000)\n- ‚úÖ **Starts Streamlit UI** (localhost:8501)\n- ‚úÖ **Configures environment** for MCP server development\n\n### For MCP/AI Assistant Development\nAfter running `scripts/local-dev.sh`, you can:\n\n```bash\n# Configure AI assistants (Claude Desktop + Cursor IDE)\ncd src/mcp_server\npython setup_integration.py\n\n# Test MCP server\nobs-mcp-server --test-config\n```\n\nThe output should look like this:\n![Command Output](images/openshift-ai-observability-summarizer_local-dev-expected.png)\n\n#### Macos weasyprint install\n\n**Still verifying whether we need this setup or not as weasyprint is installed using `uv` in previous step.**\n\nIn order to run the metrics API locally you'll need to install weasyprint:\n1. Install via brew `brew install weasyprint`\n2. Ensure installation `weasyprint --version`\n3. Set **DYLD_FALLBACK_LIBRARY_PATH** `export DYLD_FALLBACK_LIBRARY_PATH=/opt/homebrew/lib:$DYLD_FALLBACK_LIBRARY_PATH`\n\n## Running Tests with Pytest\n\nThe test suite is located in the `tests/` directory, with the tests for each service in their respective directories.\n\n1. Using `uv`, sync up the test dependencies listed in `pyproject.toml`:\n\n```bash\n# Create virtual environment\nuv sync --group test\n```\n\n2. Use the `pytest` command go run all tests\n\n```bash\n# Run all tests with verbose output and coverage\nuv run pytest -v --cov=src --cov-report=html --cov-report=term\n\n# Run only MCP tests\nuv run pytest -v tests/mcp/\n\n# Run specific test file\nuv run pytest -v tests/mcp/test_api_endpoints.py\n```\n\nTo view a detailed coverage report after generating, open `htmlcov/index.html`.\n\n---\n\n## GitHub Actions CI/CD\n\nThe project uses 5 automated GitHub Actions workflows for comprehensive CI/CD:\n\n### Workflow Overview\n- **PR Review**: Run Tests and Rebase Check (parallel during PR review)\n- **Post-Merge**: Build ‚Üí Deploy (sequential after merge)\n- **Manual**: Undeploy (manual only with safety confirmation)\n\n### Key Changes\n- **Semantic Versioning**: Now prioritizes PR labels and PR title over commit messages\n- **Deploy Workflow**: Default namespace `dev`\n- **Undeploy Workflow**: Manual execution only with required safety confirmation\n\n### Quick Setup\n1. **Service Account**: Run `./scripts/ocp-setup.sh -s -t -n <namespace>` to create OpenShift service account\n2. **GitHub Secrets**: Configure `OPENSHIFT_SERVER`, `OPENSHIFT_TOKEN`, `HUGGINGFACE_API_KEY`, `QUAY_USERNAME`, `QUAY_PASSWORD`\n3. **Ready**: Workflows automatically run on PR events and merges\n\nüìñ **[Complete GitHub Actions Documentation](docs/GITHUB_ACTIONS.md)** - Detailed workflow configuration, service account setup, troubleshooting, and manual execution instructions.\n\n---\n\n## Semantic Versioning\n\nThis project uses automated semantic versioning based on PR labels, PR titles, and commit message conventions. Version bumps are determined by analyzing PRs in order of priority when merged.\n\n### Version Bump Priority Order\n1. **PR Labels** (highest priority) - Add labels like `major`, `feature`, `bugfix`\n2. **PR Title** (medium priority) - Use conventional commit format in PR title\n3. **Commit Messages** (fallback) - Traditional commit message analysis\n\n### Version Bump Rules\n- **Major (`X`.0.0)**: Breaking changes - Keywords: `BREAKING CHANGE:`, `BREAKING CHANGE` (colon optional), `breaking:`, `!:`, `major:`\n- **Minor (X.`Y`.0)**: New features - Keywords: `feat:`, `feature:`, `add:`, `minor:`\n- **Patch (X.Y.`Z`)**: Bug fixes and other changes - Keywords: `patch`, `bugfix`, `fix`, `documentation` (no colons)\n\n### Quick Examples\n```bash\n# Recommended: Use PR labels\nPR with label \"feature:\" ‚Üí Minor bump\nPR with label \"bugfix\" ‚Üí Patch bump\nPR with label \"major:\" ‚Üí Major bump\n\n# Alternative: Use PR title\n\"feat: add user authentication\" ‚Üí Minor bump\n\"fix: resolve login timeout\" ‚Üí Patch bump  \n\"refactor!: redesign API endpoints\" ‚Üí Major bump\n```\n\nüìñ **[Complete Semantic Versioning Documentation](docs/SEMANTIC_VERSIONING.md)** - Detailed rules, implementation, examples, and troubleshooting.\n\n---\n\n## Helm Charts Documentation\n\nThe project uses Helm charts for OpenShift deployment with centralized image management. Both image repositories and versions are controlled through Makefile variables using Helm's `--set` override functionality:\n\n- **Image repositories**: Controlled via `REGISTRY`, `ORG`, and `IMAGE_PREFIX` variables\n- **Image versions**: Controlled via the `VERSION` variable\n- **Helm overrides**: `--set image.repository=$(IMAGE_NAME)` and `--set image.tag=$(VERSION)`\n\nüìñ **[Complete Helm Charts Documentation](docs/HELM_CHARTS.md)** - Detailed information about Helm chart structure, image management, deployment patterns, and customization options.\n\n---\n\n## Contributing\n\nWe welcome contributions and feedback! Please open issues or submit PRs to improve this dashboard or expand model compatibility.\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for full contribution guidelines.\n\n---\n\n## Community\n\n- [GitHub Discussions](https://github.com/rh-ai-quickstart/openshift-ai-observability-summarizer/discussions)\n- [CNCF Landscape](https://landscape.cncf.io/)\n- [OpenShift AI](https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-ai)\n- [Prometheus](https://prometheus.io/)\n- [Streamlit](https://streamlit.io/)\n\n---\n\n## License\n\nLicensed under the [MIT License](LICENSE).\n\n---\n\n",
      "readmeFilename": "README.md",
      "images": [
        {
          "original": "https://img.shields.io/badge/CNCF%20Compatible-Yes-blue.svg",
          "absolute": "https://img.shields.io/badge/CNCF%20Compatible-Yes-blue.svg",
          "local": "images/openshift-ai-observability-summarizer_CNCF%20Compatible-Yes-blue.svg",
          "alt": "CNCF Compatible"
        },
        {
          "original": "https://img.shields.io/badge/License-MIT-yellow.svg",
          "absolute": "https://img.shields.io/badge/License-MIT-yellow.svg",
          "local": "images/openshift-ai-observability-summarizer_License-MIT-yellow.svg",
          "alt": "License: MIT"
        },
        {
          "original": "https://github.com/rh-ai-quickstart/openshift-ai-observability-summarizer/actions/workflows/build-and-push.yml/badge.svg",
          "absolute": "https://github.com/rh-ai-quickstart/openshift-ai-observability-summarizer/actions/workflows/build-and-push.yml/badge.svg",
          "local": "images/openshift-ai-observability-summarizer_badge.svg",
          "alt": "Build Status"
        },
        {
          "original": "docs/img/arch-2.jpg",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/openshift-ai-observability-summarizer/main/docs/img/arch-2.jpg",
          "local": "images/openshift-ai-observability-summarizer_arch-2.jpg",
          "alt": "Architecture"
        },
        {
          "original": "docs/img/os.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/openshift-ai-observability-summarizer/main/docs/img/os.png",
          "local": "images/openshift-ai-observability-summarizer_os.png",
          "alt": "UI"
        },
        {
          "original": "docs/img/vllm.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/openshift-ai-observability-summarizer/main/docs/img/vllm.png",
          "local": "images/openshift-ai-observability-summarizer_vllm.png",
          "alt": "UI"
        },
        {
          "original": "docs/img/chat.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/openshift-ai-observability-summarizer/main/docs/img/chat.png",
          "local": "images/openshift-ai-observability-summarizer_chat.png",
          "alt": "UI"
        },
        {
          "original": "docs/img/report.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/openshift-ai-observability-summarizer/main/docs/img/report.png",
          "local": "images/openshift-ai-observability-summarizer_report.png",
          "alt": "UI"
        },
        {
          "original": "docs/img/local-dev-expected.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/openshift-ai-observability-summarizer/main/docs/img/local-dev-expected.png",
          "local": "images/openshift-ai-observability-summarizer_local-dev-expected.png",
          "alt": "Command Output"
        }
      ],
      "githubLink": "https://github.com/rh-ai-quickstart/openshift-ai-observability-summarizer",
      "categories": [
        "Python"
      ],
      "stars": 15,
      "lastUpdated": "9/12/2025"
    },
    {
      "id": "guardrailing-llms",
      "title": "Guardrailing Llms",
      "description": "Guardrailing LLMs",
      "readmePreview": "# Guardrailing LLMs\n\nWelcome to the LLM Guardrails quickstart!\nUse this to quickly deploy a comprehensive AI safety framework with TrustyAI orchestrat...",
      "readmeContent": "# Guardrailing LLMs\n\nWelcome to the LLM Guardrails quickstart!\nUse this to quickly deploy a comprehensive AI safety framework with TrustyAI orchestrator and multiple detector services.  \nTo see how it's done, jump straight to [installation](#install). \n\n## Detailed description \n\nThe LLM Guardrails quickstart is a quick-start template for deploying a comprehensive AI safety framework within Red Hat OpenShift AI. It's designed to provide multiple layers of protection for LLM applications using TrustyAI's orchestrator and specialized detector services.\n\nThis quickstart includes a Helm chart for deploying:\n\n- A Llama 3.2 3B Instruct model with GPU acceleration.\n- Multiple AI safety detectors: gibberish detection, prompt injection detection, and hate/profanity detection.\n- TrustyAI GuardrailsOrchestrator for coordinating safety checks.\n- Configurable detection thresholds and routing policies.\n\n## Healthcare use case example\n\nThis quickstart includes a healthcare AI assistant demo that shows how guardrails protect HIPAA-compliant applications.\n\nThe demo tests a patient services AI with four protection layers:\n1. **PII Detection** - Protects Social Security Numbers and medical IDs\n2. **Content Moderation** - Blocks inappropriate language  \n3. **Prompt Injection Protection** - Prevents system manipulation\n4. **Gibberish Detection** - Filters out nonsense queries\n\nFor example, here's how PII detection works in action:\n\n![diagram.png](images/guardrailing-llms_wb0.png)\n\nExplore the complete interactive demo in `assets/healthcare-guardrails.ipynb`.\n\n## Arcade demo\n\nShort on time or don't have an environment? No problem! Try our step-by-step Arcade Demo for a guided walkthrough.\n\n*Coming soon*\n\n### Architecture diagrams\n\n![architecture.png](images/guardrailing-llms_architecture.png)\n\n### References \n\n- [Red Hat documentation](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.23/html/monitoring_data_science_models/configuring-the-guardrails-orchestrator-service_monitor)\n\n## Requirements \n\n### Recommended hardware requirements \n\n- GPU required for main LLM: +24GiB vRAM\n- CPU cores: 12+ cores total (4 for LLM + 8 for detectors)\n- Memory: 24Gi+ RAM total\n- Storage: 10Gi\n\n### Minimum hardware requirements \n\n- GPU required for main LLM: 1 x NVIDIA GPU with 24GiB vRAM\n- CPU cores: 8+ cores total\n- Memory: 16Gi+ RAM total\n- Storage: 5Gi \n\n### Required software  \n\n- Red Hat OpenShift\n- Red Hat OpenShift Service Mesh 2\n- Red Hat OpenShift AI\n    - KServe needs to be enabled\n\n### Required permissions\n\n- Cluster admin permissions are required\n\n## Install\n\n**Please note before you start**\n\nThis example was tested on Red Hat OpenShift 4.19.9 & Red Hat OpenShift AI 2.23.0.  \n\n### Clone the repository\n\n```\ngit clone https://github.com/rh-ai-quickstart/guardrailing-llms.git && cd guardrailing-llms/\n```\n\n### Create a new project\n\n```bash\nPROJECT=\"guardrails-demo\"\n\noc new-project ${PROJECT}\n``` \n\n### Install with Helm\n\n```bash\nhelm install guardrailing-llms helm/ --namespace ${PROJECT} \n```\n\n### Wait for the pods to be ready\n\n```bash\noc get pod -n ${PROJECT}\n```\n\nYou should see an output similar to:\n<pre>\nNAME                                                         READY   STATUS      RESTARTS   AGE\ngibberish-detector-predictor-578fc59776-www4s                2/2     Running     0          25h\ngorch-sample-5f95f587fd-wmk4x                                3/3     Running     0          51m\nguardrails-workbench-0                                       2/2     Running     0          93m\nguardrails-workbench-clone-repo-96jhd                        0/1     Completed   0          93m\nibm-hate-and-profanity-detector-predictor-846758cfb5-wnlnd   2/2     Running     0          25h\nllama-32-3b-instruct-predictor-c8d55bd58-lctjn               2/2     Running     0          18m\nprompt-injection-detector-predictor-7d784957f9-f2x5g         2/2     Running     0          25h\n</pre>\n\n### Test\n\nYou can get the OpenShift AI Dashboard URL by:\n```bash\noc get routes rhods-dashboard -n redhat-ods-applications\n```\n\nOnce inside the dashboard, navigate to Data Science Projects -> guardrails-demo (or what you called your ${PROJECT} if you changed from default).\n\n![OpenShift AI Projects](images/guardrailing-llms_wb1.png)\n\nInside the project you can see Workbenches, open up the one for guardrails-workbench.\n\n![OpenShift AI WB](images/guardrailing-llms_wb2.png)\n\nOpen the workbench, inside of the Jupyter Notebook folder, you'll see the `guardrailing-llms` repository already cloned, go to `assets/healthcare-guardrails.ipynb` and follow the instructions.\n\n![OpenShift AI Jupyter Notebook](images/guardrailing-llms_wb3.png)\n\nEnjoy!\n\n## Uninstall\n\n```bash\nhelm uninstall guardrailing-llms --namespace ${PROJECT} \n```\n",
      "readmeFilename": "README.md",
      "images": [
        {
          "original": "assets/images/wb0.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/guardrailing-llms/main/assets/images/wb0.png",
          "local": "images/guardrailing-llms_wb0.png",
          "alt": "diagram.png"
        },
        {
          "original": "assets/images/architecture.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/guardrailing-llms/main/assets/images/architecture.png",
          "local": "images/guardrailing-llms_architecture.png",
          "alt": "architecture.png"
        },
        {
          "original": "assets/images/wb1.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/guardrailing-llms/main/assets/images/wb1.png",
          "local": "images/guardrailing-llms_wb1.png",
          "alt": "OpenShift AI Projects"
        },
        {
          "original": "assets/images/wb2.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/guardrailing-llms/main/assets/images/wb2.png",
          "local": "images/guardrailing-llms_wb2.png",
          "alt": "OpenShift AI WB"
        },
        {
          "original": "assets/images/wb3.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/guardrailing-llms/main/assets/images/wb3.png",
          "local": "images/guardrailing-llms_wb3.png",
          "alt": "OpenShift AI Jupyter Notebook"
        }
      ],
      "githubLink": "https://github.com/rh-ai-quickstart/guardrailing-llms",
      "categories": [
        "Jupyter Notebook"
      ],
      "stars": 0,
      "lastUpdated": "9/10/2025"
    },
    {
      "id": "llm-cpu-serving",
      "title": "Llm Cpu Serving",
      "description": "This quickstart will serve a small language model on CPUs, using vLLM inference runtime",
      "readmePreview": "# vllm-cpu\n\nWelcome to the vLLM CPU quickstart!\nUse this to quickly get a vLLM up and running in your environment.  \nTo see how it's done, jump straig...",
      "readmeContent": "# vllm-cpu\n\nWelcome to the vLLM CPU quickstart!\nUse this to quickly get a vLLM up and running in your environment.  \nTo see how it's done, jump straight to [installation](#install).\n\n## Detailed description \n\nThe vLLM CPU quickstart is a quick-start template for deploying vLLM on CPU-based infrastructure within Red Hat OpenShift. It‚Äôs designed for environments where GPUs are not available or necessary, making it ideal for lightweight inference use cases, prototyping, or constrained environments.  \nIn this Quickstart, we are utilizing it to easily get an LLM deployed in most environments.\n\nThis quickstart includes a Helm chart for deploying:\n\n- An OpenShift AI Project.\n- vLLM with CPU support running an instance of TinyLlama.\n- AnythingLLM (a versitile chat interface) running as a workbench and connected to the vLLM.\n\nUse this project to quickly spin up a minimal vLLM instance and start serving models like TinyLlama on CPU‚Äîno GPU required. üöÄ\n\n### See it in action\n\nRed Hat uses Arcade software to create interactive demos. Check out \n[Quickstart with TinyLlama on CPU](https://interact.redhat.com/share/zsT3j9cgPt9yyPchb7EJ)\n to see it in action.\n\n\n### Architecture diagrams\n\n![architecture.png](images/llm-cpu-serving_architecture.png)\n\n### References \n\n- The runtime is built from [vLLM CPU](https://docs.vllm.ai/en/latest/getting_started/installation/cpu.html)\n- Runtime image is pushed to [quay.io/repository/rh-aiservices-bu/vllm-cpu-openai-ubi9](https://quay.io/repository/rh-aiservices-bu/vllm-cpu-openai-ubi9)\n- Code for Runtime image and deployment can be found on [github.com/rh-aiservices-bu/llm-on-openshift](https://github.com/rh-aiservices-bu/llm-on-openshift/tree/main/serving-runtimes/vllm_runtime)\n\n## Requirements \n\n### Recommended hardware requirements \n\n- No GPU needed! ü§ñ\n- 8 cores \n- 8 Gi \n- Storage: 5Gi\n\nNote: This version is compiled for Intel CPU's (preferrably with AWX512 enabled to be able to run compressed models as well, but optional).  \nHere's an example machine from AWS that works well: [https://instances.vantage.sh/aws/ec2/m6i.4xlarge](https://instances.vantage.sh/aws/ec2/m6i.4xlarge)\n\n### Minimum hardware requirements \n\n- No GPU needed! ü§ñ\n- 2 cores \n- 4 Gi \n- Storage: 5Gi \n\n### Required software  \n\n- Red Hat OpenShift \n- Red Hat OpenShift AI \n- Dependencies for [Single-model server](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.16/html/installing_and_uninstalling_openshift_ai_self-managed/installing-the-single-model-serving-platform_component-install#configuring-automated-installation-of-kserve_component-install):\n    - Red Hat OpenShift Service Mesh\n    - Red Hat OpenShift Serverless\n\n### Required permissions\n\n- Standard user. No elevated cluster permissions required \n\n## Install\n\n**Please note before you start**\n\nThis example was tested on Red Hat OpenShift 4.16.24 & Red Hat OpenShift AI v2.16.2.  \n\n### Clone\n\n```\ngit clone https://github.com/rh-ai-quickstart/llm-cpu-serving.git && \\\n    cd llm-cpu-serving/  \n```\n\n\n\n### Create the project\n\n```bash\nPROJECT=\"tinyllama-cpu-demo\"\n\noc new-project ${PROJECT}\n``` \n\n### Install with Helm\n\n```\nhelm install llm-cpu-serving helm/ \\\n    --namespace  ${PROJECT} \n```\n\n### Wait for pods\n\n```\noc -n ${PROJECT}  get pods -w\n```\n\n```\n(Output)\nNAME                                         READY   STATUS    RESTARTS   AGE\nanythingllm-0                                2/2     Running   0          5m\ntinyllama-1b-cpu-predictor-df76b56d6-fw8fp   2/2     Running   0          5m\n```\n\n### Test\n\nYou can get the OpenShift AI Dashboard URL by:\n```bash\noc get routes rhods-dashboard -n redhat-ods-applications\n```\n\nOnce inside the dashboard, navigate to Data Science Projects -> tinyllama-cpu-demo (or what you called your ${PROJECT} if you changed from default).\n![OpenShift AI Projects](images/llm-cpu-serving_rhoai-1.png)\n\nInside the project you can see Workbenches, open up the one for AnythingLLM.\n![OpenShift AI Projects](images/llm-cpu-serving_rhoai-2.png)\n\nFinally, you can create a new Workspace in AnythingLLM and start chatting with your model! :)\n![AnythingLLM](images/llm-cpu-serving_anythingllm-1.png)\n\n\n\n## Uninstall\n```\nhelm uninstall vllm-cpu --namespace ${PROJECT} \n```\n",
      "readmeFilename": "README.md",
      "images": [
        {
          "original": "images/architecture.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/llm-cpu-serving/main/images/architecture.png",
          "local": "images/llm-cpu-serving_architecture.png",
          "alt": "architecture.png"
        },
        {
          "original": "images/rhoai-1.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/llm-cpu-serving/main/images/rhoai-1.png",
          "local": "images/llm-cpu-serving_rhoai-1.png",
          "alt": "OpenShift AI Projects"
        },
        {
          "original": "images/rhoai-2.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/llm-cpu-serving/main/images/rhoai-2.png",
          "local": "images/llm-cpu-serving_rhoai-2.png",
          "alt": "OpenShift AI Projects"
        },
        {
          "original": "images/anythingllm-1.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/llm-cpu-serving/main/images/anythingllm-1.png",
          "local": "images/llm-cpu-serving_anythingllm-1.png",
          "alt": "AnythingLLM"
        }
      ],
      "githubLink": "https://github.com/rh-ai-quickstart/llm-cpu-serving",
      "categories": [
        "Smarty"
      ],
      "stars": 4,
      "lastUpdated": "9/2/2025"
    },
    {
      "id": "vllm-tool-calling",
      "title": "Vllm Tool Calling",
      "description": "AI quickstart for deploying an LLM with Tool Calling enabled on top of OpenShift AI",
      "readmePreview": "# vLLM Tool Calling \n\n![Ready for publish](https://img.shields.io/badge/ready_for-publish-red)\n![Ready for publish](https://img.shields.io/badge/Sourc...",
      "readmeContent": "# vLLM Tool Calling \n\n![Ready for publish](images/vllm-tool-calling_ready_for-publish-red.png)\n![Ready for publish](images/vllm-tool-calling_Authors-AIBU-green.png)\n\n\nWelcome to the vLLM [Function Calling](https://ai-on-openshift.io/odh-rhoai/enable-function-calling/) Quickstart!  \n\nUse this to quickly get a vLLM runtime with Function Calling enabled in your OpenShift AI environment, loading models directly from ModelCar containers.  \n\nTo see how it's done, jump straight to [installation](#install).\n\n## Table of Contents\n\n- [vLLM Tool Calling](#vllm-tool-calling)\n- [Detailed description](#detailed-description)\n    - [See it in action](#see-it-in-action)\n    - [Architecture diagrams](#architecture-diagrams)\n    - [References](#references)\n- [Requirements](#requirements)\n    - [Minimum hardware requirements](#minimum-hardware-requirements)\n    - [Required software](#required-software)\n    - [Required permissions](#required-permissions)\n- [Install](#install)\n    - [Clone the repository](#clone-the-repository)\n    - [Create the project](#create-the-project)\n    - [Choose your LLM to be deployed](#choose-your-llm-to-be-deployed)\n    - [Check the deployment](#check-the-deployment)\n\n## Detailed description\n\nThe vLLM Function Calling Quickstart is a template for deploying vLLM with Function Calling enabled, integrated with ModelCar containerized models, within Red Hat OpenShift AI.\n\nIt‚Äôs designed for environments where you want to:\n\n- Enable LLMs to call external tools (Tool/Function Calling).\n- Serve LLMs (like Granite3, Llama3) directly from a container.\n- Easily customize your model deployments without needing cluster admin privileges.\n\nUse this project to quickly spin up a powerful vLLM instance ready for function-enabled Agents or AI applications.\n\n### See it in action\n\nRed Hat uses Arcade software to create interactive demos. Check out [Function Calling Quickstart Example](TBD) to see it live.\n\n### Architecture diagrams\n\n![architecture.png](images/vllm-tool-calling_architecture.png)\n\n### References \n\n- The runtime is out of the box in RHOAI called [vLLM ServingRuntime for KServe](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.19/html/serving_models/serving-large-models_serving-large-models#supported-model-serving-runtimes_serving-large-models)\n- Detailed guide and documentations is available in [this article.](https://ai-on-openshift.io/odh-rhoai/enable-function-calling/)\n- Code for testing the Function Calling in OpenShift AI is in [github.com/rh-aiservices-bu/llm-on-openshift](https://github.com/rh-aiservices-bu/llm-on-openshift/blob/main/examples/notebooks/langchain/Langchain-FunctionCalling.ipynb)\n\nNOTE: To find more patterns and pre-built ModelCar images, take a look at the [Red Hat AI Services ModelCar Catalog repo](https://github.com/redhat-ai-services/modelcar-catalog) on GitHub and the [ModelCar Catalog registry](https://quay.io/repository/redhat-ai-services/modelcar-catalog) on Quay. \n\n## Requirements\n\n### Minimum hardware requirements\n\n- 8+ vCPUs\n- 24+ GiB RAM\n- Storage: 30Gi minimum in PVC (larger models may require more)\n\n#### Optional, depending on selected hardware platform\n- 1 GPU (NVIDIA L40, A10, or similar)\n- 1 Intel¬Æ Gaudi¬Æ AI Accelerator\n\n### Required software  \n- Red Hat OpenShift \n- Red Hat OpenShift AI 2.16+\n- Dependencies for [Single-model server](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.16/html/installing_and_uninstalling_openshift_ai_self-managed/installing-the-single-model-serving-platform_component-install#configuring-automated-installation-of-kserve_component-install):\n    - Red Hat OpenShift Service Mesh\n    - Red Hat OpenShift Serverless\n\n### Required permissions\n\n- Standard user. No elevated cluster permissions required \n\n## Install\n\n**Please note before you start**\n\nThis example was tested on Red Hat OpenShift 4.16.24 & Red Hat OpenShift AI v2.16.2.  \n\n### Clone the repository\n\n```\ngit clone https://github.com/rh-ai-quickstart/vllm-tool-calling.git && \\\n    cd vllm-tool-calling/  \n```\n\n### Create the project\n\n`PROJECT` can be set to any value. This will also be used as the namespace.\n\n```bash\nexport PROJECT=\"vllm-tool-calling-demo\"\n\noc new-project ${PROJECT}\n```\n\n### Choose your LLM to be deployed\n\n* For [Llama3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B):\n\n```\noc apply -k vllm-tool-calling/llama3.2-1b\n```\n\nSpecify your LLM and device:\n- MODEL: select from [[granite3.2-8b](https://huggingface.co/ibm-granite/granite-3.2-8b-instruct), [llama3.2-1b](https://huggingface.co/meta-llama/Llama-3.2-1B), [llama3.2-3b](https://huggingface.co/meta-llama/Llama-3.2-3B)]\n- DEVICE: select from [gpu, hpu]\n\nSet variables to the selected options. Example is shown below.\n```bash\nexport MODEL=\"granite3.2-8b\"\nexport DEVICE=\"gpu\"\n```\n\nUpdate the following files in the `vllm-tool-calling/${MODEL}/${DEVICE}` folder if `PROJECT` is different from `vllm-tool-calling-demo`. The `namespace` field must match EXACTLY with the value of `PROJECT` set in the previous step to ensure the model is deployed in the proper namespace.\n- `kustomization.yaml`\n\nDeploy the LLM on the target hardware:\n```bash\noc apply -k vllm-tool-calling/${MODEL}/${DEVICE}\n```\n\n\n### Check the deployment\n\n* From the OpenShift Console, go to the App Switcher / Waffle and go to the Red Hat OpenShift AI Console.\n\n* Once inside the dashboard, navigate to Data Science Projects -> vllm-tool-calling-demo (or what you called your ${PROJECT} if you changed from default):\n\n![OpenShift AI Projects](images/vllm-tool-calling_rhoai-1.png)\n\n* Check the models deployed, and wait until you get the green tick in the Status, meaning that the model is deployed successfully:\n\n![OpenShift AI Projects](images/vllm-tool-calling_rhoai-2.png)\n",
      "readmeFilename": "README.md",
      "images": [
        {
          "original": "https://img.shields.io/badge/ready_for-publish-red",
          "absolute": "https://img.shields.io/badge/ready_for-publish-red",
          "local": "images/vllm-tool-calling_ready_for-publish-red.png",
          "alt": "Ready for publish"
        },
        {
          "original": "https://img.shields.io/badge/Source/Authors-AIBU-green",
          "absolute": "https://img.shields.io/badge/Source/Authors-AIBU-green",
          "local": "images/vllm-tool-calling_Authors-AIBU-green.png",
          "alt": "Ready for publish"
        },
        {
          "original": "assets/images/architecture.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/vllm-tool-calling/main/assets/images/architecture.png",
          "local": "images/vllm-tool-calling_architecture.png",
          "alt": "architecture.png"
        },
        {
          "original": "assets/images/rhoai-1.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/vllm-tool-calling/main/assets/images/rhoai-1.png",
          "local": "images/vllm-tool-calling_rhoai-1.png",
          "alt": "OpenShift AI Projects"
        },
        {
          "original": "assets/images/rhoai-2.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/vllm-tool-calling/main/assets/images/rhoai-2.png",
          "local": "images/vllm-tool-calling_rhoai-2.png",
          "alt": "OpenShift AI Projects"
        }
      ],
      "githubLink": "https://github.com/rh-ai-quickstart/vllm-tool-calling",
      "categories": [
        "AI"
      ],
      "stars": 1,
      "lastUpdated": "9/1/2025"
    },
    {
      "id": "dynamic-model-router",
      "title": "Dynamic Model Router",
      "description": "Dynamically route user prompts to LoRA adapters or a base LLM using semantic evaluation on Red Hat OpenShift AI with LiteLLM and vLLM.",
      "readmePreview": "\n# Dynamic Model Routing with Semantic Evaluation and LoRA Adapters\nWelcome to the LLM Dynamic Model Routing quickstart! Use this to quickly deploy a ...",
      "readmeContent": "\n# Dynamic Model Routing with Semantic Evaluation and LoRA Adapters\nWelcome to the LLM Dynamic Model Routing quickstart! Use this to quickly deploy a semantic router with LoRA adapters and a fallback base model using LiteLLM and vLLM.\n\n## Detailed description\nThis quickstart demonstrates how to build a cost-efficient and scalable LLM deployment by combining semantic routing, LoRA adapters, and vLLM on Red Hat OpenShift AI. Incoming user requests are processed through LiteLLM, semantically evaluated, and routed to the most appropriate LoRA adapter or the default base model to ensure more precise and context-aware responses without hosting multiple full models.\n\n### Architecture diagrams\nThe architecture integrates several components to ensure efficient request handling and accurate responses. [Open WebUI](https://openwebui.com/) provides an intuitive interface for users to interact smoothly with the system, and [LiteLLM](https://www.litellm.ai/), which acts as a proxy, utilizes the [semantic router](https://github.com/aurelio-labs/semantic-router) to determine which is the most suitable destination‚Äîwhether it‚Äôs the base model or a specialized LoRA adapter. Based on this decision, LiteLLM forwards the request to [vLLM](https://github.com/vllm-project/vllm) for inference.\n\n![vllm_lora_litellm.jpg](images/dynamic-model-router_vllm_lora_litellm.jpg)\n\n### References\n* [Creating cost effective specialized AI solutions with LoRA adapters on Red Hat OpenShift AI\n](https://www.redhat.com/en/blog/creating-cost-effective-specialized-ai-solutions-lora-adapters-red-hat-openshift-ai)\n* https://le.qun.ch/en/blog/2023/09/11/multi-lora-potentials/\n* https://le.qun.ch/en/blog/2023/05/13/transformer-batching/\n\n## Requirements \n\n### Minimum hardware requirements\nThis demo is designed to run a model using GPU acceleration. The following hardware resources are required:\n- CPU: 1 vCPU\n- Memory: 4 GiB\n- GPU: 1 NVIDIA GPU (e.g., A10, A100, L40S, or similar)\n  \n### Required software\n- Red Hat OpenShift\n- Red Hat OpenShift AI\n\n### Required permissions\n- Standard user. No elevated cluster permissions required.\n\n## Install\nLet‚Äôs dive into the technical aspects of building this setup with a practical example. We are using the [Phi-2](https://huggingface.co/microsoft/phi-2) LLM as the base model, and two LoRA adapters [Phi2-Doctor28e](https://huggingface.co/petualang/Phi2-Doctor28e/tree/main) and [phi-2-dcot](https://huggingface.co/haritzpuerto/phi-2-dcot).\n\nThis repository packages all the required components as a helm chart, which helps you deploy with one command.\n\n### Clone\n```bash\ngit clone https://github.com/rh-ai-quickstart/dynamic-model-router\ncd dynamic-model-router/chart\n```\n\n### Create a project\n```bash\nPROJECT=\"dynamic-model-router-demo\"\noc new-project ${PROJECT}\n```\n\n### Install with Helm\n```bash\nhelm install dynamic-model-router . --namespace ${PROJECT}\n```\n\n### Validating the deployment\nAfter deployment, verify that the setup works as expected by logging into Open WebUI and sending various queries. By checking the LiteLLM logs, you can ensure that requests are correctly routed to either the base model or the relevant adapter.\n\nFor instance, in a clinical healthcare scenario, if you ask, ‚ÄúIs there medication for my symptoms?‚Äù the query will be routed to the `phi-2-doctor` adapter. You can then verify the routing by checking the LiteLLM logs, as shown below.\n\n_Please keep in mind that while the model may provide useful information, all output should be reviewed for its suitability and it is essential to consult a professional for personalized advice._\n\nTo validate the connection between the OpenWebUI and the LiteLLM proxy click on the top left and you should see _phi2_, _dcot_ and _doctor_ models listed. And to confirm that the proxy is working, you should see similar logs in LiteLLM pod as you ask questions.\n\n```\nLiteLLM: Proxy initialized with Config, Set models:\ndcot\ndoctor\nphi2\nINFO: 10.131.166.110:55626 - \"GET /models HTTP/1.1\" 200 OK\nINFO: 10.131.166.110:52568 - \"GET /models HTTP/1.1\" 200 OK\n[RouteChoice(name='dcot', function_call=None, similarity_score=0.6218189497571418), RouteChoice(name='doctor', function_call=None, similarity_score=0.6802293320602901)]\ndoctor\nINFO: 10.131.166.110:52576 - \"POST /chat/completions HTTP/1.1\" 200 OK\n```\n\n### Configuring the Semantic Router\n\nThe semantic router is invoked by a LiteLLM pre-invoke function and is run before the call to the actual LLM endpoint is made. This functions uses the **semantic router** framework to decide which models the request should be sent to.\n\nThe code is located in [litellm-config/custom_router.py](chart/litellm-configs/custom_router.py) if you'd like to make changes.\n\n## Uninstall\n```bash\nhelm uninstall dynamic-model-router --namespace ${PROJECT} \n```\n\n",
      "readmeFilename": "README.md",
      "images": [
        {
          "original": "./images/vllm_lora_litellm.jpg",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/dynamic-model-router/main/./images/vllm_lora_litellm.jpg",
          "local": "images/dynamic-model-router_vllm_lora_litellm.jpg",
          "alt": "vllm_lora_litellm.jpg"
        }
      ],
      "githubLink": "https://github.com/rh-ai-quickstart/dynamic-model-router",
      "categories": [
        "Python"
      ],
      "stars": 1,
      "lastUpdated": "8/29/2025"
    },
    {
      "id": "byo-agentic-framework",
      "title": "Byo Agentic Framework",
      "description": "Quickstart for BringYourOwn / MultiAgent Agent into Red Hat AI with Llama Stack",
      "readmePreview": "# [INSERT quickstart name here]\n\n*Section is required. NOTE: any italicized or bracketed text should be deleted\nor replaced.*  \n\n*Replace title & this...",
      "readmeContent": "# [INSERT quickstart name here]\n\n*Section is required. NOTE: any italicized or bracketed text should be deleted\nor replaced.*  \n\n*Replace title & this section with a high-level description\nof your quickstart.* \n\n*Think of it as your elevator pitch. What is the quickstart? What does it do? Why\nshould I bother deploying it?*\n\n*Need an example README? Look at:\n[vllm-cpu](https://github.com/rh-ai-quickstart/vllm-cpu)*\n\n*OR, [vLLM Tool Calling](https://github.com/rh-ai-quickstart/vllm-tool-calling)*\n\n*Lastly, include a link to the installation section so the user can start quickly.*\n\n*For example:* \n\nTo see how it's done, jump straight to [installation](#install). \n\n## Table of contents\n\n*Table of contents is optional, but recommended*\n\n## Detailed description\n*This section is required. This is your chance to describe the AI quickstart.*\n\n### See it in action \n\n*This section is optional but recommended*\n\n### Architecture diagrams\n\n*Section is required. Put images in `assets/images` folder* \n\n### References \n\n*Section required. Include links to supporting information, documentation, or\nlearning materials.*\n\n## Requirements\n\n*Section required* \n\n### Minimum hardware requirements \n\n*Section is required* \n\n### Required software \n\n*Section is required. What software dependencies do they need?* \n\n### Required permissions\n\n*Section is required. Describe the permissions the user will need. Cluster\nadmin? Regular user?*\n\n## Install\n\n*Section is required. Include the explicit steps needed to deploy your\nquickstart. If screenshots are included, remember to put them in the\n`assets/images` folder.*\n\n## Uninstall \n\n*Section required. Include explicit steps to cleanup quickstart.*\n",
      "readmeFilename": "README.md",
      "images": [],
      "githubLink": "https://github.com/rh-ai-quickstart/byo-agentic-framework",
      "categories": [
        "AI"
      ],
      "stars": 0,
      "lastUpdated": "8/27/2025"
    },
    {
      "id": "llama-stack-mcp-server",
      "title": "Llama Stack Mcp Server",
      "description": "Deploys Llama 3.2-3B on vLLM with Llama Stack and MCP servers in OpenShift AI.",
      "readmePreview": "# Llama Stack with MCP Server\n\nWelcome to the Llama Stack with MCP Server Quickstart!\n\nUse this to quickly deploy Llama 3.2-3B on vLLM with Llama Stac...",
      "readmeContent": "# Llama Stack with MCP Server\n\nWelcome to the Llama Stack with MCP Server Quickstart!\n\nUse this to quickly deploy Llama 3.2-3B on vLLM with Llama Stack and MCP servers in your OpenShift AI environment.\n\nTo see how it's done, jump straight to [installation](#install).\n\n## Table of Contents\n\n1. [Description](#description)\n2. [Custom MCP Server](#custom-mcp-server)\n3. [How MCP Servers Work with Llama Stack](#how-mcp-servers-work-with-llama-stack)\n   - [MCP Server Registration](#mcp-server-registration)\n   - [Tool Execution Flow](#tool-execution-flow)\n4. [Architecture diagrams](#architecture-diagrams)\n5. [References](#references)\n6. [Prerequisites](#prerequisites)\n   - [Minimum hardware requirements](#minimum-hardware-requirements)\n   - [Required software](#required-software)\n   - [Required permissions](#required-permissions)\n7. [Install](#install)\n   - [Clone the repository](#clone-the-repository)\n   - [Create the project](#create-the-project)\n   - [Single Command Installation (Recommended)](#single-command-installation-recommended)\n8. [Test](#test)\n9. [Cleanup](#cleanup)\n\n## Description\n\nThis quickstart provides a complete setup for deploying:\n- Llama 3.2-3B model using vLLM on OpenShift AI\n- Llama Stack for agent-based interactions\n- Sample HR application providing restful services to HR data e.g. vacation booking\n- MCP Weather Server for real-time weather data access\n- Custom MCP server providing access to the sample HR application\n\n## Custom MCP Server\n\nThe custom MCP server (`custom-mcp-server/`) demonstrates how to build a Model Context Protocol server that integrates with enterprise APIs. This server provides the following tools to the LLM:\n\n- **Vacation Management**: Check vacation balances and create vacation requests\n\nThe MCP server acts as a bridge between the Llama Stack and the HR Enterprise API, translating LLM tool calls into REST API requests.\n\n**Source Code & Build Instructions**: If you want to modify the custom MCP server, see the complete source code and build instructions in the `custom-mcp-server/` directory. The server is built using Python and can be customized to integrate with your own enterprise APIs.\n\n## How MCP Servers Work with Llama Stack\n\n### MCP Server Registration\n\nMCP servers are registered with Llama Stack through configuration. The Llama Stack server automatically discovers and connects to configured MCP servers at startup. Here's an example of how MCP servers are configured:\n\n```yaml\n# Llama Stack MCP server configuration\nmcpServers:\n  - name: \"mcp-weather\"\n    uri: \"http://mcp-weather:3001\"\n    description: \"Weather data MCP server\"\n  - name: \"hr-api-tools\"\n    uri: \"http://custom-mcp-server:8000/sse\"\n    description: \"HR API MCP server with employee, vacation, job, and performance tools\"\n```\n\nIn this example, this configuration is maintained in the `llama-stack-config` config map, part of the llama-stack helm chart.\n\nWhen Llama Stack starts, it:\n1. **Connects to each MCP server** via Server-Sent Events (SSE) or WebSocket\n2. **Discovers available tools** by querying each server's capabilities\n3. **Registers tool schemas** that describe what each tool does and its parameters\n4. **Makes tools available** to the LLM for use in conversations\n\n### Tool Execution Flow\n\nWhen a user requests to use a tool, here's the complete flow:\n\n1. **User Request**: User asks a question in the Llama Stack Playground (e.g., \"What's the weather in New York?\")\n\n2. **LLM Context**: Llama Stack includes the available tool definitions in the system message sent to the LLM\n\n3. **LLM Response**: The LLM decides to use a tool and responds with a structured tool call (e.g., `getforecast` with location parameter)\n\n4. **Tool Execution**: Llama Stack intercepts the tool call and routes it to the appropriate MCP server\n\n5. **MCP Processing**: The MCP server executes the tool (e.g., calls the weather API or HR database)\n\n6. **Result Return**: The MCP server returns structured results back to Llama Stack\n\n7. **LLM Integration**: Llama Stack provides the tool results to the LLM as context\n\n8. **Final Response**: The LLM incorporates the tool results into a natural language response for the user\n\nThis seamless integration allows the LLM to access real-time data and perform actions while maintaining a natural conversational interface.\n\n## Architecture diagrams\n\n![Llama Stack with MCP Servers Architecture](images/llama-stack-mcp-server_architecture-diagram.png)\n\n## References\n\n- [Llama Stack Documentation](https://rh-aiservices-bu.github.io/llama-stack-tutorial/)\n- [Model Context Protocol (MCP) Quick Start](https://modelcontextprotocol.io/quickstart/server)\n- [vLLM Documentation](https://github.com/vllm-project/vllm)\n- [Red Hat OpenShift AI Documentation](https://access.redhat.com/documentation/en-us/red_hat_openshift_ai)\n\n\n## Prerequisites\n\n### Minimum hardware requirements\n\n- 1 GPU required (NVIDIA L40, A10, or similar)\n- 8+ vCPUs\n- 24+ GiB RAM\n\n### Required software\n\n- Red Hat OpenShift\n- Red Hat OpenShift AI 2.16+\n- OpenShift CLI (`oc`) - [Download here](https://docs.openshift.com/container-platform/latest/cli_reference/openshift_cli/getting-started-cli.html)\n- Helm CLI (`helm`) - [Download here](https://helm.sh/docs/intro/install/)\n\n\n### Required permissions\n\n- Standard user. No elevated cluster permissions required\n\n## Install\n\n**Please note before you start**\n\nThis example was tested on Red Hat OpenShift 4.17.30 & Red Hat OpenShift AI v2.19.0.\n\nAll components are deployed using Helm charts located in the `helm/` directory:\n- `helm/llama3.2-3b/` - Llama 3.2-3B model on vLLM\n- `helm/llama-stack/` - Llama Stack server\n- `helm/mcp-weather/` - Weather MCP server\n- `helm/llama-stack-playground/` - Playground UI\n- `helm/custom-mcp-server/` - Custom HR API MCP server\n- `helm/hr-api/` - HR Enterprise API\n- `helm/llama-stack-mcp/` - Umbrella chart for single-command deployment\n\n### Clone the repository\n\n```bash\ngit clone https://github.com/rh-ai-quickstart/llama-stack-mcp-server.git && \\\n    cd llama-stack-mcp-server/\n```\n\n### Create the project\n\n```bash\noc new-project llama-stack-mcp-demo\n```\n\n### Build and deploy the helm chart\n\nDeploy the complete Llama Stack with MCP servers using the umbrella chart:\n\n```bash\n\n# Build dependencies (downloads and packages all required charts)\nhelm dependency build ./helm/llama-stack-mcp\n\n# Deploy everything with a single command\nhelm install llama-stack-mcp ./helm/llama-stack-mcp \n```\n\n**Note:** The `llama-stack` pod will be in `CrashLoopBackOff` status until the Llama model is fully loaded and being served. This is normal behavior as the Llama Stack server requires the model endpoint to be available before it can start successfully.\n\nThis will deploy all components including:\n- Llama 3.2-3B model on vLLM\n- Llama Stack server with automatic configuration\n- MCP Weather Server\n- HR Enterprise API  \n- HR MCP Server\n- Llama Stack Playground\n\nOnce the deployment is complete, you should see:\n\nTo get the playground URL:\n```bash\n  export PLAYGROUND_URL=$(oc get route llama-stack-playground -o jsonpath='{.spec.host}' 2>/dev/null || echo \"Route not found\")\n  echo \"Playground: https://$PLAYGROUND_URL\"\n```\n\nTo check the status of all components:\n```bash\n  helm status llama-stack-mcp\n  oc get pods \n```\n\nFor troubleshooting:\n```bash\n  oc get pods\n  oc logs -l app.kubernetes.io/name=llama-stack\n  oc logs -l app.kubernetes.io/name=llama3-2-3b\n  oc logs -l app.kubernetes.io/name=custom-mcp-server\n  oc logs -l app.kubernetes.io/name=hr-enterprise-api\n  oc logs -l app.kubernetes.io/name=mcp-weather\n```\n\nWhen the deployment is complete, you should see all pods running in your OpenShift console:\n\n![OpenShift Deployment](images/llama-stack-mcp-server_deployment.png)\n\n## Test\n\n1. Get the Llama Stack playground route:\n```bash\noc get route llama-stack-playground -n llama-stack-mcp-demo\n```\n\n2. Open the playground URL in your browser (it will look something like `https://llama-stack-playground-llama-stack-mcp-demo.apps.openshift-cluster.company.com`)\n\n\n3. In the playground:\n   - Click on the \"Tools\" tab\n   - Select \"Weather\" MCP Server from the available tools\n   - In the chat interface, type: \"What's the weather in New York?\"\n\n4. You should receive a response similar to:\n```\nüõ† Using \"getforecast\" tool:\n\nThe current weather in New York is mostly sunny with a temperature of 75¬∞F and a gentle breeze coming from the southwest at 7 mph. There is a chance of showers and thunderstorms this afternoon. Tonight, the temperature will drop to 66¬∞F with a wind coming from the west at 9 mph. The forecast for the rest of the week is mostly sunny with temperatures ranging from 69¬∞F to 85¬∞F. There is a slight chance of showers and thunderstorms on Thursday and Friday nights.\n```\n\nThis confirms that the Llama Stack is successfully communicating with the MCP Weather Server and can process weather-related queries.\n\n2. **Test HR API MCP server tools**:\n\nIn the playground interface:\n\n- **Navigate to Tools**: Click on the \"Tools\" tab\n- **Verify availability**: Look for your Internal HR tools:\n  - `get_vacation_balance` - Check employee vacation balances\n  - `create_vacation_request` - Submit new vacation requests\n\nSelect the \"internal-hr\" MCP Server\n\n3. **Test with sample queries**:\n\n**Test vacation balance:**\n```\nWhat is the vacation balance for employee EMP001?\n```\n\n**Test vacation request:**\n```\nbook some annual vacation time off for EMP001 for June 8th and 9th\n```\n\n![Llama Stack Playground](images/llama-stack-mcp-server_playground.png)\n\n### Verification\n\nVerify that your custom MCP server is working correctly:\n\n```bash\n# Check all pods are running\noc get pods\n\n# Check the custom MCP server logs\noc logs -l app.kubernetes.io/name=custom-mcp-server\n\n# Test the service connectivity\noc exec -it deployment/llama-stack -- curl http://custom-mcp-server/health\n\n## Cleanup\n\nTo remove all components from OpenShift:\n\n### Option 1: Remove umbrella chart (if using single command installation)\n```bash\n# Remove the complete deployment\nhelm uninstall llama-stack-mcp\n```\n\n\n### Option 2: Delete the entire project\n```bash\n# Delete the project and all its resources\noc delete project llama-stack-mcp-demo\n```\n\nThis will remove:\n- Llama 3.2-3B vLLM deployment\n- Llama Stack services and playground\n- MCP Weather Server\n- Custom MCP Server (if deployed)\n- HR Enterprise API (if deployed)\n- All associated ConfigMaps, Services, Routes, and Secrets",
      "readmeFilename": "README.md",
      "images": [
        {
          "original": "assets/images/architecture-diagram.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/llama-stack-mcp-server/main/assets/images/architecture-diagram.png",
          "local": "images/llama-stack-mcp-server_architecture-diagram.png",
          "alt": "Llama Stack with MCP Servers Architecture"
        },
        {
          "original": "assets/images/deployment.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/llama-stack-mcp-server/main/assets/images/deployment.png",
          "local": "images/llama-stack-mcp-server_deployment.png",
          "alt": "OpenShift Deployment"
        },
        {
          "original": "assets/images/playground.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/llama-stack-mcp-server/main/assets/images/playground.png",
          "local": "images/llama-stack-mcp-server_playground.png",
          "alt": "Llama Stack Playground"
        }
      ],
      "githubLink": "https://github.com/rh-ai-quickstart/llama-stack-mcp-server",
      "categories": [
        "JavaScript"
      ],
      "stars": 0,
      "lastUpdated": "8/25/2025"
    },
    {
      "id": "custom-workbench-images-examples",
      "title": "Custom Workbench Images Examples",
      "description": "An easy way to quickly add a lot of useful, community-provided, custom Workbench Images",
      "readmePreview": "# Custom Workbench Images for Red Hat OpenShift AI\n\nThis repository was created by the CAI team.\nThis repository provides a fast way to add a collecti...",
      "readmeContent": "# Custom Workbench Images for Red Hat OpenShift AI\n\nThis repository was created by the CAI team.\nThis repository provides a fast way to add a collection of community-contributed Custom Workbench Images for Red Hat OpenShift AI (RHOAI), helping you get to work faster with specialized tools and libraries.\n\nThese images extend the functionality of your RHOAI environment with additional tools and capabilities that can be immediately accessed by all users.\n\nThey are not part of the core product, and therefore, do not fall under the support umbrella of OpenShift AI.\n\n\n### Architecture\n\n![OpenShift AI Workbenches](images/custom-workbench-images-examples_simple-arch-diag.png)\n\n## Available Images\n\nThe following custom workbench images are available in this collection. For the full list of available images, see the [`imagestreams`](./imagestreams) directory.\n\n### [AnythingLLM](./imagestreams/AnythingLLM-Custom-Workbench-Image.yaml)\n\n- **Description**: A powerful chatbot frontend that easily connects to Large Language Models.\n- **Key Features**:\n  - OpenAI-compatible API integration\n  - User-friendly chat interface\n  - Customizable chat experience\n- **Use Cases**:\n  - Quickly build and deploy conversational AI applications\n  - Test and iterate on different LLMs\n  - Create interactive demos and prototypes\n- **GitHub Repository**: [llm-on-openshift/anythingllm](https://github.com/rh-aiservices-bu/llm-on-openshift/tree/main/llm-clients/anythingllm)\n\n### [ODH-TEC](./imagestreams/ODH-TEC-Custom-Workbench-Image.yaml)\n\n- **Description**: An enhanced data science workbench with integrated S3 capabilities for seamless data access.\n- **Key Features**:\n  - Web-based S3 browser\n  - Integrated data science tools\n  - Streamlined data access\n- **GitHub Repository**: [odh-tec](https://github.com/opendatahub-io-contrib/odh-tec)\n\n## Getting Started\n\nFollow these steps to install and use the custom workbench images in your Red Hat OpenShift AI environment. For detailed installation instructions, see the documentation in the [`imagestreams`](./imagestreams) directory.\n\n### Prerequisites\n\nBefore you begin, ensure you have the following:\n\n- Red Hat OpenShift cluster with administrator access\n- Red Hat OpenShift AI (RHOAI) installed and configured\n- `oc` CLI tool installed and logged in with cluster-admin privileges\n\n### Installation\n\nYou can install all images at once or individually from the [`imagestreams`](./imagestreams) directory based on your needs.\n\n#### Install All Images\n\nTo install all available custom workbench images, run the following command:\n\n```bash\n# Apply all custom workbench images\noc apply -k https://github.com/rh-ai-kickstart/custom-workbench-images-examples/imagestreams/\n```\n\n#### Install Individual Images\n\nTo install a specific image, use the appropriate command below:\n\n```bash\n# Set the base URL for individual image streams\nURL='https://raw.githubusercontent.com/rh-ai-kickstart/custom-workbench-images-examples/main/imagestreams'\n\n# Install AnythingLLM\noc apply -f ${URL}/AnythingLLM-Custom-Workbench-Image.yaml\n\n# Install ODH-TEC\noc apply -f ${URL}/ODH-TEC-Custom-Workbench-Image.yaml\n```\n\n## Usage\n\nAfter installation, the new images will be available in the RHOAI dashboard. Users can select them when creating or configuring workbenches, just as they would with the default images.\n\n### Uninstallation\n\nTo remove all custom workbench images added by this repository, run the following command:\n\n```bash\noc delete -k https://github.com/rh-ai-kickstart/custom-workbench-images-examples/imagestreams/\n```\n\n## Community\n\n### Contributing\n\nWe welcome contributions to expand this collection of custom workbench images. If you have an image that would be useful to the community, please open a pull request with your proposed changes.\n\n### Support\n\nThese images are provided by the community and are not officially supported by Red Hat. They are provided as-is, and while we strive for quality, there is no guarantee of support. If you encounter issues, please check the respective GitHub repositories or open an issue in this repository. \n\n\n### Categories\n\nThis project falls into the following categories:\n\n- AI\n- Workbench Images\n- RHOAI\n- OpenShift\n\n### Tags\n\n- custom-workbench\n- oc-apply\n",
      "readmeFilename": "README.md",
      "images": [
        {
          "original": "assets/images/simple-arch-diag.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/custom-workbench-images-examples/main/assets/images/simple-arch-diag.png",
          "local": "images/custom-workbench-images-examples_simple-arch-diag.png",
          "alt": "OpenShift AI Workbenches"
        }
      ],
      "githubLink": "https://github.com/rh-ai-quickstart/custom-workbench-images-examples",
      "categories": [
        "AI",
        "OpenShift",
        "RHOAI",
        "Workbench Images"
      ],
      "stars": 0,
      "lastUpdated": "8/13/2025"
    },
    {
      "id": "llama-stack-ReAct",
      "title": "Llama Stack ReAct",
      "description": "No description available",
      "readmePreview": "# Llama Stack with ReACT Agent\n\nWelcome to the Llama Stack with ReACT Agent Kickstart!\n\nUse this to quickly deploy Llama 3.2-3B or Llama 4 Scout on vL...",
      "readmeContent": "# Llama Stack with ReACT Agent\n\nWelcome to the Llama Stack with ReACT Agent Kickstart!\n\nUse this to quickly deploy Llama 3.2-3B or Llama 4 Scout on vLLM with Llama Stack and ReACT agents in your OpenShift AI environment.\n\nTo see how it's done, jump straight to [installation](#install).\n\n## Table of Contents\n\n1. [Description](#description)\n2. [Architecture diagrams](#architecture-diagrams)\n3. [References](#references)\n4. [Prerequisites](#prerequisites)\n   - [Minimum hardware requirements](#minimum-hardware-requirements)\n   - [Required software](#required-software)\n   - [MachineSet considerations](#machineset-considerations)\n   - [Required permissions](#required-permissions)\n5. [Install](#install)\n   - [Clone the repository](#clone-the-repository)\n   - [Create the project](#create-the-project)\n   - [Build and deploy the helm chart](#build-and-deploy-the-helm-chart)\n6. [Test](#test)\n   - [Verification](#verification)\n7. [Cleanup](#cleanup)\n   - [Option 1: Remove umbrella chart](#option-1-remove-umbrella-chart-if-using-single-command-installation)\n   - [Option 2: Delete the entire project](#option-2-delete-the-entire-project)\n8. [ReACT Agent Implementation](#react-agent-implementation)\n9. [How ReACT Agents Work with Llama Stack](#how-react-agents-work-with-llama-stack)\n   - [Agent Initialization](#agent-initialization)\n   - [Tool Execution Flow](#tool-execution-flow)\n\n## Description\n\nThis kickstart provides a complete setup for deploying:\n- Llama 3.2-3B or Llama 4 Scout model using vLLM on OpenShift AI\n- Llama Stack for ReACT agent-based interactions\n- ReACT Agent implementation with reasoning and tool execution capabilities\n- Sample HR application providing RESTful services for HR data e.g. vacation booking\n- MCP Integration with HR API for tool-based vacation management\n\n## Architecture diagrams\n\n![Llama Stack with ReACT Agent Architecture](images/llama-stack-ReAct_architecture-diagram.png)\n\n## References\n\n- [Llama Stack Documentation](https://rh-aiservices-bu.github.io/llama-stack-tutorial/)\n- [Llama Stack ReACT Tutorial](https://rh-aiservices-bu.github.io/llama-stack-tutorial/modules/elementary-02-react.html)\n- [ReACT: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)\n- [vLLM Documentation](https://github.com/vllm-project/vllm)\n- [Red Hat OpenShift AI Documentation](https://access.redhat.com/documentation/en-us/red_hat_openshift_ai)\n\n\n## Prerequisites\n\n### Minimum hardware requirements\n\n- 1 GPU required for Llama 3.2-3b (NVIDIA L40, A10, or similar), 4 GPUs required for Llama 4 Scount (NVIDIA L40S)\n- 8+ vCPUs\n- 24+ GiB RAM\n\n### Required software\n\n- Red Hat OpenShift\n- Red Hat OpenShift AI 2.16+\n- OpenShift CLI (`oc`) - [Download here](https://docs.openshift.com/container-platform/latest/cli_reference/openshift_cli/getting-started-cli.html)\n- Helm CLI (`helm`) - [Download here](https://helm.sh/docs/intro/install/)\n\n### MachineSet considerations\n\nWhen deploying GPU-accelerated models, you'll need to ensure your OpenShift nodes have the appropriate GPU resources and taints/tolerations configured.\n\n#### GPU Node Taints\n\nYour GPU nodes should be tainted to ensure only GPU workloads are scheduled on them:\n\n**For NVIDIA A10G nodes (Llama 3.2-3B):**\n```yaml\ntaints:\n- effect: NoSchedule\n  key: nvidia.com/gpu\n  value: NVIDIA-A10G-SHARED\n```\n\n**For NVIDIA L40S nodes (Llama 4 Scout):**\n```yaml\ntaints:\n- effect: NoSchedule  \n  key: nvidia.com/gpu\n  value: NVIDIA-L40S-SHARED\n```\n\n#### Model Tolerations\n\nThe Helm charts automatically configure the appropriate tolerations for each model:\n\n**Llama 3.2-3B tolerations** (requires 1 GPU):\n```yaml\ntolerations:\n- effect: NoSchedule\n  key: nvidia.com/gpu\n  value: NVIDIA-A10G-SHARED\n```\n\n**Llama 4 Scout tolerations** (requires 4 GPUs):\n```yaml\ntolerations:\n- effect: NoSchedule\n  key: nvidia.com/gpu\n  value: NVIDIA-L40S-SHARED\n```\n\n#### Example MachineSet Configuration\n\nHere's an example MachineSet configuration for GPU nodes:\n\n```yaml\napiVersion: machine.openshift.io/v1beta1\nkind: MachineSet\nmetadata:\n  name: gpu-machineset\n  namespace: openshift-machine-api\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      machine.openshift.io/cluster-api-machineset: gpu-machineset\n  template:\n    metadata:\n      labels:\n        machine.openshift.io/cluster-api-machineset: gpu-machineset\n    spec:\n      taints:\n      - effect: NoSchedule\n        key: nvidia.com/gpu\n        value: NVIDIA-L40S-SHARED  # or NVIDIA-A10G-SHARED for A10G nodes\n      providerSpec:\n        value:\n          # Your cloud provider specific configuration\n          # Include GPU instance types (e.g., g5.xlarge, p4d.24xlarge)\n```\n\n**Note:** Adjust the `value` field in the taint to match your specific GPU type. The models will only schedule on nodes with matching taints.\n\n### Required permissions\n\n- Standard user. No elevated cluster permissions required\n\n## Install\n\n**Please note before you start**\n\nThis example was tested on Red Hat OpenShift 4.17.30 & Red Hat OpenShift AI v2.19.0.\n\nAll components are deployed using Helm charts located in the `helm/` directory:\n- `helm/llama3.2-3b/` - Llama 3.2-3B model on vLLM\n- `helm/llama4-scout/` - Llama 4 Scout model on vLLM\n- `helm/llama-stack/` - Llama Stack server\n- `helm/react-agent/` - ReACT Agent implementation\n- `helm/hr-api/` - HR Enterprise API\n- `helm/custom-mcp-server` - An MCP Server to connect to the HR API\n- `helm/llama-stack-react/` - Umbrella chart for single-command deployment\n\n### Clone the repository\n\n```bash\ngit clone https://github.com/rh-ai-kickstart/llama-stack-react-agent.git && \\\n    cd llama-stack-react-agent/\n```\n\n### Create the project\n\n```bash\noc new-project llama-stack-react-demo\n```\n\n### Build and deploy the helm chart\n\nDeploy the complete Llama Stack with ReACT agent using the umbrella chart\n\nTo deploy with the default model Llama 3.2-3b:\n\n```bash\n\n# Build dependencies (downloads and packages all required charts)\nhelm dependency build ./helm/llama-stack-react\n\n# Deploy everything with a single command\nhelm install llama-stack-react ./helm/llama-stack-react \n\n```\n\nor alternatively, to deploy the Llama4 scout model run:\n\n```bash\n  helm install llama-stack-react ./helm/llama-stack-react \\\n    --set llama3-2-3b.enabled=false \\\n    --set llama4-scout.enabled=true \\\n    --set llama-stack.llamaStack.inferenceModel=llama4-scout \\\n    --set llama-stack.llamaStack.vllmUrl=\"http://llama4-scout-predictor:8080/v1\"\n```\n\n**Note:** The `llama-stack` pod will be in `CrashLoopBackOff` status until the Llama model is fully loaded and being served. This is normal behavior as the Llama Stack server requires the model endpoint to be available before it can start successfully.\n\nThis will deploy all components including:\n- Llama 3.2-3B or Llama 4 Scout model on vLLM\n- Llama Stack server with ReACT agent support\n- ReACT Agent implementation\n- HR Enterprise API  \n- Web interface for agent interaction\n\nOnce the deployment is complete, you should see:\n\nTo get the Streamlit interface URL:\n```bash\n  export INTERFACE_URL=$(oc get route react-agent -o jsonpath='{.spec.host}' 2>/dev/null || echo \"Route not found\")\n  echo \"ReACT Agent Streamlit Interface: https://$INTERFACE_URL\"\n```\n\nTo check the status of all components:\n```bash\n  helm status llama-stack-react\n  oc get pods \n```\n\nFor troubleshooting:\n```bash\n  oc get pods\n  oc logs -l app.kubernetes.io/name=llama-stack\n  oc logs -l app.kubernetes.io/name=llama3-2-3b\n  oc logs -l app.kubernetes.io/name=react-agent\n  oc logs -l app.kubernetes.io/name=hr-enterprise-api\n```\n\nWhen the deployment is complete, you should see all pods running in your OpenShift console:\n\n![OpenShift Deployment](images/llama-stack-ReAct_deployment.png)\n\n## Test\n\n1. Get the ReACT Agent Streamlit interface route:\n```bash\noc get route llama-stack-react-react-agent\n```\n\n2. Open the Streamlit interface URL in your browser (it will look something like `https://llama-stack-react-react-agent-react-demo.apps.openshift-cluster.company.com`)\n\n3. **Configure the Streamlit Interface**:\n   - In the sidebar, set Host: `llama-stack` and Port: `80`\n   - Click \"üîå Connect to Llama Stack\"\n   - Wait for the \"‚úÖ Connected to Llama Stack\" message\n\n4. **Test ReACT Agent Reasoning and Tool Usage**:\n\nIn the Streamlit interface, try these complex queries to test the ReACT reasoning capabilities:\n\n**Test 1: Conditional vacation booking**\n```\nIf user EMP001 has enough remaining vacation days, book two days off for 2nd and 3rd of July 2025\n```\n\n**Test 2: Vacation balance inquiry**\n```\nWhat is the vacation balance for employee EMP001?\n```\n\n\nThe Streamlit interface will show:\n- ü§î **Reasoning steps** in yellow boxes as the agent thinks through problems\n- üõ†Ô∏è **Tool executions** in green boxes when calling HR MCP server\n- ‚úÖ **Results** in blue boxes with the outcomes and decisions\n- üí≠ **Conversation history** showing the complete interaction\n\nThis demonstrates the ReACT agent's ability to:\n- Reason through multi-step problems\n- Use tools conditionally based on previous results\n- Provide comprehensive responses with step-by-step thinking\n\n![ReACT Agent Streamlit Interface](images/llama-stack-ReAct_playground.png)\n\n### Verification\n\nVerify that your ReACT agent is working correctly:\n\n```bash\n# Check all pods are running\noc get pods\n\n# Check the ReACT agent logs\noc logs -l app.kubernetes.io/name=react-agent\n\n# Test the service connectivity\noc exec -it deployment/llama-stack -- curl http://react-agent:8501/_stcore/health\n\n## Cleanup\n\nTo remove all components from OpenShift:\n\n### Option 1: Remove umbrella chart (if using single command installation)\n```bash\n# Remove the complete deployment\nhelm uninstall llama-stack-react\n```\n\n### Option 2: Delete the entire project\n```bash\n# Delete the project and all its resources\noc delete project llama-stack-react-demo\n```\n\nThis will remove:\n- Llama 3.2-3B vLLM deployment\n- Llama Stack services\n- ReACT Agent implementation\n- HR Enterprise API\n- Agent interface\n- All associated ConfigMaps, Services, Routes, and Secrets\n\n## ReACT Agent Implementation\n\nThe ReACT agent implementation (`react-agent/`) demonstrates how to build intelligent agents that can reason through problems and use tools dynamically. This simplified implementation provides:\n\n- **Step-by-Step Reasoning**: Uses ReACT (Reasoning + Acting) pattern to think through problems\n- **HR MCP Integration**: Uses the HR MCP server tools for vacation management\n- **Simple Command Line Interface**: Easy to run and test examples\n- **Conditional Logic**: Makes intelligent decisions based on data analysis\n\nThe ReACT agent connects to Llama Stack and uses HR MCP server tools for vacation operations.\n\n**Quick Start - Streamlit Web Interface**: \n```bash\ncd react-agent/\npip install -r requirements.txt\nstreamlit run streamlit_app.py\n# Open http://localhost:8501 in your browser\n```\n\n**Source Code**: See the complete source code in the `react-agent/` directory. The main web interface is in `streamlit_app.py` and the core agent logic is in `simple_agent.py` - based on the weather MCP example but updated for HR operations.\n\n## How ReACT Agents Work with Llama Stack\n\n### Agent Initialization\n\nReACT agents are initialized using the Llama Stack client with MCP tool definitions. The agent connects to the Llama Stack server and configures available MCP tools. Here's how ReACT agents are configured:\n\n```python\nfrom llama_stack_client import LlamaStackClient\nfrom llama_stack_client.lib.agents.react.agent import ReActAgent\n\n# Initialize client and agent\nclient = LlamaStackClient(base_url=\"http://llama-stack:11011\")\nagent = ReActAgent(\n    client=client,\n    model=\"llama3.2:3b-instruct\",\n    tools=[\"mcp::hr-api-tools\"],  # HR MCP server\n    response_format={\n        \"type\": \"json_schema\",\n        \"json_schema\": ReActOutput.model_json_schema(),\n    },\n    sampling_params={\n        \"strategy\": {\"type\": \"top_p\", \"temperature\": 1.0, \"top_p\": 0.9},\n    }\n)\n```\n\nWhen a ReACT agent starts, it:\n1. **Connects to Llama Stack** via HTTP client\n2. **Discovers MCP tools** available through the HR MCP server\n3. **Initializes reasoning capabilities** for step-by-step problem solving\n4. **Creates sessions** for handling user interactions\n\n### Tool Execution Flow\n\nWhen a user makes a request to a ReACT agent, here's the complete reasoning and execution flow:\n\n1. **User Request**: User asks a complex question (e.g., \"If user EMP001 has enough remaining vacation days, book two days off for 2nd and 3rd of July 2025\")\n\n2. **Agent Session**: ReACT agent creates a new session and begins reasoning process\n\n3. **Reasoning Phase**: The agent thinks through the problem step by step:\n   - \"I need to check EMP001's vacation balance first\"\n   - \"Then I need to determine if they have enough days available\"\n   - \"If yes, I should book the requested dates\"\n\n4. **Tool Selection**: Based on reasoning, the agent decides which tools to use and in what order\n\n5. **Tool Execution**: The agent executes MCP tools sequentially:\n   - Calls HR MCP server tools to check vacation balance\n   - Analyzes the result to determine if booking is possible\n   - Calls HR MCP server tools to book the vacation days\n\n6. **Result Integration**: The agent incorporates tool results into its reasoning\n\n7. **Final Response**: The agent provides a comprehensive answer based on all gathered information\n\nThis ReACT pattern allows the agent to think through complex multi-step problems and execute actions intelligently based on real-time data.",
      "readmeFilename": "README.md",
      "images": [
        {
          "original": "assets/images/architecture-diagram.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/llama-stack-ReAct/main/assets/images/architecture-diagram.png",
          "local": "images/llama-stack-ReAct_architecture-diagram.png",
          "alt": "Llama Stack with ReACT Agent Architecture"
        },
        {
          "original": "assets/images/deployment.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/llama-stack-ReAct/main/assets/images/deployment.png",
          "local": "images/llama-stack-ReAct_deployment.png",
          "alt": "OpenShift Deployment"
        },
        {
          "original": "assets/images/playground.png",
          "absolute": "https://raw.githubusercontent.com/rh-ai-quickstart/llama-stack-ReAct/main/assets/images/playground.png",
          "local": "images/llama-stack-ReAct_playground.png",
          "alt": "ReACT Agent Streamlit Interface"
        }
      ],
      "githubLink": "https://github.com/rh-ai-quickstart/llama-stack-ReAct",
      "categories": [
        "JavaScript"
      ],
      "stars": 0,
      "lastUpdated": "8/8/2025"
    },
    {
      "id": "rhoai-metrics-dashboard",
      "title": "Rhoai Metrics Dashboard",
      "description": "RHOAI Metrics Dashboard for Single Serving Models",
      "readmePreview": "# RHOAI Metrics Dashboard for Single Serving Models\n\nEnable RHOAI User Workload Metrics for Single Serving Models\n\n## Prerequisites\n\n- OpenShift 4.10 ...",
      "readmeContent": "# RHOAI Metrics Dashboard for Single Serving Models\n\nEnable RHOAI User Workload Metrics for Single Serving Models\n\n## Prerequisites\n\n- OpenShift 4.10 or later\n- OpenShift AI 2.10+ installed\n- OpenShift AI KServe installed and configured\n- NVIDIA GPU Operator installed and configured\n",
      "readmeFilename": "README.md",
      "images": [],
      "githubLink": "https://github.com/rh-ai-quickstart/rhoai-metrics-dashboard",
      "categories": [
        "AI"
      ],
      "stars": 0,
      "lastUpdated": "8/5/2025"
    }
  ]
}